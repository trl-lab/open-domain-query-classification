[{"id":1711,"annotations":[{"id":551,"completed_by":1,"result":[{"id":"OWlHzQRly0","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"r9GdseUApG","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"CDG4PTHXg-","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"D7LEbBqNRv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"TS12-zqizV","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-13T11:06:43.125894Z","updated_at":"2025-10-13T11:06:43.125909Z","draft_created_at":"2025-10-13T11:06:00.601404Z","lead_time":61.503,"prediction":{"id":151,"task":1711,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764340Z","updated_at":"2025-10-13T10:55:30.764350Z","created_ago":"11 minutes","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"b90129c6-2c94-48c5-8f49-03eed8f65064","import_id":null,"last_action":null,"bulk_created":false,"task":1711,"project":9,"updated_by":1,"parent_prediction":151,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[151],"data":{"id":322,"query":"What is the name of the most difficult course?","dataset":"BIRD","task_specification_analysis":"The user wants identification of a single item: the course with the maximum difficulty. This is a clear selection\/ranking task (find the top course by a difficulty measure).","scope_specification_analysis":"The metric for \"most difficult\" is not defined (e.g., highest failure\/withdrawal rate, lowest average grade, highest workload hours, toughest prerequisites, lowest pass rate, student survey difficulty). This missing parameter materially changes the result; optional tie-breaking and time window could also matter.","domain_specification_analysis":"No contextual boundary is provided (which institution, platform, sport, region, or organization). Without a domain, \"most difficult course\" is undefined.","entity_specification_analysis":"The core entity \"course\" is undefined: it could mean a university class, an online\/MOOC module, a training course, a golf course, a race course, etc. Additionally, the metric for \"most difficult\" is unspecified (failure rates, ratings, handicap\/slope rating for golf, course records, etc.). Multiple plausible entities with no disambiguation path.","temporal_specification_analysis":"No timeframe is given. Difficulty can vary over time (e.g., course content\/ratings change; golf course setups change). There is no clear all-time interpretation, so a specific time or period is required to make sense of \"most difficult.\""},"meta":{},"created_at":"2025-10-13T10:55:30.641835Z","updated_at":"2025-10-13T11:06:43.180966Z","inner_id":1,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1712,"annotations":[{"id":843,"completed_by":1,"result":[{"id":"OnrntQUB5d","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"CkQeOO7bdb","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"TftCEGHIfw","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"kELTVzbpgN","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"7mgbusgjAo","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:00:49.940239Z","updated_at":"2025-10-22T15:00:49.940262Z","draft_created_at":"2025-10-22T15:00:18.842117Z","lead_time":39.839,"prediction":{"id":152,"task":1712,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764366Z","updated_at":"2025-10-13T10:55:30.764370Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"263a33c3-7924-4b4a-ad06-0d40a58d96de","import_id":null,"last_action":null,"bulk_created":false,"task":1712,"project":9,"updated_by":1,"parent_prediction":152,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[152],"data":{"id":31,"query":"What is the name of the team that won match ID 336000?","dataset":"BIRD","task_specification_analysis":"The query requests a straightforward retrieval: identify the winning team for a specific match ID. This is a clear lookup\/identification task.","scope_specification_analysis":"The operation is unambiguous: locate the match by its unique ID and either read the 'winner' field or infer by comparing the final scores. No additional metrics or methods are needed.","domain_specification_analysis":"No contextual boundary is provided (sport, league, competition, season, or data provider). Since IDs are only meaningful within a particular domain\/namespace, the domain is essential and missing.","entity_specification_analysis":"The core entity 'match ID 336000' is ambiguous because match IDs are namespace-specific (league, sport, or data provider). Without specifying the source system (e.g., FIFA, NBA, ESPN, Cricinfo), multiple different matches could share that ID. Therefore the winning 'team' cannot be uniquely identified.","temporal_specification_analysis":"Time is not inherently required if the match ID were uniquely resolvable; the question targets a specific event. The lack of time is not the blocking issue here."},"meta":{},"created_at":"2025-10-13T10:55:30.641894Z","updated_at":"2025-10-22T15:00:50.028529Z","inner_id":2,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1713,"annotations":[{"id":806,"completed_by":1,"result":[{"id":"6cgaUonFfd","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"UwIRHoUoWM","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"sg-6tqw53x","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"IjaBc-7dyV","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"zwWhXfvRwm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:28:14.364161Z","updated_at":"2025-10-22T14:28:14.364170Z","draft_created_at":"2025-10-22T14:28:08.964061Z","lead_time":15.992,"prediction":{"id":153,"task":1713,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764383Z","updated_at":"2025-10-13T10:55:30.764386Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"40bc4b66-e55c-4036-9d7c-c4be65f4c15c","import_id":null,"last_action":null,"bulk_created":false,"task":1713,"project":9,"updated_by":1,"parent_prediction":153,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[153],"data":{"id":200,"query":"How many sales transactions were given by the customer named Joe L. Lopez?","dataset":"BIRD","task_specification_analysis":"The query requests a single, clear operation: count the number of sales transactions associated with a specific customer.","scope_specification_analysis":"The calculation is straightforward: filter sales transactions where the customer equals 'Joe L. Lopez' and return the count of those transactions. No additional parameters (ranking, grouping, special metrics) are needed beyond a standard row count.","domain_specification_analysis":"The organizational\/domain boundary is missing (which company's or system's sales records?). Without a specified CRM\/ledger or geographic\/organizational context, the domain is ambiguous.","entity_specification_analysis":"The core entity \"customer named Joe L. Lopez\" is not uniquely identifiable in an open domain; many people could share this name and no unique identifier (e.g., customer ID, email, company context) is provided. The referenced data entity \"sales transactions\" is also dataset-specific and unspecified.","temporal_specification_analysis":"No time frame is provided. The count could be lifetime, year-to-date, last quarter, etc. This is not a stable fact nor a superlative that implies all-time, so the period is ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.641932Z","updated_at":"2025-10-22T14:28:14.403052Z","inner_id":3,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1714,"annotations":[{"id":757,"completed_by":1,"result":[{"id":"WJ257TIIkt","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"uplBO3vYdi","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"wDf5A4vVpZ","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"wcXFNv4uEb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"7KsGGwHMSd","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:00:21.407313Z","updated_at":"2025-10-22T06:00:21.407324Z","draft_created_at":"2025-10-22T06:00:08.394598Z","lead_time":22.968,"prediction":{"id":154,"task":1714,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764399Z","updated_at":"2025-10-13T10:55:30.764402Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"d2a35b65-6fbc-495b-9a7d-57a025ae172e","import_id":null,"last_action":null,"bulk_created":false,"task":1714,"project":9,"updated_by":1,"parent_prediction":154,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[154],"data":{"id":259,"query":"Write down the last name of employee who did inspection ID 52238?","dataset":"BIRD","task_specification_analysis":"The user requests a direct lookup: retrieve the last name of the employee associated with inspection ID 52238. This is a clear, executable retrieval task.","scope_specification_analysis":"No complex calculations are needed. Filter by inspection_id = 52238 and return the associated employee last name field. If multiple employees are tied to that inspection, return all corresponding last names.","domain_specification_analysis":"The organizational\/geographical context (which inspection system, company, or agency) is missing. Many domains could have an inspection with ID 52238, so the boundary is unclear.","entity_specification_analysis":"The key entity is the specific inspection record \"ID 52238\" and the associated employee. An ID like 52238 is not globally unique and depends on a particular system\/agency\/company; without that context, the inspection and employee cannot be uniquely identified.","temporal_specification_analysis":"No time frame is referenced or required; the query targets a specific record (if the dataset were known). This is a stable lookup rather than a time-bound statistic."},"meta":{},"created_at":"2025-10-13T10:55:30.641967Z","updated_at":"2025-10-22T06:00:21.450766Z","inner_id":4,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1715,"annotations":[{"id":814,"completed_by":1,"result":[{"id":"yAhXMeDdxh","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"gEzdEeouSN","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Bnpi3KtWie","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"7825K0jzSC","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"W4tjALvlCT","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:33:02.172894Z","updated_at":"2025-10-22T14:33:02.172903Z","draft_created_at":"2025-10-22T14:32:36.212747Z","lead_time":47.603,"prediction":{"id":155,"task":1715,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764415Z","updated_at":"2025-10-13T10:55:30.764418Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"1b3615e5-7e4e-4b32-8e83-e8c26d1ce1c8","import_id":null,"last_action":null,"bulk_created":false,"task":1715,"project":9,"updated_by":1,"parent_prediction":155,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[155],"data":{"id":183,"query":"What is the short description for the title Future Einsteins Of America?","dataset":"BIRD","task_specification_analysis":"The user requests a direct lookup: retrieve the \"short description\" associated with the specified title. No additional computation or modeling is required.","scope_specification_analysis":"Method is clear: filter records where title equals \"Future Einsteins Of America\" and return the short description field. No ranking, aggregation, or derived metrics needed.","domain_specification_analysis":"The query does not specify the source\/context for \"title\" and \"short description\" (e.g., Wikipedia, Wikidata, a catalog, or a proprietary table), making the domain boundary unclear for table retrieval.","entity_specification_analysis":"The core entity is the exact title string \"Future Einsteins Of America,\" which is a concrete identifier and could be looked up once a source\/domain is known.","temporal_specification_analysis":"No time frame provided; for attributes like a page’s short description, the sensible default is the most recent\/current value."},"meta":{},"created_at":"2025-10-13T10:55:30.642001Z","updated_at":"2025-10-22T14:33:02.211808Z","inner_id":5,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1716,"annotations":[{"id":743,"completed_by":1,"result":[{"id":"cLrHwS1gqI","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"pgQbjkuhzv","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"99y-lAaF4h","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"SsEUYzNMdw","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"cmV9Rov__W","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:50:06.251566Z","updated_at":"2025-10-22T05:50:06.251580Z","draft_created_at":"2025-10-22T05:48:26.872461Z","lead_time":109.473,"prediction":{"id":156,"task":1716,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764431Z","updated_at":"2025-10-13T10:55:30.764433Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"32a9e495-53ca-4cde-986a-280b14586d4a","import_id":null,"last_action":null,"bulk_created":false,"task":1716,"project":9,"updated_by":1,"parent_prediction":156,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[156],"data":{"id":0,"query":"List the ID of all images with objects that have multiple relations.","dataset":"BIRD","task_specification_analysis":"The user requests a concrete operation: produce a list of image IDs that satisfy a condition. This is a clearly defined output (a list of IDs).","scope_specification_analysis":"The filtering criterion \"objects that have multiple relations\" is ambiguous in several material ways: (1) whether to count relations where the object is subject only, object only, or either role; (2) whether \"multiple\" counts distinct relation predicates versus total relation edges (including duplicates or multiple targets with the same predicate); (3) whether multiple relations to the same counterpart count separately; (4) whether the image qualifies if any object has multiple relations versus requiring all or a minimum number of such objects; and (5) how to treat self-relations or attribute links if present. These choices can change which images are included.","domain_specification_analysis":"The contextual boundary (which dataset\/corpus or platform with relation-annotated images) is missing. Without specifying a dataset like Visual Genome, Open Images, COCO with scene graphs, etc., the domain is ambiguous because \"relations\" are annotation-specific.","entity_specification_analysis":"Core entities like \"images,\" their \"ID,\" \"objects,\" and \"relations\" are undefined. The dataset or system that defines image IDs and relation annotations is not named, and even \"multiple relations\" (e.g., >1 edge, >1 type, or >1 instance) is unspecified. This prevents unambiguous identification of the target records.","temporal_specification_analysis":"No temporal aspect is implied or required; this is a structural\/annotation-based retrieval rather than a time-dependent fact."},"meta":{},"created_at":"2025-10-13T10:55:30.642033Z","updated_at":"2025-10-22T05:50:06.308329Z","inner_id":6,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1717,"annotations":[{"id":849,"completed_by":1,"result":[{"id":"N7u6hGqlyM","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"j9E3mu0xmW","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"M7af6Xkxle","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"OnN911QseW","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"zcWwzxAtR2","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:08:05.915231Z","updated_at":"2025-10-22T15:08:05.915241Z","draft_created_at":"2025-10-22T15:07:09.097666Z","lead_time":67.911,"prediction":{"id":157,"task":1717,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764446Z","updated_at":"2025-10-13T10:55:30.764448Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"b967aae5-f127-4598-803d-11769fce8057","import_id":null,"last_action":null,"bulk_created":false,"task":1717,"project":9,"updated_by":1,"parent_prediction":157,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[157],"data":{"id":120,"query":"List the first names of customers who have purchased products from sale person id 1.","dataset":"BIRD","task_specification_analysis":"Clear directive to list a specific field (first names) for a subset of customers based on a purchase-from-salesperson condition.","scope_specification_analysis":"This is a straightforward filtered lookup. The condition (purchased from salesperson id = 1) and the projection (first names) are clear. Any deduplication (e.g., DISTINCT) can be assumed as a common-sense default for listing names.","domain_specification_analysis":"The organizational context (which company\/database\/system) is missing. Without a domain boundary, \"customers\" and \"salesperson id 1\" are ambiguous and cannot be located in an open-domain corpus.","entity_specification_analysis":"Entities like \"customers,\" \"products,\" and especially \"sale(s) person id 1\" depend on a specific database\/organization. The internal ID '1' is not globally resolvable, so the core subjects cannot be uniquely identified.","temporal_specification_analysis":"The phrasing \"have purchased\" reasonably implies an all-time\/ever constraint over the available data, so no explicit time filter is required."},"meta":{},"created_at":"2025-10-13T10:55:30.642072Z","updated_at":"2025-10-22T15:08:05.956707Z","inner_id":7,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1718,"annotations":[{"id":708,"completed_by":1,"result":[{"id":"5RieK4giTr","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"0d4O4VP3T1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"QR1RY0GOpQ","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"t0o1Z1cRKu","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"bRKqXqHWwL","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:46:20.521858Z","updated_at":"2025-10-21T22:46:20.521869Z","draft_created_at":"2025-10-21T22:45:47.701752Z","lead_time":66.75,"prediction":{"id":158,"task":1718,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764461Z","updated_at":"2025-10-13T10:55:30.764463Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"fe589d10-da5a-4c08-88c8-e2e22f6efc3a","import_id":null,"last_action":null,"bulk_created":false,"task":1718,"project":9,"updated_by":1,"parent_prediction":158,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[158],"data":{"id":272,"query":"Indicate if there is any pair formed by the words fukunaga and d'egees.","dataset":"BIRD","task_specification_analysis":"The user requests a yes\/no existence check: whether a pair composed of the two specified words exists. This is a clear, executable operation (presence\/boolean).","scope_specification_analysis":"This is a simple lookup for the presence of a pair consisting of the two words (treatable with a standard default of unordered, or by checking both orders if pairs are stored directionally). No additional analytical parameters are required.","domain_specification_analysis":"The query does not specify the context or dataset\/corpus in which to check for the pair (e.g., a particular table of word pairs, a specific text corpus, or adjacency rule). Without a defined domain, a researcher cannot know where to search.","entity_specification_analysis":"The entities are the literal word tokens \"fukunaga\" and \"d'egees\"; as strings they are uniquely identifiable.","temporal_specification_analysis":"No time frame is implied or necessary; the question is not time-dependent."},"meta":{},"created_at":"2025-10-13T10:55:30.642104Z","updated_at":"2025-10-21T22:46:20.559117Z","inner_id":8,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1719,"annotations":[{"id":792,"completed_by":1,"result":[{"id":"aLvX062NLA","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"MKFKvQ9pu8","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"RS_ESAlFAm","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"ttvVVYPSs3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"HhN_Or8nbh","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:15:22.624559Z","updated_at":"2025-10-22T14:15:22.624570Z","draft_created_at":"2025-10-22T14:14:27.764361Z","lead_time":64.145,"prediction":{"id":159,"task":1719,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764475Z","updated_at":"2025-10-13T10:55:30.764478Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"584a87c0-8dcc-4d8c-bf6b-8ffcb04a8d1b","import_id":null,"last_action":null,"bulk_created":false,"task":1719,"project":9,"updated_by":1,"parent_prediction":159,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[159],"data":{"id":161,"query":"Please provide a disability breakdown for each school.","dataset":"BIRD","task_specification_analysis":"The user asks for a \"breakdown\" by school, which is a clear request to produce a descriptive tabulation (i.e., distribution) across schools.","scope_specification_analysis":"Key analytical parameters are missing: what constitutes the \"disability breakdown\" (overall disability status vs. specific disability categories; which classification scheme), whether to show counts, percentages, or both, the denominator for rates, handling of students with multiple disabilities (primary vs. any), and any ordering or inclusion of totals. These choices materially affect results.","domain_specification_analysis":"No geographical or organizational boundary is given. The scope could be a particular district, city, state\/province, country, or an institution network. Disability category standards also vary by jurisdiction, reinforcing the need for a domain boundary.","entity_specification_analysis":"Core entities are ambiguous: \"each school\" does not specify which schools (district, city, state, country, public\/private, K–12 vs. higher ed). The subject of the breakdown is also unclear (students vs. staff vs. programs) and which disability taxonomy to use (e.g., IDEA categories, other classifications).","temporal_specification_analysis":"No time frame is provided. Such breakdowns are typically reported per school year or a specific census date; multiple plausible periods exist, so a year\/as-of date is required to select the correct data."},"meta":{},"created_at":"2025-10-13T10:55:30.642136Z","updated_at":"2025-10-22T14:15:22.665361Z","inner_id":9,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1720,"annotations":[{"id":763,"completed_by":1,"result":[{"id":"7Un1stmeLd","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"TrmId0kaL3","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"nhjc40pi9c","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"b2Ruzqv4xq","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"mbrB2a5Sbv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:03:54.691762Z","updated_at":"2025-10-22T06:03:54.691775Z","draft_created_at":"2025-10-22T06:03:40.562918Z","lead_time":28.079,"prediction":{"id":160,"task":1720,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764490Z","updated_at":"2025-10-13T10:55:30.764493Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"014d3f6f-9f93-4810-9d8d-44bb82d69b17","import_id":null,"last_action":null,"bulk_created":false,"task":1720,"project":9,"updated_by":1,"parent_prediction":160,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[160],"data":{"id":419,"query":"Give the name of the territory where Mr. Robert King works.","dataset":"BIRD","task_specification_analysis":"This is a simple lookup request: retrieve the territory name(s) associated with the employee \"Mr. Robert King.\" No complex analysis is required.","scope_specification_analysis":"No calculations or modeling needed; just fetch the territory name field(s) linked to the specified person. If multiple territories exist, return all corresponding names.","domain_specification_analysis":"\"Territory\" is a domain-specific concept (e.g., sales territory) that depends on an organization or dataset context. The query does not specify which organization or domain, making the boundary ambiguous.","entity_specification_analysis":"\"Mr. Robert King\" is a common name with many plausible individuals. No additional qualifiers (company, role, location, dataset) are provided to uniquely identify the person, so the core entity cannot be confidently resolved.","temporal_specification_analysis":"The query uses present tense (\"works\") but provides no time frame. A reasonable default would be the current\/most recent assignment if the person were known, but no explicit date is given."},"meta":{},"created_at":"2025-10-13T10:55:30.642168Z","updated_at":"2025-10-22T06:03:54.781597Z","inner_id":10,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1721,"annotations":[{"id":784,"completed_by":1,"result":[{"id":"vLVCi501oK","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"N2B_R168Pi","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"xikoclEey1","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"lSQ8OM75_c","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"nPNFDJmj8q","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:08:42.463651Z","updated_at":"2025-10-22T14:08:42.463664Z","draft_created_at":"2025-10-22T14:08:25.443337Z","lead_time":58.519,"prediction":{"id":161,"task":1721,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764505Z","updated_at":"2025-10-13T10:55:30.764508Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"55e76947-1569-41eb-86c4-bb9206673aa0","import_id":null,"last_action":null,"bulk_created":false,"task":1721,"project":9,"updated_by":1,"parent_prediction":161,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[161],"data":{"id":728,"query":"Are any of the albums released in multiple countries in the same year? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"The user requests a binary existence check: determine if there exists at least one album that was released in multiple countries within the same calendar year, and return only 'Yes' or 'No'.","scope_specification_analysis":"Method is clear: for each album, group releases by calendar year, compute the number of distinct countries per album-year, and check if any album-year has count >= 2. Use standard year extraction from release dates and deduplicate repeated entries within the same country-year.","domain_specification_analysis":"\"The albums\" implies a specific dataset or context not provided; the boundary of which albums is unknown and not universally scoped.","entity_specification_analysis":"Refers to \"the albums\" without identifying which albums, artist, dataset, or catalog; relies on an implicit table\/context.","temporal_specification_analysis":"No timeframe is required; the property compares release events within the same year, which are fixed historical facts."},"meta":{},"created_at":"2025-10-13T10:55:30.642210Z","updated_at":"2025-10-22T14:08:42.512757Z","inner_id":11,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1722,"annotations":[{"id":809,"completed_by":1,"result":[{"id":"QNRwfkESHK","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"u8hXFSS1jI","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"kWbJ5e4cG5","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"4J7KXhJEta","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"jgKhrvkp7_","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:29:50.564705Z","updated_at":"2025-10-22T14:29:50.564717Z","draft_created_at":"2025-10-22T14:29:43.639573Z","lead_time":29.238,"prediction":{"id":162,"task":1722,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764520Z","updated_at":"2025-10-13T10:55:30.764522Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a9e2e08c-f63a-4560-9c84-30589a619e40","import_id":null,"last_action":null,"bulk_created":false,"task":1722,"project":9,"updated_by":1,"parent_prediction":162,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[162],"data":{"id":872,"query":"Are there any players that have been drafted from the same college by the Washington Redskins? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"Binary existence check: determine if at least two Washington Redskins draftees share the same college. The requested output format is strictly Yes\/No.","scope_specification_analysis":"Procedure is clear: filter to players drafted by Washington Redskins, group by college, check if any group count ≥ 2. No additional metrics or modeling choices required.","domain_specification_analysis":"The contextual boundary is the NFL, specifically the Washington Redskins franchise’s draft history. No additional geographical or organizational scope is required.","entity_specification_analysis":"\"Washington Redskins\" uniquely identifies the Washington NFL franchise. The query targets the broad class of that team's draft picks and colleges (as attributes) and asks an existential condition across all colleges, not a specific one—sufficiently resolvable without further entity details.","temporal_specification_analysis":"Present perfect phrasing (\"have been drafted\") implies a cumulative, all-time assessment of the franchise’s draft history up to the present. This is a standard, well-defined scope for such queries."},"meta":{},"created_at":"2025-10-13T10:55:30.642243Z","updated_at":"2025-10-22T14:29:50.608138Z","inner_id":12,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1723,"annotations":[{"id":749,"completed_by":1,"result":[{"id":"JFjneL0z1T","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"6dmqjStxWw","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"jm8cMmhmFa","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"-jDz1WaMil","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"BFBkWrgRUH","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:54:06.210400Z","updated_at":"2025-10-22T05:54:06.210410Z","draft_created_at":"2025-10-22T05:53:54.489141Z","lead_time":23.591,"prediction":{"id":163,"task":1723,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764535Z","updated_at":"2025-10-13T10:55:30.764537Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"dc71e299-7915-4885-8787-ef46bafa739a","import_id":null,"last_action":null,"bulk_created":false,"task":1723,"project":9,"updated_by":1,"parent_prediction":163,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[163],"data":{"id":902,"query":"Does the Paraguayan guaraní have a stronger or weaker exchange rate than the Brazilian real?","dataset":"CRT-QA","task_specification_analysis":"Clear comparative task: determine whether the Paraguayan guaraní (PYG) is stronger or weaker than the Brazilian real (BRL).","scope_specification_analysis":"Apply a standard nominal FX comparison using the most recent available spot rates. Compare per-unit values via a common base currency (e.g., USD) or directly via the PYG\/BRL cross-rate; both approaches yield the same qualitative outcome. No additional analytical parameters are necessary.","domain_specification_analysis":"Foreign exchange rates are globally defined and do not require a narrower domain boundary; any standard FX data source applies.","entity_specification_analysis":"Entities are two specific national currencies—Paraguayan guaraní (PYG) and Brazilian real (BRL)—which are uniquely identifiable.","temporal_specification_analysis":"No time frame is given; exchange rates fluctuate continuously. The present-tense wording implies using the most recent available rates."},"meta":{},"created_at":"2025-10-13T10:55:30.642275Z","updated_at":"2025-10-22T05:54:06.253873Z","inner_id":13,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1724,"annotations":[{"id":828,"completed_by":1,"result":[{"id":"hTbQryiCvx","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"yUUL9oiBqw","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"1V2DUtfXEh","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"MV6UzjOTjr","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"KKuQoKnmNW","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:52:34.013061Z","updated_at":"2025-10-22T14:52:34.013075Z","draft_created_at":"2025-10-22T14:41:58.868530Z","lead_time":656.878,"prediction":{"id":164,"task":1724,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764549Z","updated_at":"2025-10-13T10:55:30.764552Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"91e5f63a-3dd6-4ab0-9e50-b829754543b3","import_id":null,"last_action":null,"bulk_created":false,"task":1724,"project":9,"updated_by":1,"parent_prediction":164,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[164],"data":{"id":693,"query":"What is the average Roll size of schools with Decile of 4 in the Manawatu-Wanganui region?","dataset":"CRT-QA","task_specification_analysis":"The query clearly requests an aggregation: compute the average (mean) Roll size for a subset of schools.","scope_specification_analysis":"Filters are explicit (Decile = 4, region = Manawatu-Wanganui) and the aggregation metric is clear (average of Roll size). No ambiguous grouping or weighting is implied.","domain_specification_analysis":"The geographic boundary is explicitly the Manawatu–Wanganui region of New Zealand, and the use of school \"Decile\" and \"Roll\" implies the NZ education context. The domain is clear.","entity_specification_analysis":"The query targets a well-defined class: schools in the Manawatu–Wanganui region that have a school decile of 4 (NZ-specific usage). \"Roll size\" clearly refers to student enrollment. No ambiguity about which entities to include beyond that class.","temporal_specification_analysis":"No time frame is given. Both roll (enrollment) and decile assignments are time-varying, and datasets are versioned by year\/term. Without a date (e.g., year), a researcher cannot choose the correct edition, making the timing ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.642306Z","updated_at":"2025-10-22T14:52:34.067233Z","inner_id":14,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1725,"annotations":[{"id":781,"completed_by":1,"result":[{"id":"TOF40rRMsZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"v4F2Ko3EEt","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"dyT6VlQ7AP","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"vO7PnsTqC0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"ZCpZONiF-l","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:05:14.164399Z","updated_at":"2025-10-22T14:05:14.164411Z","draft_created_at":"2025-10-22T14:04:51.093264Z","lead_time":61.301,"prediction":{"id":165,"task":1725,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764564Z","updated_at":"2025-10-13T10:55:30.764567Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"d79795d3-2ac3-465d-885f-74c468989464","import_id":null,"last_action":null,"bulk_created":false,"task":1725,"project":9,"updated_by":1,"parent_prediction":165,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[165],"data":{"id":626,"query":"Is there a difference in the duration of the expeditions to Salyut 6 depending on the country of the crew? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"The user asks for a binary determination of whether expedition durations differ by crew country; the high-level analytical action (assess existence of a difference) is clear.","scope_specification_analysis":"Key methodological details are missing: how to define \"country of the crew\" for mixed-nationality crews (primary\/launch\/any\/guest), what metric of difference to use (means, medians, distributional test), whether to require statistical significance (test type, alpha) versus any numerical disparity, handling of long-duration vs visiting expeditions, and precise duration definition. These choices could materially change the outcome.","domain_specification_analysis":"Context is unambiguously human spaceflight\/astronautics focused on the specific space station Salyut 6; no additional geographical or organizational boundary is needed.","entity_specification_analysis":"Core entities are clear: 'expeditions to Salyut 6' is a unique, well-defined set of missions, and 'country of the crew' can be resolved via crew member nationalities from mission records. While multi-national crews exist, this does not prevent locating the relevant data tables.","temporal_specification_analysis":"Timeframe is implicitly bound to the operational period of Salyut 6 (historical, 1977–1982) by the entity itself."},"meta":{},"created_at":"2025-10-13T10:55:30.642342Z","updated_at":"2025-10-22T14:05:14.211127Z","inner_id":15,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1726,"annotations":[{"id":827,"completed_by":1,"result":[{"id":"9sf68JZ0nm","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"eXrbjhV0CV","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"UKJQMJnXuc","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"UGCjo8ZsB8","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"YOHvN3UI0g","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:41:36.415588Z","updated_at":"2025-10-22T14:41:36.415602Z","draft_created_at":"2025-10-22T14:41:13.606665Z","lead_time":34.241,"prediction":{"id":166,"task":1726,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764579Z","updated_at":"2025-10-13T10:55:30.764581Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"dc64b3ea-0dea-44f5-888a-006c17eb22d1","import_id":null,"last_action":null,"bulk_created":false,"task":1726,"project":9,"updated_by":1,"parent_prediction":166,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[166],"data":{"id":627,"query":"Does the release date of the paid download format in each region vary from the other formats? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"The user requests a binary (Yes\/No) determination about whether the paid download release date differs from other formats, which is a clear, executable check.","scope_specification_analysis":"The query does not specify how to aggregate comparisons across multiple regions into a single Yes\/No (e.g., “any region differs” vs. “all regions differ”). It is also unclear whether “vary from the other formats” means differing from all other formats or from at least one, and which formats are included as “other.” These ambiguities materially affect the result.","domain_specification_analysis":"The contextual boundary is unclear: \"each region\" could mean countries, sales territories, platforms\/stores, and \"formats\" vary by industry. Without specifying the domain (e.g., music, gaming) and the regional scheme, the scope is ambiguous.","entity_specification_analysis":"The core subject (which product\/work’s release) is missing. Terms like \"paid download format\" and \"other formats\" imply formats of a specific item (album\/game\/film\/software), but that item is not identified, making the entities unresolvable in an open-domain setting.","temporal_specification_analysis":"The query concerns whether release dates differ across formats\/regions for a given item; no time frame is needed or implied beyond the inherent release dates, which are fixed attributes."},"meta":{},"created_at":"2025-10-13T10:55:30.642374Z","updated_at":"2025-10-22T14:41:36.464038Z","inner_id":16,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1727,"annotations":[{"id":774,"completed_by":1,"result":[{"id":"Scj927qzQm","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"-yXAUAtGG_","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Sm5mOBXsM8","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"rv5kHGUgLS","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"1Mxg6ESrUq","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:11:39.328160Z","updated_at":"2025-10-22T06:11:39.328170Z","draft_created_at":"2025-10-22T06:10:48.803859Z","lead_time":72.372,"prediction":{"id":167,"task":1727,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764593Z","updated_at":"2025-10-13T10:55:30.764596Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"73d512d0-8fc2-44ea-b6af-0c9b22074385","import_id":null,"last_action":null,"bulk_created":false,"task":1727,"project":9,"updated_by":1,"parent_prediction":167,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[167],"data":{"id":510,"query":"Are there any class d stations in the kslt table that have higher erp w values than other stations in the same city of license? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"The query asks for a binary (Yes\/No) existence check: whether any class D station has a higher ERP (W) than other stations in the same city. This is a clear, executable task.","scope_specification_analysis":"Ambiguity exists in the comparison criterion: does “higher ... than other stations” mean higher than all other stations in the city (strict maximum) or merely higher than at least one other station (any)? This materially affects the outcome. Tie-handling is implicitly strict (\"higher\"), but the quantifier (any vs all) is not specified.","domain_specification_analysis":"The domain boundary hinges on the unknown \"kslt table\" (no publisher, geography, or organizational context). Without identifying the dataset, we cannot constrain the scope (e.g., FCC\/US broadcast vs other jurisdictions).","entity_specification_analysis":"References a specific but unidentified data container (\"kslt table\"). \"Class D\" could refer to different broadcast categories (AM\/FM\/TV) without clear context. While \"ERP W\" and \"city of license\" hint at broadcast stations, the core entity set depends on an unspecified table and is not uniquely identifiable.","temporal_specification_analysis":"No timeframe is provided, and the answer would depend on the specific snapshot\/version of the referenced table. Referencing a data container for timing is ambiguous in an open-domain setting."},"meta":{},"created_at":"2025-10-13T10:55:30.642407Z","updated_at":"2025-10-22T06:11:39.373198Z","inner_id":17,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1728,"annotations":[{"id":747,"completed_by":1,"result":[{"id":"vNC8kpYnwz","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"0p5Lnl1P-I","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"5EY2l3TFYA","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"g9W70kRfo6","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"N93ZKvqp1d","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:52:41.231404Z","updated_at":"2025-10-22T05:52:41.231414Z","draft_created_at":"2025-10-22T05:52:17.951732Z","lead_time":36.091,"prediction":{"id":168,"task":1728,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764608Z","updated_at":"2025-10-13T10:55:30.764610Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a47eca77-d2ab-440c-a342-3def43cb943d","import_id":null,"last_action":null,"bulk_created":false,"task":1728,"project":9,"updated_by":1,"parent_prediction":168,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[168],"data":{"id":851,"query":"Are there any noticeable differences in population of cast members throughout the episodes of the series? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"The user asks for a binary determination (Yes\/No) about whether differences exist across episodes—an existence test—so the overall analytical action is clear.","scope_specification_analysis":"Critical analytical parameters are missing: what exactly constitutes the 'population of cast members' per episode (e.g., credited cast count, main vs. guest, unique appearances), how to operationalize 'noticeable differences' (any change vs. a defined threshold vs. statistical significance), and what method or threshold to use. Different reasonable choices could yield different answers.","domain_specification_analysis":"The contextual boundary (which specific TV series) is missing; without naming the series, the domain cannot be bounded.","entity_specification_analysis":"Core entities are ambiguous: \"the series\" is not identified, and \"population of cast members\" is not clearly defined (main vs recurring vs guest\/credited). Thus the entities cannot be uniquely resolved.","temporal_specification_analysis":"The query implies the full span of episodes of the (unspecified) series—i.e., across all its episodes—so the temporal scope is inherently the series’ entire run."},"meta":{},"created_at":"2025-10-13T10:55:30.642439Z","updated_at":"2025-10-22T05:52:41.271236Z","inner_id":18,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1729,"annotations":[{"id":727,"completed_by":1,"result":[{"id":"sZ-b6rS6S-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Zz2PeV_0kr","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"k-pYi8wc1C","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Njpj__ERFD","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"ZbyvHuDrBw","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:34:08.499374Z","updated_at":"2025-10-22T05:34:08.499386Z","draft_created_at":"2025-10-22T05:33:33.139319Z","lead_time":53.666,"prediction":{"id":169,"task":1729,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764623Z","updated_at":"2025-10-13T10:55:30.764625Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"c7efd276-7d07-4e77-aaa2-ebcf30e49a53","import_id":null,"last_action":null,"bulk_created":false,"task":1729,"project":9,"updated_by":1,"parent_prediction":169,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[169],"data":{"id":517,"query":"Was the highest 'b score' earned by the same person who earned the highest total score? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"Binary comparison task: determine whether the argmax over b score and argmax over total score refer to the same person, and answer Yes\/No.","scope_specification_analysis":"Tie-handling for 'highest' is not defined. If multiple people share the highest b score and\/or total score, it is unclear whether any overlap should yield 'Yes', whether a unique winner is required, or how to handle a tie (e.g., return No or treat as indeterminate).","domain_specification_analysis":"No domain boundary is provided (e.g., which competition, exam, organization, or dataset). The question requires a specific table\/context to make sense, making the domain ambiguous.","entity_specification_analysis":"Core entities like 'b score', 'total score', and the 'person' achieving them are undefined outside a specific table or dataset. 'b score' is a column-like term with no global meaning, and no dataset or context is provided to identify the relevant people or scores.","temporal_specification_analysis":"No time frame is given, and the metrics depend on a particular dataset\/snapshot or event context. Since the data container is unspecified, the temporal context cannot be inferred or defaulted to recency."},"meta":{},"created_at":"2025-10-13T10:55:30.642470Z","updated_at":"2025-10-22T05:34:08.546081Z","inner_id":19,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1730,"annotations":[{"id":759,"completed_by":1,"result":[{"id":"pIYwqsA9nm","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"iJNsykNItl","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"O5hCWN3WnK","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"c-LE3ygPTC","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"EDFmGVvC03","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:01:18.974199Z","updated_at":"2025-10-22T06:01:18.974213Z","draft_created_at":"2025-10-22T06:01:01.970200Z","lead_time":34.67,"prediction":{"id":170,"task":1730,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764637Z","updated_at":"2025-10-13T10:55:30.764640Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"81e431e8-f545-4054-aef1-e9d3204c35ae","import_id":null,"last_action":null,"bulk_created":false,"task":1730,"project":9,"updated_by":1,"parent_prediction":170,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[170],"data":{"id":711,"query":"Is there a correlation between the voltage core and the TDP of these mobile microprocessors? Answer with only 'Yes' or 'No' that is most accurate and nothing else.","dataset":"CRT-QA","task_specification_analysis":"The user requests an assessment of whether there is a correlation between two variables (core voltage and TDP) and wants a binary Yes\/No answer. This is a clear analytical action: evaluate association between two specified variables.","scope_specification_analysis":"Key methodological parameters are not provided: the correlation metric (Pearson, Spearman, Kendall), the decision criterion for answering Yes (e.g., statistical significance threshold like p<0.05, or a magnitude threshold for the coefficient), and treatment of potential issues (e.g., handling ties\/outliers). These choices can materially change the result, so the procedure is ambiguous without further guidance.","domain_specification_analysis":"The boundary is dataset-dependent (\"these\" microprocessors). Without the referenced table\/context, the domain is unclear in an open-domain setting.","entity_specification_analysis":"The set of entities is \"these mobile microprocessors,\" which is deictic and depends on an unspecified list\/table; models and vendors are unknown. Attributes (core voltage, TDP) are clear but the target entities are not.","temporal_specification_analysis":"No time frame is implied or needed; the query concerns a cross-sectional correlation independent of time."},"meta":{},"created_at":"2025-10-13T10:55:30.642502Z","updated_at":"2025-10-22T06:01:19.023600Z","inner_id":20,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1731,"annotations":[{"id":835,"completed_by":1,"result":[{"id":"wv17nc7Gx7","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"RpmXm3bye_","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"10r1ddIcht","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Jqzpv2ytQZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"XgMUuMtR99","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:56:37.295292Z","updated_at":"2025-10-22T14:56:37.295304Z","draft_created_at":"2025-10-22T14:56:27.243204Z","lead_time":23.223,"prediction":{"id":171,"task":1731,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764652Z","updated_at":"2025-10-13T10:55:30.764654Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"d5e64a3f-a875-4a71-bc89-275b4ceb5d67","import_id":null,"last_action":null,"bulk_created":false,"task":1731,"project":9,"updated_by":1,"parent_prediction":171,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[171],"data":{"id":1156,"query":"Please apply a delta hedging approach to our IBM portfolio using European put options.","dataset":"DA-Code","task_specification_analysis":"The user requests performing a delta hedging approach on an IBM portfolio; the overarching analytical action (implement delta hedging) is clear and executable in principle.","scope_specification_analysis":"Critical methodological details are missing: target hedge objective (exact delta-neutral vs tolerance band), instrument selection within European puts (strike(s), maturity(ies), single-series vs multi-series), pricing\/greeks model for delta (e.g., Black–Scholes with dividends vs alternative), inputs (implied\/historical vol, risk-free rate, dividend yield), rebalancing policy (time-based vs threshold-based frequency), transaction cost\/slippage assumptions, position and lot-size constraints, whether shorting options is allowed, how to handle residual delta with stock if puts alone are insufficient, and the evaluation horizon. These choices materially affect implementation and outcomes.","domain_specification_analysis":"The relevant domain boundary is the user's own portfolio\/account context, which is not accessible or defined in an open-domain setting. While IBM’s listing venue is inferable, the portfolio context is essential and missing, making the domain ambiguous.","entity_specification_analysis":"IBM (International Business Machines Corp.) is identifiable, and \"European put options\" is a well-defined instrument class. However, \"our IBM portfolio\" is not defined (holdings, quantities, existing derivatives), and the specific option contracts (strikes, expiries) required for delta hedging are not specified. Multiple plausible interpretations exist, so the core entities needed to act are missing.","temporal_specification_analysis":"No time frame is provided. For an execution-style request like delta hedging, a reasonable default is to use the most recent market data (spot, option chain, Greeks). It could also be a historical\/backtest request, but that intent is not stated."},"meta":{},"created_at":"2025-10-13T10:55:30.642534Z","updated_at":"2025-10-22T14:56:37.338935Z","inner_id":21,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1732,"annotations":[{"id":717,"completed_by":1,"result":[{"id":"Lq0rnBIxLc","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"wtfgOmTLQD","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"cPLVIBgarK","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"t1gWbYXVWN","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"4uE_Dhi8Mi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:53:45.788002Z","updated_at":"2025-10-21T22:53:45.788014Z","draft_created_at":"2025-10-21T22:53:26.430105Z","lead_time":32.762,"prediction":{"id":172,"task":1732,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764667Z","updated_at":"2025-10-13T10:55:30.764669Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2863d096-6d95-4c98-8ff1-b8c854b911f8","import_id":null,"last_action":null,"bulk_created":false,"task":1732,"project":9,"updated_by":1,"parent_prediction":172,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[172],"data":{"id":1052,"query":"Use the mean to fill in missing values, then identify the countries with the highest and lowest population densities in the dataset.","dataset":"DA-Code","task_specification_analysis":"The user requests two concrete steps: (1) impute missing values using the mean, and (2) identify the countries with the highest and lowest population densities. This is an executable workflow (impute then find extrema).","scope_specification_analysis":"The method is sufficiently clear: apply mean imputation (reasonable default is column-wise mean over available values) and then select the max and min by the population density metric. Minor implementation details (which numeric columns to impute, whether density is a provided column or derived, and tie handling) can follow standard defaults without changing the intent.","domain_specification_analysis":"The boundary is defined as “in the dataset,” which is an unknown data container. Per the guideline, references to an unspecified dataset make the domain ambiguous.","entity_specification_analysis":"The query’s target class (“countries”) and metric (“population density”) are clear, but the core entity context is an unnamed data container (“the dataset”). Without a specified dataset, the set of countries\/records to operate on cannot be resolved.","temporal_specification_analysis":"Population density is time-varying and datasets typically correspond to specific years or versions. No time frame is given and the dataset version\/year is unknown, so timing cannot be inferred."},"meta":{},"created_at":"2025-10-13T10:55:30.642568Z","updated_at":"2025-10-21T22:53:45.833173Z","inner_id":22,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1733,"annotations":[{"id":805,"completed_by":1,"result":[{"id":"vDG-d0Df0g","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"cVO8duko3b","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"kCDeKISUXB","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Ae0VUXhwT-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"FXrXGJE4iM","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:27:57.616918Z","updated_at":"2025-10-22T14:27:57.616931Z","draft_created_at":"2025-10-22T14:27:36.132559Z","lead_time":32.056,"prediction":{"id":173,"task":1733,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764681Z","updated_at":"2025-10-13T10:55:30.764684Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"1b0eb9de-1277-48b7-a4ef-a3cd8359e3c3","import_id":null,"last_action":null,"bulk_created":false,"task":1733,"project":9,"updated_by":1,"parent_prediction":173,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[173],"data":{"id":1197,"query":"Calculate the monthly total sales volume for each bike category.","dataset":"DA-Code","task_specification_analysis":"Aggregate sales to compute the total sales volume per month for each bike category. This is a clear grouping-and-summing task.","scope_specification_analysis":"Group records by bike category and by calendar month extracted from the transaction\/sale date; compute the sum of the sales volume field within each (month, category) group. Assume standard calendar months and a simple sum aggregation.","domain_specification_analysis":"No organizational or geographic boundary is provided. Sales volume requires a context such as a specific company, retailer, or market\/region to locate relevant tables.","entity_specification_analysis":"Core subjects like \"bike category\" and \"sales\" are not tied to any specific catalog or data source. The entity whose sales are being analyzed (company\/market) is unstated, and \"sales volume\" could mean units or revenue.","temporal_specification_analysis":"\"Monthly\" gives granularity but no time frame or as-of date. In an open-domain setting, the range cannot be inferred and needs an explicit period (e.g., 2022–2024) or a dataset-bound scope."},"meta":{},"created_at":"2025-10-13T10:55:30.642600Z","updated_at":"2025-10-22T14:27:57.670521Z","inner_id":23,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1734,"annotations":[{"id":780,"completed_by":1,"result":[{"id":"jX6el8pS4b","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"r8wVRpeMPJ","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"m7HK_W12uy","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"AzXpJvXTjm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"GFJ-pztA0o","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:15:25.223129Z","updated_at":"2025-10-22T06:15:25.223139Z","draft_created_at":"2025-10-22T06:15:09.695406Z","lead_time":32.924,"prediction":{"id":174,"task":1734,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764696Z","updated_at":"2025-10-13T10:55:30.764699Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"48f4c182-a4c4-4fca-8505-fde9ad946eed","import_id":null,"last_action":null,"bulk_created":false,"task":1734,"project":9,"updated_by":1,"parent_prediction":174,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[174],"data":{"id":1128,"query":"Follow the data schema to add new columns.","dataset":"DA-Code","task_specification_analysis":"The user requests an executable operation: modify the dataset by adding columns to align with a referenced schema.","scope_specification_analysis":"Key procedural details are missing: which table(s) to modify; the exact column names; data types and constraints (nullable, unique, primary\/foreign keys); default values or formulas for backfilling; source fields for derived columns; handling of existing rows and nulls; column order and indexing. Without these parameters from the schema (or an explicit schema provided), the transformation cannot be unambiguously executed.","domain_specification_analysis":"No organizational or system boundary is provided (which database, application, or project). The scope is entirely ambiguous.","entity_specification_analysis":"The query mentions \"data schema\" and \"new columns\" without identifying which schema, table, or dataset; the core entities (specific table(s)\/fields) are missing, making it impossible to locate relevant data.","temporal_specification_analysis":"This is a procedural instruction with no temporal component; no time frame is required or implied."},"meta":{},"created_at":"2025-10-13T10:55:30.642631Z","updated_at":"2025-10-22T06:15:25.262390Z","inner_id":24,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1735,"annotations":[{"id":768,"completed_by":1,"result":[{"id":"GVxWp2gdw-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"UAzyyqpWe5","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"7XGa3lLux3","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"1gEUwALQ8X","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"OsIZL9x1iM","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:06:51.899273Z","updated_at":"2025-10-22T06:07:01.078613Z","draft_created_at":"2025-10-22T06:06:40.377697Z","lead_time":25.117,"prediction":{"id":175,"task":1735,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764711Z","updated_at":"2025-10-13T10:55:30.764714Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"5bf074c4-bd8c-49e3-bed9-22920cd05649","import_id":null,"last_action":null,"bulk_created":false,"task":1735,"project":9,"updated_by":1,"parent_prediction":175,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[175],"data":{"id":1498,"query":"Join the competition focusing on U.","dataset":"DA-Code","task_specification_analysis":"The query is an imperative (“Join the competition focusing on U”) rather than an analytical request. It does not ask to list, count, rank, calculate, or model anything. No clear analytical goal is stated.","scope_specification_analysis":"No calculation, metric, grouping, ranking criterion, or method is provided. The phrase “focusing on U” is ambiguous (what is ‘U’ and how should focus be applied—filter, weight, rank?). There are no procedural details to execute.","domain_specification_analysis":"No domain boundary is provided (field, geography, organization, or platform). \"Competition\" could span sports, academics, coding, marketing, etc., and \"focusing on U\" does not resolve the context.","entity_specification_analysis":"Core entities are unclear. \"competition\" is unnamed and could refer to any event; \"U\" is ambiguous (could be the letter U, university, uranium, a grade level\/category, etc.). No unique identifiers or well-defined class are provided to locate relevant data.","temporal_specification_analysis":"No timeframe is given. Since \"join\" implies a specific registration period or event date, a time context is required and cannot be safely inferred as the latest."},"meta":{},"created_at":"2025-10-13T10:55:30.642663Z","updated_at":"2025-10-22T06:07:01.134171Z","inner_id":25,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1736,"annotations":[{"id":773,"completed_by":1,"result":[{"id":"usf30aj-GO","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"GrCnd8U008","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"edElGdsN_Z","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"4cSoucbRHj","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"IMi74EUYWC","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:10:26.227792Z","updated_at":"2025-10-22T06:10:26.227806Z","draft_created_at":"2025-10-22T06:10:12.792762Z","lead_time":25.828,"prediction":{"id":176,"task":1736,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764726Z","updated_at":"2025-10-13T10:55:30.764728Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"d3e8e730-a779-49bb-9e7f-119a49aef3b4","import_id":null,"last_action":null,"bulk_created":false,"task":1736,"project":9,"updated_by":1,"parent_prediction":176,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[176],"data":{"id":1370,"query":"You are working on a project for an economic research institute.","dataset":"DA-Code","task_specification_analysis":"The prompt provides only context (“working on a project for an economic research institute”) and does not state any analytical action (e.g., list, count, compare, model). No explicit goal or question is posed.","scope_specification_analysis":"No calculations, metrics, groupings, ranking criteria, or methodological details are provided. There are no variables, outputs, or procedures to execute.","domain_specification_analysis":"Although it hints at the economics field, there is no actionable domain boundary (e.g., geography, sector, dataset). The context is insufficient to select relevant tables.","entity_specification_analysis":"No concrete data entity is mentioned; this is merely a scenario statement with no target subject to resolve.","temporal_specification_analysis":"No time frame is implied or needed because there is no actual data request; the statement is not temporal."},"meta":{},"created_at":"2025-10-13T10:55:30.642694Z","updated_at":"2025-10-22T06:10:26.283328Z","inner_id":26,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1737,"annotations":[{"id":815,"completed_by":1,"result":[{"id":"hTdblyF0gn","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"6p_KFbH-0_","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"},{"id":"opvaZRZx6i","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"fPifoX8pwr","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"0IeYtuLssx","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:33:24.036167Z","updated_at":"2025-10-22T14:33:24.036179Z","draft_created_at":"2025-10-22T14:33:13.899552Z","lead_time":21.176,"prediction":{"id":177,"task":1737,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764740Z","updated_at":"2025-10-13T10:55:30.764743Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"e9fd543d-bd1d-4a94-b88e-a85a000d764a","import_id":null,"last_action":null,"bulk_created":false,"task":1737,"project":9,"updated_by":1,"parent_prediction":177,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[177],"data":{"id":1252,"query":"You are working on a project for an aviation industry analytics firm.","dataset":"DA-Code","task_specification_analysis":"The statement provides only context and does not request any analytical action (e.g., list, compute, compare, model). There is no clear goal or operation to perform.","scope_specification_analysis":"No calculations, metrics, filters, groupings, ranking criteria, or methods are specified. There is no definition of what to analyze or how to analyze it.","domain_specification_analysis":"Although it mentions the aviation industry, the boundary is too vague for data discovery (e.g., civil vs. military, commercial vs. general aviation, global vs. regional). Without a specific domain context, the scope remains ambiguous.","entity_specification_analysis":"No concrete data subjects are provided. The statement gives only scenario context; it does not specify which entities (e.g., airlines, airports, flights, fleets, manufacturers) to analyze.","temporal_specification_analysis":"No time frame is indicated, and there is no implicit all-time or recency context. Any aviation analysis would require a defined period (e.g., specific year(s), quarter, or date range)."},"meta":{},"created_at":"2025-10-13T10:55:30.642727Z","updated_at":"2025-10-22T14:33:24.080352Z","inner_id":27,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1738,"annotations":[{"id":753,"completed_by":1,"result":[{"id":"Y_CsdMH5hE","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"886mzWyeLU","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"4pbjeEiuBm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"lm05772tDY","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"WPwcPXxDBn","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:57:28.428141Z","updated_at":"2025-10-22T05:57:28.428154Z","draft_created_at":"2025-10-22T05:56:16.639710Z","lead_time":21.759,"prediction":{"id":178,"task":1738,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764769Z","updated_at":"2025-10-13T10:55:30.764772Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"78363f25-58ed-4596-8df1-676a60273e5f","import_id":null,"last_action":null,"bulk_created":false,"task":1738,"project":9,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[178],"data":{"id":1159,"query":"Using the Daily Temperature of Major Cities dataset, plot the yearly average temperatures (in Celsius) for Karachi and Islamabad from 1995 to 2019.","dataset":"DA-Code","task_specification_analysis":"The user clearly requests creating a plot of yearly average temperatures for two specified cities over a defined period. The analytical action (plot) is explicit.","scope_specification_analysis":"The operations are clear: filter to Karachi and Islamabad; restrict years 1995–2019; ensure temperatures are in Celsius; group by city and year; compute the mean of daily temperatures per year; plot (reasonably as a line chart) with year on x-axis and average temperature on y-axis for both cities. These are standard defaults and sufficiently specified.","domain_specification_analysis":"Geographical boundary is the two specific cities; no additional organizational or contextual boundary is needed.","entity_specification_analysis":"Core entities are the cities Karachi and Islamabad, which are uniquely identifiable. The specified dataset title is likely resolvable (commonly a Kaggle dataset) and does not introduce ambiguity for the target entities or measure. The metric (yearly average temperature) is derivable from daily data and unit (Celsius) is specified.","temporal_specification_analysis":"Explicit range given: 1995 to 2019."},"meta":{},"created_at":"2025-10-13T10:55:30.642762Z","updated_at":"2025-10-22T05:57:28.469147Z","inner_id":28,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1739,"annotations":[{"id":752,"completed_by":1,"result":[{"id":"eyDDmU8J4p","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"VxTaLYTgbe","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"6N-aBNGisX","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"st7QcT8BYb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"G-Mzp03GFF","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:56:01.369186Z","updated_at":"2025-10-22T05:56:01.369206Z","draft_created_at":"2025-10-22T05:55:09.870108Z","lead_time":59.217,"prediction":{"id":179,"task":1739,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764784Z","updated_at":"2025-10-13T10:55:30.764787Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"013129b2-3621-4ead-a054-d2856516ffca","import_id":null,"last_action":null,"bulk_created":false,"task":1739,"project":9,"updated_by":1,"parent_prediction":179,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[179],"data":{"id":1177,"query":"Analyze the dataset to identify the top 20 teams based on their average score.","dataset":"DA-Code","task_specification_analysis":"The user requests a clear analytic action: rank teams and return the top 20 by a defined metric (average score). This is a concrete, executable task (ranking and selection).","scope_specification_analysis":"The required calculations are sufficiently defined: group records by team, compute the (unweighted) arithmetic mean of the score for each team, sort descending by that mean, and take the top 20. Any minor tie-breaking defaults can be assumed without materially changing intent.","domain_specification_analysis":"The query relies on \"the dataset\" as the contextual boundary and does not specify sport, competition, geography, or organization. This is ambiguous in an open-domain context.","entity_specification_analysis":"The core subjects are \"teams\" from \"the dataset,\" but the dataset is not identified and no sport\/league\/context is given. Without a specified dataset or domain, the entities cannot be located in an open-domain setting.","temporal_specification_analysis":"No time frame is provided. The phrase implicitly ties timing to \"the dataset,\" which is unknown; per guidance, container-based timing makes the query ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.642793Z","updated_at":"2025-10-22T05:56:01.446206Z","inner_id":29,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1740,"annotations":[{"id":736,"completed_by":1,"result":[{"id":"vYZ2RiikBR","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"orq0GaPjwO","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"},{"id":"_KMroEGK-V","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Tnep5FJ49N","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"RIC4NHEOEb","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:41:54.735034Z","updated_at":"2025-10-22T05:41:54.735047Z","draft_created_at":"2025-10-22T05:41:35.759044Z","lead_time":34.63,"prediction":{"id":180,"task":1740,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764799Z","updated_at":"2025-10-13T10:55:30.764802Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"5cdd5d40-27ba-4964-b06f-8e64dbdb289a","import_id":null,"last_action":null,"bulk_created":false,"task":1740,"project":9,"updated_by":1,"parent_prediction":180,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[180],"data":{"id":1257,"query":"You are analyzing the Synthetic Indian Automobile Crash Data dataset.","dataset":"DA-Code","task_specification_analysis":"No explicit analytical action is requested. The query only names a dataset without specifying what to compute, compare, list, summarize, or model.","scope_specification_analysis":"No methodological details are provided—no metrics, filters, groupings, aggregations, ranking criteria, or models. There is no definition of outputs or calculations.","domain_specification_analysis":"The domain is defined only by referring to a specific dataset (the data container). In an open-domain setting this is ambiguous (multiple versions\/sources, unclear which table), and per guidance, dataset-referential boundaries are not sufficient.","entity_specification_analysis":"The only concrete entity mentioned is the named dataset \"Synthetic Indian Automobile Crash Data,\" which is likely uniquely identifiable (e.g., a Kaggle dataset). No additional entities (e.g., states, vehicle types) are requested.","temporal_specification_analysis":"No time frame is requested or implied; the statement is merely contextual and not asking a time-dependent metric."},"meta":{},"created_at":"2025-10-13T10:55:30.642824Z","updated_at":"2025-10-22T05:41:54.787777Z","inner_id":30,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1741,"annotations":[{"id":764,"completed_by":1,"result":[{"id":"bkz26Y3zIM","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Ovmy7NF1lg","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"MjbHA0Kx6_","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"efkmUEzg_G","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"qgyc3bb-h4","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:04:32.951973Z","updated_at":"2025-10-22T06:04:32.951985Z","draft_created_at":"2025-10-22T06:04:08.892560Z","lead_time":37.658,"prediction":{"id":181,"task":1741,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764815Z","updated_at":"2025-10-13T10:55:30.764817Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"c7bce09b-fe16-4bc9-8557-07dc9e2d6427","import_id":null,"last_action":null,"bulk_created":false,"task":1741,"project":9,"updated_by":1,"parent_prediction":181,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[181],"data":{"id":1753,"query":"2. Is there a correlation between the maximum temperature (TMAX_F) and the observation values (obs_value)? If yes, what is the correlation coefficient?","dataset":"DA-Eval","task_specification_analysis":"The query asks to determine whether a correlation exists between TMAX_F and obs_value and, if so, to provide the correlation coefficient—an explicit analytical action.","scope_specification_analysis":"The method can reasonably default to computing Pearson’s correlation coefficient between two numeric variables and assessing significance with a conventional threshold (e.g., p < 0.05). No additional grouping, filtering, or ranking parameters are required.","domain_specification_analysis":"The contextual boundary (which dataset, source, geography, station, or study) is missing. Without a defined domain or data source, an open-domain system cannot identify the relevant tables.","entity_specification_analysis":"The query references columns\/variables named TMAX_F and obs_value without identifying the specific dataset\/table or source. These names are non-unique and could refer to many possible entities across different datasets.","temporal_specification_analysis":"No timeframe is given, and the scope depends on the (unspecified) dataset. Correlation could be computed over different time windows or the entire dataset, but the intended period is unclear and tied to an unknown data container."},"meta":{},"created_at":"2025-10-13T10:55:30.642856Z","updated_at":"2025-10-22T06:04:32.995581Z","inner_id":31,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1742,"annotations":[{"id":808,"completed_by":1,"result":[{"id":"DL_zlrHv8N","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"},{"id":"QN6uVkv-ko","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"vKPhQHGz1u","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"oWAX0Z8Vdh","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"1h0nJoY9_l","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:29:20.653651Z","updated_at":"2025-10-22T14:29:20.653662Z","draft_created_at":"2025-10-22T14:29:08.071334Z","lead_time":26.098,"prediction":{"id":182,"task":1742,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764830Z","updated_at":"2025-10-13T10:55:30.764833Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"99bd2147-d638-4fa8-bb56-7e1f3830033a","import_id":null,"last_action":null,"bulk_created":false,"task":1742,"project":9,"updated_by":1,"parent_prediction":182,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[182],"data":{"id":1626,"query":"Are there any outliers in the fare paid by the passengers? If so, how many outliers are there and what is their range?","dataset":"DA-Eval","task_specification_analysis":"The query asks to determine whether outliers exist in a numeric variable (fare), count them, and report their range—an explicit analytical action.","scope_specification_analysis":"The definition of 'outlier' is not specified (e.g., Tukey 1.5×IQR, 3×IQR, z-score > 3, modified z-score\/MAD, percentile cutoffs), which would materially change the result. It’s also unclear whether 'range' means the min and max of outlier values or the span (max−min), whether to consider one- or two-sided outliers, and how to handle missing\/zero fares or transformations.","domain_specification_analysis":"No geographic, organizational, or modal boundary is given. \"Passengers\" could belong to any transport domain or dataset; the context is missing.","entity_specification_analysis":"Core entities \"fare\" and \"passengers\" are not tied to any specific dataset, carrier, mode (airline, rail, maritime, rideshare), route, or population. Multiple plausible interpretations exist (e.g., Titanic dataset, airline passengers, bus riders).","temporal_specification_analysis":"No time frame is provided and the relevant period depends entirely on the unspecified dataset\/population. This is not a standard \"assume latest\" query and could refer to historical or current data."},"meta":{},"created_at":"2025-10-13T10:55:30.642887Z","updated_at":"2025-10-22T14:29:20.693563Z","inner_id":32,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1743,"annotations":[{"id":765,"completed_by":1,"result":[{"id":"RTNkHh4YxF","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"5cibTVkzEy","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"siqXUwNgba","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"fmyDZMtU6w","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"3eNVpk0upm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:05:01.402063Z","updated_at":"2025-10-22T06:05:01.402073Z","draft_created_at":"2025-10-22T06:04:46.529920Z","lead_time":27.795,"prediction":{"id":183,"task":1743,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764845Z","updated_at":"2025-10-13T10:55:30.764848Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"c8087ca0-5cac-4278-98c1-f994e1872e82","import_id":null,"last_action":null,"bulk_created":false,"task":1743,"project":9,"updated_by":1,"parent_prediction":183,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[183],"data":{"id":1755,"query":"5. Calculate the median and range of the maximum temperature (TMAX_F) for each type of observation (obs_type) recorded in the dataset. Are there any differences in the median and range between different observation types?","dataset":"DA-Eval","task_specification_analysis":"The query asks to compute two specific summary statistics (median and range) of a defined variable (TMAX_F) grouped by a defined category (obs_type), and to compare those results across groups. This is a clear, executable analytical action.","scope_specification_analysis":"Operations are fully defined: group by obs_type; calculate median(TMAX_F) and range(TMAX_F) using the standard definition (max - min), ignoring missing values by default; then note whether the statistics differ across obs_type groups. No further parameters are required.","domain_specification_analysis":"The contextual boundary is defined only as \"in the dataset,\" which is unknown. It could refer to any meteorological dataset, region, or organization. Thus, the domain boundary cannot be determined.","entity_specification_analysis":"The query depends on a specific, unnamed dataset and its columns ('TMAX_F' and 'obs_type'). Without identifying which dataset\/table this refers to, a researcher cannot locate the relevant data. Many datasets could contain similarly named fields, so the core subjects are ambiguous.","temporal_specification_analysis":"No time frame is given and the scope is tied to \"in the dataset.\" Per guidance, references to an unspecified data container make the time context ambiguous; the dataset could cover any period."},"meta":{},"created_at":"2025-10-13T10:55:30.642919Z","updated_at":"2025-10-22T06:05:01.449382Z","inner_id":33,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1744,"annotations":[{"id":718,"completed_by":1,"result":[{"id":"zwRldvPtFL","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"4FSeEGDmTb","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"},{"id":"TyPG0CK0Qq","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"C8oerVCRVp","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"p7iS4nVYWH","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:55:07.796326Z","updated_at":"2025-10-21T22:55:07.796339Z","draft_created_at":"2025-10-21T22:54:10.849949Z","lead_time":81.266,"prediction":{"id":184,"task":1744,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764861Z","updated_at":"2025-10-13T10:55:30.764864Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"888e06a3-2dad-4f75-9b2c-5045cf61b9a5","import_id":null,"last_action":null,"bulk_created":false,"task":1744,"project":9,"updated_by":1,"parent_prediction":184,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[184],"data":{"id":1633,"query":"3. Perform comprehensive data preprocessing on the trading volume column. Handle any missing values and transform the data to a suitable format for further analysis.","dataset":"DA-Eval","task_specification_analysis":"The user asks to perform data preprocessing on the trading volume column, explicitly mentioning handling missing values and transforming the data for further analysis. This constitutes a clear high-level analytical action (preprocess).","scope_specification_analysis":"Key methodological details are missing: the strategy for handling missing values (drop vs. impute; if impute, which method), the specific transformations desired (e.g., log transform, scaling\/normalization, winsorization, outlier treatment), target data type\/format, and any temporal aggregation or smoothing. These choices materially affect the outcome and cannot be assumed without guidance.","domain_specification_analysis":"\"Trading volume\" could pertain to stocks, crypto, commodities, or other markets, and the dataset\/table context is unknown. Without a domain boundary (e.g., exchange, asset class, or specific dataset), the scope is ambiguous.","entity_specification_analysis":"The core subject \"trading volume column\" refers to a column in an unspecified dataset\/table. No dataset name, instrument (stock\/crypto\/etc.), or column identifier is provided, so a researcher cannot locate the specific entity.","temporal_specification_analysis":"This is a procedural preprocessing task, not a time-bound inquiry. Although trading volume is typically time-series, no time frame is needed or implied for the instruction."},"meta":{},"created_at":"2025-10-13T10:55:30.642950Z","updated_at":"2025-10-21T22:55:07.846785Z","inner_id":34,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1745,"annotations":[{"id":831,"completed_by":1,"result":[{"id":"-Rc4xbB6wx","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"},{"id":"4Tj_f0hn-f","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"urLhEx3FCm","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"dmj1N40vQw","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"eBWtFeFNQH","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:54:07.746764Z","updated_at":"2025-10-22T14:54:07.746778Z","draft_created_at":"2025-10-22T14:53:49.037124Z","lead_time":41.09,"prediction":{"id":185,"task":1745,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764876Z","updated_at":"2025-10-13T10:55:30.764879Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"b39ffc97-458d-4cfa-aeee-5029bbfeb368","import_id":null,"last_action":null,"bulk_created":false,"task":1745,"project":9,"updated_by":1,"parent_prediction":185,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[185],"data":{"id":1701,"query":"1. Identify and remove any outliers in the duration of the trajectories based on the Z-score method where an outlier is defined as a data point that is located outside the whiskers of the box plot (a data point is considered to be an outlier if its z-score is less than -2.5 or greater than 2.5). Calculate the new mean and standard deviation of the trajectory durations after removing the outliers.","dataset":"DA-Eval","task_specification_analysis":"The user explicitly requests detecting outliers using the Z-score method with a threshold of |z| > 2.5, removing those points, and then computing the mean and standard deviation of the remaining trajectory durations. This is a clear, actionable analytical task.","scope_specification_analysis":"The procedure is sufficiently detailed: compute z-scores for the trajectory duration field, flag values with z < -2.5 or z > 2.5 as outliers, remove them, and recompute mean and standard deviation on the filtered data. While the mention of box-plot whiskers is conceptually inconsistent with Z-scores, the explicit ±2.5 Z-score criterion provides a clear rule to follow.","domain_specification_analysis":"The contextual boundary (which project, dataset, organization, geography) is missing. The instruction depends on a specific dataset that is not identified, so a researcher could not select the correct tables.","entity_specification_analysis":"The query refers to \"duration of the trajectories\" and \"trajectory durations\" without identifying which dataset, source, or specific trajectories. These entities depend entirely on an unspecified data table\/column and cannot be resolved in an open-domain setting.","temporal_specification_analysis":"No time frame is relevant or referenced; the task is a statistical cleaning\/computation on a dataset irrespective of time."},"meta":{},"created_at":"2025-10-13T10:55:30.642981Z","updated_at":"2025-10-22T14:54:07.796374Z","inner_id":35,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1746,"annotations":[{"id":552,"completed_by":1,"result":[{"id":"lOIpiAZzVM","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"E0MNgixgr9","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"gTNk20fM07","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"YXoQCjza6d","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"gIwQ2HiWzn","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-13T11:07:17.642410Z","updated_at":"2025-10-13T11:07:17.642423Z","draft_created_at":"2025-10-13T11:06:57.073374Z","lead_time":33.97,"prediction":{"id":186,"task":1746,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764891Z","updated_at":"2025-10-13T10:55:30.764894Z","created_ago":"11 minutes","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"79a3cbb4-f40e-4999-be9d-0853a060ab3f","import_id":null,"last_action":null,"bulk_created":false,"task":1746,"project":9,"updated_by":1,"parent_prediction":186,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[186],"data":{"id":1604,"query":"Are there any outliers in the SCOREMARGIN column? If so, how many?","dataset":"DA-Eval","task_specification_analysis":"The user wants to determine whether outliers exist in a specific numeric column and to count them—this is a clear, executable goal (detect and count).","scope_specification_analysis":"The method to define and detect outliers is not specified (e.g., IQR rule with 1.5×IQR or 3×IQR, z-score threshold such as |z|>3, MAD\/robust z-score, percentile-based, or time-series anomaly detection). Thresholds, whether to use one- or two-sided detection, and handling of NaNs are unspecified. Different choices materially change the result.","domain_specification_analysis":"The contextual boundary (which dataset\/table) is missing; referring to \"the column\" in an unspecified dataset makes the domain ambiguous in an open-domain setting.","entity_specification_analysis":"References \"the SCOREMARGIN column\" without identifying the dataset or table; many possible columns could match, so the core entity cannot be resolved.","temporal_specification_analysis":"This is a statistical property of a column; no timeframe is relevant or implied."},"meta":{},"created_at":"2025-10-13T10:55:30.643015Z","updated_at":"2025-10-13T11:07:17.699292Z","inner_id":36,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1747,"annotations":[{"id":794,"completed_by":1,"result":[{"id":"bNHFhSngd2","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"EUcU_DbKPm","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"XSRFy-ENtc","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"ISd9KHUG2Q","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"9naY3hZ_-b","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:18:40.386428Z","updated_at":"2025-10-22T14:18:40.386437Z","draft_created_at":"2025-10-22T14:16:04.863096Z","lead_time":169.123,"prediction":{"id":187,"task":1747,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764906Z","updated_at":"2025-10-13T10:55:30.764909Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a6276eea-d67c-4ae0-abf6-e010f9597f83","import_id":null,"last_action":null,"bulk_created":false,"task":1747,"project":9,"updated_by":1,"parent_prediction":187,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[187],"data":{"id":1545,"query":"Is there a linear relationship between the GDP per capita and the life expectancy score in the dataset? Conduct linear regression and use the resulting coefficient of determination (R-squared) to evaluate the model's goodness of fit.","dataset":"DA-Eval","task_specification_analysis":"The query clearly instructs to test for a linear relationship by running a linear regression and to evaluate goodness of fit using R-squared. This defines a concrete analytical action.","scope_specification_analysis":"The variables to use (GDP per capita and life expectancy score) are identified, the method (linear regression) and evaluation metric (R-squared) are specified. Standard defaults (OLS with intercept) suffice; in simple regression R-squared is invariant to swapping dependent\/independent. No critical methodological parameter is missing.","domain_specification_analysis":"The query does not specify the contextual boundary (e.g., countries worldwide, a region, specific population, or organizational dataset). Referencing 'the dataset' provides no global or unique boundary, making the domain unclear.","entity_specification_analysis":"The core subjects are variables ('GDP per capita' and 'life expectancy score') referenced only as existing 'in the dataset.' The dataset itself is not identified, and variable definitions\/units may differ across datasets. Without a named dataset, the entities cannot be resolved.","temporal_specification_analysis":"No time frame is provided. Both GDP per capita and life expectancy are time-dependent, and the query ties scope to an unspecified dataset. Per the guideline, references to 'in the dataset' make the temporal scope ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.643046Z","updated_at":"2025-10-22T14:18:40.422995Z","inner_id":37,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1748,"annotations":[{"id":709,"completed_by":1,"result":[{"id":"9YLLY3PYa9","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"uIQNKhF2jR","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Am5avzcv-z","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"97BA5zbwOM","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"N3Q6AN2Ba5","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:48:18.860704Z","updated_at":"2025-10-21T22:48:37.494283Z","draft_created_at":"2025-10-21T22:46:53.490553Z","lead_time":82.61200000000001,"prediction":{"id":188,"task":1748,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764922Z","updated_at":"2025-10-13T10:55:30.764924Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"0a015f99-9c7f-4859-a42e-7ddeab78555c","import_id":null,"last_action":null,"bulk_created":false,"task":1748,"project":9,"updated_by":1,"parent_prediction":null,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[188],"data":{"id":1528,"query":"Are there any outliers in the \"No. of deaths_max\" column for each country? How do these outliers affect the overall distribution of recorded deaths?","dataset":"DA-Eval","task_specification_analysis":"The user asks to identify outliers in a specific numeric column by country and to evaluate how those outliers impact the distribution of recorded deaths. This constitutes a clear analytical goal (outlier detection plus impact assessment).","scope_specification_analysis":"Key methodological details are missing: the definition of an outlier (e.g., IQR\/Tukey fences, z-score, MAD) and thresholds; whether outliers are flagged within-country only (implied) and how the “overall distribution” should be summarized (e.g., effects on mean, median, variance, skewness, kurtosis, or via visual comparisons). Also unclear is whether the impact should be assessed by removing or winsorizing outliers, and whether the overall distribution is across all countries or aggregated summaries. These choices materially affect results.","domain_specification_analysis":"Geography implies countries globally, but the conceptual\/organizational boundary (what deaths: COVID-19, all-cause, disasters, etc.) and the data source are not specified; the query relies on an unspecified table.","entity_specification_analysis":"References a dataset-specific column name ('No. of deaths_max') without identifying the dataset\/table. While 'each country' is a broad, resolvable class, the core entity (the column and its parent table) cannot be uniquely identified.","temporal_specification_analysis":"No time frame is provided. 'Max' suggests an aggregation across time within a specific dataset, but without knowing the dataset or period, the intended timeframe cannot be resolved."},"meta":{},"created_at":"2025-10-13T10:55:30.643078Z","updated_at":"2025-10-21T22:48:37.549787Z","inner_id":38,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1749,"annotations":[{"id":750,"completed_by":1,"result":[{"id":"UNml_hjCa-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"W0wmnIn-cJ","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"t9qy8OqfyU","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"9JbZwhsD-o","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"8pXLYL3SNx","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:54:28.470979Z","updated_at":"2025-10-22T05:54:28.470988Z","draft_created_at":"2025-10-22T05:54:15.122877Z","lead_time":21.638,"prediction":{"id":189,"task":1749,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764937Z","updated_at":"2025-10-13T10:55:30.764939Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a7cac2c2-353e-43d0-802d-53b3290f43e4","import_id":null,"last_action":null,"bulk_created":false,"task":1749,"project":9,"updated_by":1,"parent_prediction":189,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[189],"data":{"id":1712,"query":"Check if the 'Volume' column adheres to a normal distribution.","dataset":"DA-Eval","task_specification_analysis":"The user’s goal is clear: determine whether the 'Volume' variable follows a normal distribution.","scope_specification_analysis":"The procedure to assess normality is not defined. Key analytical choices are missing: which normality test or diagnostic to use (e.g., Shapiro–Wilk, Anderson–Darling, Kolmogorov–Smirnov, Jarque–Bera, QQ-plot), the significance level (e.g., α=0.05), and decision criteria. Different choices can yield different conclusions, so the scope is materially underspecified.","domain_specification_analysis":"No contextual boundary (dataset, table, organization, or subject area) is provided, making the request ambiguous in an open-domain setting.","entity_specification_analysis":"The core entity is the 'Volume' column, but no dataset\/table\/source is identified. Many tables may have a 'Volume' field, so the target column cannot be uniquely resolved.","temporal_specification_analysis":"Assessing whether a column follows a normal distribution is not inherently time-bound; no timeframe is required."},"meta":{},"created_at":"2025-10-13T10:55:30.643109Z","updated_at":"2025-10-22T05:54:28.507249Z","inner_id":39,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1750,"annotations":[{"id":712,"completed_by":1,"result":[{"id":"3odeoWuYS0","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"wS0MRPnBpv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"ziuVdNjbub","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"WU8L9lWfcK","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"U0Qqs3WU8-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:49:10.412464Z","updated_at":"2025-10-21T22:49:10.412493Z","draft_created_at":"2025-10-21T22:48:53.129469Z","lead_time":29.098,"prediction":{"id":190,"task":1750,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764952Z","updated_at":"2025-10-13T10:55:30.764955Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"9335149c-bc24-4875-8cae-de166442b186","import_id":null,"last_action":null,"bulk_created":false,"task":1750,"project":9,"updated_by":1,"parent_prediction":190,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[190],"data":{"id":1718,"query":"Check if the MedInc column adheres to a normal distribution in the provided dataset.","dataset":"DA-Eval","task_specification_analysis":"The user clearly asks to assess whether the MedInc column follows a normal distribution (i.e., a normality check). This is a specific analytical action.","scope_specification_analysis":"The query does not specify the method for assessing normality (e.g., Shapiro–Wilk, Anderson–Darling, Kolmogorov–Smirnov with estimated parameters\/Lilliefors), the significance level (alpha), treatment of missing values\/outliers, or whether to rely on statistical tests vs. visual diagnostics (Q–Q plot, histogram). These choices can materially affect the conclusion.","domain_specification_analysis":"The contextual boundary is merely 'the provided dataset,' which is not identified. Without a dataset name or broader domain (e.g., a specific study, repository, or organization), the scope is ambiguous.","entity_specification_analysis":"The core entity is the 'MedInc' column in 'the provided dataset,' but the dataset is not identified. Many datasets may contain a column named 'MedInc' (e.g., housing datasets), so the target column cannot be uniquely resolved without specifying the dataset.","temporal_specification_analysis":"The query concerns a statistical property (distribution shape) of a dataset column and does not depend on a time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.643140Z","updated_at":"2025-10-21T22:49:10.469735Z","inner_id":40,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1751,"annotations":[{"id":801,"completed_by":1,"result":[{"id":"8jGBSL5JUQ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"RPyiRxjB0K","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"V1EYJ3ya-_","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"nImTpTLJT0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"X3WJGcFk8l","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:24:32.309540Z","updated_at":"2025-10-22T14:24:32.309551Z","draft_created_at":"2025-10-22T14:24:19.877182Z","lead_time":22.862,"prediction":{"id":191,"task":1751,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764967Z","updated_at":"2025-10-13T10:55:30.764970Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"def06b1d-a24a-48bb-8a3c-41d0ebe99b95","import_id":null,"last_action":null,"bulk_created":false,"task":1751,"project":9,"updated_by":1,"parent_prediction":191,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[191],"data":{"id":2167,"query":"In the 2005 Pengkalan Pasir by-election, who were the top two candidates and how many votes did each candidate receive?","dataset":"FeTaQA","task_specification_analysis":"The query requests identifying the top two candidates and reporting their vote counts for a specific election. This is a clear retrieval\/ranking task with a concrete output: candidate names and their vote totals.","scope_specification_analysis":"The ranking criterion is implicitly the number of votes (reinforced by asking for how many votes each received). The procedure is clear: sort candidates by votes descending for the 2005 Pengkalan Pasir by-election, select the top two, and return their names and vote counts. Reasonable defaults apply; tie-handling is not critical here.","domain_specification_analysis":"The domain is the specific by-election (Pengkalan Pasir) which provides a clear contextual boundary; no additional geographical or organizational scope is needed.","entity_specification_analysis":"The query targets a unique, well-identified event: the 2005 Pengkalan Pasir by-election, and asks for the top two candidates and their vote counts within that event. This is unambiguous and resolvable.","temporal_specification_analysis":"The year 2005 is explicitly stated, fixing the time frame of the by-election."},"meta":{},"created_at":"2025-10-13T10:55:30.643170Z","updated_at":"2025-10-22T14:24:32.350453Z","inner_id":41,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1752,"annotations":[{"id":734,"completed_by":1,"result":[{"id":"6-ZfWZ-8DT","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"1UaAsc0SvD","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"YvfqKs_D91","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"b5Cctym6ui","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"PBdpDujqZy","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:40:26.394997Z","updated_at":"2025-10-22T05:40:26.395006Z","draft_created_at":"2025-10-22T05:39:48.783599Z","lead_time":53.255,"prediction":{"id":192,"task":1752,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764982Z","updated_at":"2025-10-13T10:55:30.764985Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"30c2585a-72b0-4823-aa5b-c628149bd7d0","import_id":null,"last_action":null,"bulk_created":false,"task":1752,"project":9,"updated_by":1,"parent_prediction":192,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[192],"data":{"id":1881,"query":"What medals did Jo Pavey win in the year 2012,2006 and 2014 at the Commonwealth Games, in what events did he win them?","dataset":"FeTaQA","task_specification_analysis":"The user requests a concrete lookup: identify and list the medals Jo Pavey won at the Commonwealth Games in the specified years, along with the events. This is a straightforward retrieval\/listing task.","scope_specification_analysis":"The required steps are clear: filter records for Jo Pavey at the Commonwealth Games for years 2012, 2006, and 2014, and return medal type(s) and corresponding event(s). No ambiguous calculations or ranking metrics are involved.","domain_specification_analysis":"The domain is constrained to the Commonwealth Games, which is a globally unique, well-defined multi-sport event series.","entity_specification_analysis":"\"Jo Pavey\" uniquely identifies the British long‑distance runner; despite the pronoun error (he\/him), the entity is unambiguous. The medals and events are within her Commonwealth Games career.","temporal_specification_analysis":"Explicit years are given: 2006, 2012, and 2014. Although there was no Commonwealth Games in 2012, the timeframe is still clearly specified by the query."},"meta":{},"created_at":"2025-10-13T10:55:30.643208Z","updated_at":"2025-10-22T05:40:26.437410Z","inner_id":42,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1753,"annotations":[{"id":755,"completed_by":1,"result":[{"id":"tF043mfX7l","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"lAD4b9JUss","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"l5B0Tm9D9o","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Qz3Rzf1K80","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"sGikQTF90E","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:59:18.703096Z","updated_at":"2025-10-22T05:59:18.703105Z","draft_created_at":"2025-10-22T05:59:09.186186Z","lead_time":31.11,"prediction":{"id":193,"task":1753,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.764997Z","updated_at":"2025-10-13T10:55:30.765000Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"78ea2d40-093b-4345-ab71-dae90fcc5083","import_id":null,"last_action":null,"bulk_created":false,"task":1753,"project":9,"updated_by":1,"parent_prediction":193,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[193],"data":{"id":1927,"query":"Which Labour candidate won the 2010 Blaenau Gwent election and what was the swing vote percentage?","dataset":"FeTaQA","task_specification_analysis":"The user asks for two specific facts: the name of the Labour candidate who won and the swing percentage for the 2010 Blaenau Gwent election. This is a direct lookup request with a clearly defined goal.","scope_specification_analysis":"No complex calculations are required; both items are standard fields in election result datasets. While “swing” can have definitions, constituency results typically report a standard swing figure, so a reasonable default applies. The scope is sufficiently clear to retrieve the requested values.","domain_specification_analysis":"The domain is electoral results for the Blaenau Gwent UK parliamentary constituency. The geographical\/organizational boundary is clear and unique.","entity_specification_analysis":"Core entities are clear: the 2010 Blaenau Gwent election (UK parliamentary constituency contest) and the Labour candidate in that race, which resolves to a single person. While the exact definition of “swing” could vary by source, the entities themselves are unambiguous.","temporal_specification_analysis":"The year 2010 is explicitly given, fixing the time frame to the 2010 election cycle."},"meta":{},"created_at":"2025-10-13T10:55:30.643239Z","updated_at":"2025-10-22T05:59:18.740409Z","inner_id":43,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1754,"annotations":[{"id":787,"completed_by":1,"result":[{"id":"KgSLAuCVbc","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Dvff3AL-CF","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"8s7nBb9Mu1","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"ry0P_QOy3m","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"kOY59omNyA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:11:12.473669Z","updated_at":"2025-10-22T14:11:12.473680Z","draft_created_at":"2025-10-22T14:10:37.552395Z","lead_time":83.21,"prediction":{"id":194,"task":1754,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765024Z","updated_at":"2025-10-13T10:55:30.765027Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"61c4377a-6acf-4566-a31a-eeefbb0781d3","import_id":null,"last_action":null,"bulk_created":false,"task":1754,"project":9,"updated_by":1,"parent_prediction":194,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[194],"data":{"id":2055,"query":"When was the RSA-640 project finished and what was the next project?","dataset":"FeTaQA","task_specification_analysis":"The query asks for two concrete facts: the completion date of the RSA-640 project and the identity of the subsequent (next) project. This is a straightforward retrieval task without ambiguous analytical actions.","scope_specification_analysis":"No calculations, ranking, or modeling are required—only lookups. Any ambiguity about what constitutes the “next project” would be a data\/entity definition issue, not a procedural one. Procedurally, the steps are clear: retrieve the finish date and the successor project record.","domain_specification_analysis":"The domain boundary for \"next project\" is missing (it could mean next RSA challenge number, next project by a specific team or platform). Without specifying the organizational or program context, the domain is ambiguous.","entity_specification_analysis":"\"RSA-640\" is a recognizable, unique entity (the RSA Factoring Challenge number). However, \"the next project\" is ambiguous—next according to which sequence or organization (e.g., the RSA Factoring Challenge numbers, the same factoring team’s agenda, NFS@Home project list, etc.)? Without that referent, the core subject of the second part is not resolvable.","temporal_specification_analysis":"The question asks for the completion date of a single historical event (when the RSA-640 effort finished), which is a fixed fact and does not require a timeframe filter."},"meta":{},"created_at":"2025-10-13T10:55:30.643270Z","updated_at":"2025-10-22T14:11:12.512749Z","inner_id":44,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1755,"annotations":[{"id":744,"completed_by":1,"result":[{"id":"HeYyYEiGdf","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"0snrv3T-LE","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"hgm303I6mN","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"P4_gc3ye5t","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"ZY1lwKoemP","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:50:36.154830Z","updated_at":"2025-10-22T05:50:36.154840Z","draft_created_at":"2025-10-22T05:50:18.935929Z","lead_time":29.305,"prediction":{"id":195,"task":1755,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765038Z","updated_at":"2025-10-13T10:55:30.765041Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"32ced3e0-460b-43fa-b45d-d26e859ab8ee","import_id":null,"last_action":null,"bulk_created":false,"task":1755,"project":9,"updated_by":1,"parent_prediction":195,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[195],"data":{"id":1868,"query":"How did Sureyya Ayhan fare at the 2003 World Championships?","dataset":"FeTaQA","task_specification_analysis":"The query requests a straightforward report of an athlete’s performance at a specific event\/date. This is a simple retrieval\/summary task (e.g., placement\/medal\/result) rather than an open-ended analysis.","scope_specification_analysis":"No calculations or methodological choices are required; the expected response is the final outcome (e.g., medal\/placing and possibly event) at the specified championships. This fits a simple lookup with common-sense defaults.","domain_specification_analysis":"Although \"World Championships\" could be multi-sport, the athlete context unambiguously implies the World Athletics Championships. This is a globally unique event edition (2003, Paris).","entity_specification_analysis":"The entities are resolvable: \"Sureyya Ayhan\" uniquely identifies the Turkish middle-distance runner (Süreyya Ayhan), and in context \"2003 World Championships\" maps confidently to the 2003 World Athletics Championships where she competed (women’s 1500m).","temporal_specification_analysis":"The query explicitly specifies the year 2003 and a particular edition of the World Championships, making the timeframe clear."},"meta":{},"created_at":"2025-10-13T10:55:30.643310Z","updated_at":"2025-10-22T05:50:36.196091Z","inner_id":45,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1756,"annotations":[{"id":820,"completed_by":1,"result":[{"id":"kom9_CujTi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"MiNHirSwYz","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"k_kC5agU4A","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"_bEt2kTxAH","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"_qCVtw-GGd","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:37:56.874535Z","updated_at":"2025-10-22T14:37:56.874547Z","draft_created_at":null,"lead_time":8.999,"prediction":{"id":196,"task":1756,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765053Z","updated_at":"2025-10-13T10:55:30.765056Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"3129139e-6c4b-443d-9f83-83a2d82968d3","import_id":null,"last_action":null,"bulk_created":false,"task":1756,"project":9,"updated_by":1,"parent_prediction":196,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[196],"data":{"id":1785,"query":"What frequencies does Love FM have in Fukuoka, Nishi and Kitakyushu, Yamaguchi?","dataset":"FeTaQA","task_specification_analysis":"The user asks to list the broadcast frequencies for Love FM in specified locations. This is a straightforward retrieval\/listing task.","scope_specification_analysis":"No calculations or modeling required; simply extract the frequency values for Love FM at the named locations. Standard interpretation is a direct lookup of frequencies by location.","domain_specification_analysis":"The domain is explicitly the broadcast frequencies of a specific radio network within specified Japanese localities (Fukuoka, Nishi [ward], Kitakyushu, Yamaguchi). This provides a clear geographical\/contextual boundary.","entity_specification_analysis":"Core entity is the radio station\/network \"Love FM.\" Given the Japanese locations (Fukuoka, Kitakyushu, Yamaguchi), this reasonably resolves to the Fukuoka-based Love FM network in Japan, not other global stations named \"Love FM.\" The sub-entities are transmitter sites\/frequencies for those locales. \"Nishi\" is slightly ambiguous (there are multiple Nishi wards in Japan), but in this context it most plausibly refers to Nishi-ku in Fukuoka City and is reasonably resolvable via the station’s published transmitter list.","temporal_specification_analysis":"No time frame is given. Radio frequencies can change over time, but the query is phrased in the present tense (“does have”), so the sensible default is to provide the current\/latest assigned frequencies."},"meta":{},"created_at":"2025-10-13T10:55:30.643336Z","updated_at":"2025-10-22T14:37:56.919448Z","inner_id":46,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1757,"annotations":[{"id":802,"completed_by":1,"result":[{"id":"O06UwfTJeA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"vP8gmQqXHr","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"niOzbgTdHv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"gHJDRilSM-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"NNBrDR-61u","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:24:58.860246Z","updated_at":"2025-10-22T14:24:58.860258Z","draft_created_at":"2025-10-22T14:24:44.433967Z","lead_time":25.863,"prediction":{"id":197,"task":1757,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765068Z","updated_at":"2025-10-13T10:55:30.765071Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"cdc2f03a-35a5-4c74-b744-f47e86db367c","import_id":null,"last_action":null,"bulk_created":false,"task":1757,"project":9,"updated_by":1,"parent_prediction":197,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[197],"data":{"id":2220,"query":"What are Nathalie Poza's works?","dataset":"FeTaQA","task_specification_analysis":"The query requests a straightforward listing (lookup) of items: Nathalie Poza’s works. No complex analytical action (ranking, aggregation, modeling) is implied.","scope_specification_analysis":"No calculations or transformations are required; it’s a simple enumeration. While “works” could span films, TV, theatre, etc., that ambiguity pertains to data selection rather than analytical method, so the procedural scope is clear.","domain_specification_analysis":"The contextual boundary is the individual’s oeuvre (films, TV, theater), which is self-contained and does not require a geographic or organizational qualifier.","entity_specification_analysis":"\"Nathalie Poza\" uniquely identifies the Spanish actress; a researcher can confidently locate her oeuvre\/filmography without ambiguity.","temporal_specification_analysis":"\"Works\" implies an all-time, cumulative list of outputs (career to date); no specific timeframe is required."},"meta":{},"created_at":"2025-10-13T10:55:30.643363Z","updated_at":"2025-10-22T14:24:58.910156Z","inner_id":47,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1758,"annotations":[{"id":836,"completed_by":1,"result":[{"id":"ldd9dpDLc0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"1euh7KEPiy","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"eX11sMkb-B","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"oUb0TyK4N3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"l7ltTvSfwP","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:56:59.067906Z","updated_at":"2025-10-22T14:56:59.067922Z","draft_created_at":"2025-10-22T14:56:54.013332Z","lead_time":21.031,"prediction":{"id":198,"task":1758,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765083Z","updated_at":"2025-10-13T10:55:30.765085Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"37809d31-d3ae-446a-916c-ccf669f83bae","import_id":null,"last_action":null,"bulk_created":false,"task":1758,"project":9,"updated_by":1,"parent_prediction":198,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[198],"data":{"id":2030,"query":"The Monstercat's album titled, Monstercat Uncaged Vol. 2, featuring \"Party Don't Stop\" was released when?","dataset":"FeTaQA","task_specification_analysis":"The query asks for the release date of a specific album; this is a direct fact lookup (return the release date).","scope_specification_analysis":"No calculations or analytical methods are required—simply retrieve the album’s release date. The mention of the track helps identify the correct album but does not change the method.","domain_specification_analysis":"The domain is implicitly the global music\/discography context, and the label (Monstercat) plus exact album title provide a self-contained boundary with no ambiguity.","entity_specification_analysis":"The query identifies a unique album: “Monstercat Uncaged Vol. 2” by Monstercat, further disambiguated by the track “Party Don’t Stop,” making the target entity clear and unique.","temporal_specification_analysis":"The question asks for the specific release date of the album; it does not require a time-frame for filtering. This is a fixed historical fact rather than a scoped period."},"meta":{},"created_at":"2025-10-13T10:55:30.643389Z","updated_at":"2025-10-22T14:56:59.124192Z","inner_id":48,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1759,"annotations":[{"id":557,"completed_by":1,"result":[{"id":"wix1w4pPyU","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"d7XEYE3gu3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"gJpc3ykG3W","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"3q2avebh1F","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"vi10M6SaSq","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-13T11:27:39.626121Z","updated_at":"2025-10-13T11:27:39.626131Z","draft_created_at":"2025-10-13T11:27:28.180524Z","lead_time":30.459,"prediction":{"id":199,"task":1759,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765098Z","updated_at":"2025-10-13T10:55:30.765100Z","created_ago":"32 minutes","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"116a5499-b744-46ab-83c2-cc7e7f1db02e","import_id":null,"last_action":null,"bulk_created":false,"task":1759,"project":9,"updated_by":1,"parent_prediction":199,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[199],"data":{"id":2059,"query":"When and from which album was Cub Sport's single \"Sometimes\" first released?","dataset":"FeTaQA","task_specification_analysis":"The user requests a specific factual lookup: provide the initial release date and the associated album for the single.","scope_specification_analysis":"No calculations or modeling needed; retrieve the first release date and album title. 'First released' can be taken as the initial release date, a standard default.","domain_specification_analysis":"The context is the global music domain; no additional geographical or organizational boundary is required.","entity_specification_analysis":"The entities are uniquely identifiable: the band 'Cub Sport' and their single titled 'Sometimes.' Specifying the artist disambiguates the song title from other songs named 'Sometimes.'","temporal_specification_analysis":"The query explicitly asks for the initial release date ('when ... first released'), which targets a specific historical point in time."},"meta":{},"created_at":"2025-10-13T10:55:30.643416Z","updated_at":"2025-10-13T11:27:39.664573Z","inner_id":49,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1760,"annotations":[{"id":723,"completed_by":1,"result":[{"id":"Kc3cFREd7f","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"qH8FrV1-Qw","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"8Nm-fq5Wjx","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"7vmywVMErE","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"iKgRCxQi97","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:59:02.512439Z","updated_at":"2025-10-21T22:59:02.512454Z","draft_created_at":"2025-10-21T22:58:43.517579Z","lead_time":47.721,"prediction":{"id":200,"task":1760,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765112Z","updated_at":"2025-10-13T10:55:30.765115Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"b097fff4-bd97-4fb3-b4c7-d6a7e11b4059","import_id":null,"last_action":null,"bulk_created":false,"task":1760,"project":9,"updated_by":1,"parent_prediction":200,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[200],"data":{"id":2002,"query":"How did Ennius do at the 2010 heptathlon and how many points did he score in Austria?","dataset":"FeTaQA","task_specification_analysis":"The query requests a straightforward fact lookup: report Ennius’s performance outcome at the 2010 heptathlon and the number of points he scored in Austria. No complex analysis or modeling is required.","scope_specification_analysis":"This is a simple retrieval of event results (placement\/finish status) and points from a specific competition. No ambiguous calculation choices (e.g., ranking metric, aggregation) are needed; standard interpretation of “how did he do” as placement\/finish outcome suffices.","domain_specification_analysis":"The context is track and field heptathlon competitions, and Austria provides a clear geographic reference. No additional organizational boundary is needed.","entity_specification_analysis":"\"Ennius\" is ambiguous (likely a misspelling of Jessica Ennis\/Ennis‑Hill, and the pronoun \"he\" conflicts). \"2010 heptathlon\" does not specify which competition (e.g., Hypo-Meeting Götzis, European Championships, World Indoors, etc.). \"in Austria\" hints at Götzis but does not uniquely resolve the athlete or event.","temporal_specification_analysis":"The year 2010 is given for the first clause, but the second clause (points \"in Austria\") lacks an explicit year; it is unclear whether it refers to 2010 or some other edition. Multiple Austrian heptathlon events occur across years."},"meta":{},"created_at":"2025-10-13T10:55:30.643443Z","updated_at":"2025-10-21T22:59:02.569880Z","inner_id":50,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1761,"annotations":[{"id":721,"completed_by":1,"result":[{"id":"-RHl9Ztykj","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"z1jWy1jXFf","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"0-5IzcPeQq","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"TLPH73Emsk","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"0W1CunGXvL","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:57:33.873622Z","updated_at":"2025-10-21T22:57:33.873633Z","draft_created_at":"2025-10-21T22:57:06.738558Z","lead_time":51.128,"prediction":{"id":201,"task":1761,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765127Z","updated_at":"2025-10-13T10:55:30.765130Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"9f62f70d-07f4-4962-9e51-5e952e1ea687","import_id":null,"last_action":null,"bulk_created":false,"task":1761,"project":9,"updated_by":1,"parent_prediction":201,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[201],"data":{"id":2573,"query":"which class of service reports the lowest prevalence of targeted sexual behaviours?","dataset":"HiTab","task_specification_analysis":"The user asks to identify which class of service has the minimum prevalence of targeted sexual behaviours—an explicit selection\/ranking task (argmin).","scope_specification_analysis":"The ranking metric is specified as prevalence of targeted sexual behaviours. Procedure: group by class of service, compute\/lookup prevalence per class (e.g., proportion reporting such behaviours), and select the class(es) with the lowest value. Tie-handling can default to returning all minima; other data details (timeframe, weighting) are data-level, not required to execute the core operation.","domain_specification_analysis":"The contextual boundary is missing: no industry, organization, geography, or dataset is specified. The meaning of \"class of service\" depends entirely on the domain, so a universal or obvious default cannot be assumed.","entity_specification_analysis":"Core entities are ambiguous. \"Class of service\" could mean airline cabin class, military rank\/enlisted vs. officer, telecom QoS class, transit fare class, etc. \"Targeted sexual behaviours\" appears to be a survey-defined metric without a specified instrument or population. No organization or population is identified to resolve these.","temporal_specification_analysis":"No time frame or survey year is given. Prevalence figures are period-specific and typically tied to a particular survey iteration; present tense \"reports\" does not safely imply using the latest data because the underlying dataset is unknown."},"meta":{},"created_at":"2025-10-13T10:55:30.643473Z","updated_at":"2025-10-21T22:57:33.915427Z","inner_id":51,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1762,"annotations":[{"id":733,"completed_by":1,"result":[{"id":"Op3a_R3Fqp","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"bM5eI6EndX","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"zlb3iu4iH8","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Jc6B5VsfN8","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"_4i0zt-oFc","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:39:32.452716Z","updated_at":"2025-10-22T05:39:32.452728Z","draft_created_at":"2025-10-22T05:38:58.850850Z","lead_time":56.985,"prediction":{"id":202,"task":1762,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765142Z","updated_at":"2025-10-13T10:55:30.765144Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"bd8a2276-a812-474a-9579-a8618988271b","import_id":null,"last_action":null,"bulk_created":false,"task":1762,"project":9,"updated_by":1,"parent_prediction":202,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[202],"data":{"id":2532,"query":"what is the percentage of the cases related to the possession of cannabis resulted in a finding of guilt from 2008\/2009 to 2011\/2012?","dataset":"HiTab","task_specification_analysis":"The user requests a single percentage: the share of cannabis possession cases that resulted in a finding of guilt over a specified time range. This is a clear, executable calculation.","scope_specification_analysis":"Compute: percentage = (count of cannabis possession cases with a finding of guilt between 2008\/09 and 2011\/12) \/ (total cannabis possession cases in that same period) * 100. Assumes aggregation across the full period. If annual breakdown were intended, that would need to be stated, but a single overall percentage is a reasonable default.","domain_specification_analysis":"No jurisdiction or court system is specified (country, region, adult\/youth courts). Without a geographic\/organizational boundary, relevant datasets cannot be uniquely identified.","entity_specification_analysis":"The core subject is criminal court cases related to cannabis possession and the share resulting in a finding of guilt. As a broad class, this is well-defined; the ambiguity stems from missing context about which system’s cases, not the entity itself.","temporal_specification_analysis":"The query clearly specifies a fiscal-year range: 2008\/2009 to 2011\/2012 (inclusive)."},"meta":{},"created_at":"2025-10-13T10:55:30.643500Z","updated_at":"2025-10-22T05:39:32.495743Z","inner_id":52,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1763,"annotations":[{"id":795,"completed_by":1,"result":[{"id":"yI4626xwJu","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Di0nGwfQj6","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"xUmsgcYzOo","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"437zjKOGNb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"UrQWZVPnHZ","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:19:15.191676Z","updated_at":"2025-10-22T14:19:15.191686Z","draft_created_at":"2025-10-22T14:18:55.292375Z","lead_time":33.424,"prediction":{"id":203,"task":1763,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765157Z","updated_at":"2025-10-13T10:55:30.765159Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"04cb8d66-3f08-4b0d-91fb-58905338f784","import_id":null,"last_action":null,"bulk_created":false,"task":1763,"project":9,"updated_by":1,"parent_prediction":203,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[203],"data":{"id":2497,"query":"how many times were aboriginal people more likely to perceive social disorder in their neighbourhood than non-aboriginal people?","dataset":"HiTab","task_specification_analysis":"The user asks for a quantitative comparison: compute how many times Aboriginal people are more likely than non-Aboriginal people to perceive social disorder in their neighbourhood. This implies calculating a comparative likelihood (a ratio) between two groups for a defined outcome.","scope_specification_analysis":"Key analytical parameters are missing. Specifically: (1) the operational definition of \"perceive social disorder\" (e.g., any reported disorder vs a composite index or threshold across multiple indicators); (2) the metric for \"more likely\" (risk\/prevalence ratio vs odds ratio vs rate ratio); (3) whether the estimate should be crude or adjusted for covariates; (4) if using survey data, whether to apply weights. These choices can materially change the result.","domain_specification_analysis":"No geographic or organizational boundary is given. \"Aboriginal\" could refer to Australia or Canada (among others), and \"neighbourhood\" suggests a national survey context but remains ambiguous without a specified country or dataset.","entity_specification_analysis":"Core entities like \"Aboriginal people,\" \"non-Aboriginal people,\" and the construct \"perceive social disorder in their neighbourhood\" are not tied to a specific population definition or survey instrument. Multiple plausible interpretations exist (e.g., Australia, Canada) and the measurement depends on a specific survey question\/metric, which is not identified.","temporal_specification_analysis":"No time frame provided and the ratio likely varies by survey wave\/year. Past-tense \"were\" implies a historical snapshot but gives no period; there is no safe default."},"meta":{},"created_at":"2025-10-13T10:55:30.643527Z","updated_at":"2025-10-22T14:19:15.229396Z","inner_id":53,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1764,"annotations":[{"id":720,"completed_by":1,"result":[{"id":"9gkUEUED2D","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"n58dOQSzdw","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"BRg3XvsH_N","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"NceoXPbR98","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"CMyyVkbo0t","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:56:42.092702Z","updated_at":"2025-10-21T22:56:42.092712Z","draft_created_at":"2025-10-21T22:56:20.131565Z","lead_time":32.302,"prediction":{"id":204,"task":1764,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765171Z","updated_at":"2025-10-13T10:55:30.765174Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2425c492-9df7-4cc5-a240-876f73ce58f0","import_id":null,"last_action":null,"bulk_created":false,"task":1764,"project":9,"updated_by":1,"parent_prediction":204,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[204],"data":{"id":2608,"query":"in 2014\/2015, how many percertage points have completed cases been stayed?","dataset":"HiTab","task_specification_analysis":"Ambiguous whether the user wants the proportion of completed cases that were stayed in 2014\/2015, or the change in that proportion (percentage-point difference) relative to another period.","scope_specification_analysis":"If a percentage-point change is intended, the reference period (e.g., prior year) is not specified. If a level is intended, the computation would be stayed\/completed*100, but the wording \"percentage points\" implies a difference without a baseline.","domain_specification_analysis":"No geographical or organizational boundary is provided (e.g., country, court\/agency), making the context for 'completed cases' ambiguous.","entity_specification_analysis":"References to 'completed cases' and 'stayed' lack identification of which cases (jurisdiction, court system, case type, or dataset). Multiple plausible entities exist, so the subject cannot be uniquely resolved.","temporal_specification_analysis":"'2014\/2015' provides a clear time frame (likely a fiscal year), enabling temporal filtering."},"meta":{},"created_at":"2025-10-13T10:55:30.643554Z","updated_at":"2025-10-21T22:56:42.130508Z","inner_id":54,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1765,"annotations":[{"id":812,"completed_by":1,"result":[{"id":"T7UiRV9teY","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"VpPi7N0tKf","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Vu42GwtIRY","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"d6pFr7Dfey","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"eR3b8ST3-4","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:31:14.403424Z","updated_at":"2025-10-22T14:31:14.403433Z","draft_created_at":"2025-10-22T14:31:00.281316Z","lead_time":27.307,"prediction":{"id":205,"task":1765,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765204Z","updated_at":"2025-10-13T10:55:30.765207Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"e2fd222d-5a1f-491a-af3b-ee05b6db66a5","import_id":null,"last_action":null,"bulk_created":false,"task":1765,"project":9,"updated_by":1,"parent_prediction":205,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[205],"data":{"id":2686,"query":"in the regular 2014 season, how many tackles did kuechly make?","dataset":"HiTab","task_specification_analysis":"The user asks for a single numeric lookup: the number of tackles made by Kuechly in the 2014 regular season. This is a clear retrieval task.","scope_specification_analysis":"\"Tackles\" is ambiguous: it could mean total\/combined tackles, solo tackles, assisted tackles, or another tallying convention. The timeframe (2014 regular season) is clear, but the specific tackle metric is not, and different choices would yield different numbers.","domain_specification_analysis":"Context implies NFL defensive statistics for the regular season; this is a clear, unique domain given the entity and phrasing.","entity_specification_analysis":"\"Kuechly\" clearly refers to Luke Kuechly, the well-known NFL linebacker; no reasonable alternative entity exists in this context.","temporal_specification_analysis":"\"in the regular 2014 season\" provides an explicit, precise time frame (the 2014 NFL regular season)."},"meta":{},"created_at":"2025-10-13T10:55:30.643581Z","updated_at":"2025-10-22T14:31:14.440512Z","inner_id":55,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1766,"annotations":[{"id":786,"completed_by":1,"result":[{"id":"Lvs4lguB5P","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"eRW1u0XLct","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"EJVZj-G5Ek","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"_Nl-BqcFWR","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"mMIYyLAZ87","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:09:48.722071Z","updated_at":"2025-10-22T14:09:48.722083Z","draft_created_at":"2025-10-22T14:09:38.334469Z","lead_time":37.363,"prediction":{"id":206,"task":1766,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765232Z","updated_at":"2025-10-13T10:55:30.765235Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2923d93b-9b56-4aef-88d9-da4971a0f9f4","import_id":null,"last_action":null,"bulk_created":false,"task":1766,"project":9,"updated_by":1,"parent_prediction":206,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[206],"data":{"id":2746,"query":"what was the percentage of businesses majority-owned by visible minorities reported an increase in revenue?","dataset":"HiTab","task_specification_analysis":"The query asks to compute a single statistic: the percentage of businesses (restricted to those majority-owned by visible minorities) that reported an increase in revenue. This is a clear request to calculate a proportion\/percentage.","scope_specification_analysis":"Operational steps are clear: (1) filter to businesses majority-owned by visible minorities; (2) within this subset, identify those that reported an increase in revenue; (3) compute percentage = (count with increase \/ total in subset) * 100. Reasonable default is an unweighted proportion of businesses. No additional ranking, modeling, or ambiguous aggregation parameters are required.","domain_specification_analysis":"No geographic or organizational boundary is provided. The term \"visible minorities\" strongly suggests a Canadian context, but this is not explicit, and the relevant data source\/survey (e.g., Statistics Canada CSBC) is unspecified. Multiple plausible domains exist, so the boundary is ambiguous.","entity_specification_analysis":"The core subject is the class \"businesses majority-owned by visible minorities\" and the metric is the share that reported increased revenue. This is a coherent, well-defined class; no specific named entity needs disambiguation. Any definitional nuance (e.g., definition of \"visible minority\") is handled under domain.","temporal_specification_analysis":"The query uses past tense (\"was\") and refers to businesses that \"reported an increase,\" which implies a specific survey wave or comparison period (year\/quarter). No timeframe is given and there is no safe default, so the time window is necessary to answer."},"meta":{},"created_at":"2025-10-13T10:55:30.643607Z","updated_at":"2025-10-22T14:09:48.767434Z","inner_id":56,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1767,"annotations":[{"id":746,"completed_by":1,"result":[{"id":"U9kRFB464i","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"DrozTfawgw","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"xzKiBDRh2q","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"cPqGfKtMEU","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"hHxKVWYG98","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:52:04.448119Z","updated_at":"2025-10-22T05:52:04.448129Z","draft_created_at":"2025-10-22T05:51:38.742145Z","lead_time":43.926,"prediction":{"id":207,"task":1767,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765247Z","updated_at":"2025-10-13T10:55:30.765249Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"e351851b-de34-4b89-a55b-ce838e8ff37c","import_id":null,"last_action":null,"bulk_created":false,"task":1767,"project":9,"updated_by":1,"parent_prediction":207,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[207],"data":{"id":2581,"query":"what were the percentages of graduates that did not have a job during their pse who wanted a job related to their field of study and those not wanting a job related to their field of study?","dataset":"HiTab","task_specification_analysis":"The user asks to compute and report percentages for two categories (wanted a job related to their field vs not wanting one) among a defined subset of graduates. This is a clear descriptive statistics task.","scope_specification_analysis":"Method is clear: filter to graduates who did not have a job during their postsecondary education, then compute the share of that filtered group who (a) wanted a job related to their field of study and (b) did not want a job related to their field. Standard percentage calculation with the filtered population as the denominator. Any minor ambiguity about category coding (e.g., whether “not wanting related” includes ‘unrelated’ or ‘no preference’) can default to using the complement of “wanted related,” excluding missing\/unknown responses.","domain_specification_analysis":"No geographic or organizational boundary is specified (country, region, education system). Such percentages are typically context-bound (e.g., national graduate surveys), but none is indicated.","entity_specification_analysis":"Core entities like \"graduates\" and \"PSE\" are not tied to any specific cohort, institution, country, or dataset; \"PSE\" itself is ambiguous (commonly post-secondary education). Multiple plausible datasets\/surveys could apply, so the target entities\/tables cannot be uniquely identified.","temporal_specification_analysis":"No time frame, cohort year, or survey wave is given. The phrasing \"were\" implies a past state but provides no specific period; results would vary by graduation year\/survey date."},"meta":{},"created_at":"2025-10-13T10:55:30.643634Z","updated_at":"2025-10-22T05:52:04.487189Z","inner_id":57,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1768,"annotations":[{"id":739,"completed_by":1,"result":[{"id":"WZdg6tI1qa","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Fo3lYs4kOb","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"4TEG5cNxBp","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"5PkxF1EkgK","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"TAYPz2zuNM","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:45:02.108161Z","updated_at":"2025-10-22T05:45:02.108173Z","draft_created_at":"2025-10-22T05:43:29.967014Z","lead_time":105.508,"prediction":{"id":208,"task":1768,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765262Z","updated_at":"2025-10-13T10:55:30.765264Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"03e3c15f-5e2b-41f8-b648-92c20f628a14","import_id":null,"last_action":null,"bulk_created":false,"task":1768,"project":9,"updated_by":1,"parent_prediction":208,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[208],"data":{"id":2410,"query":"what was the unemployment rate of aboriginal people who completed postsecondary education?","dataset":"HiTab","task_specification_analysis":"The user requests a single descriptive statistic: the unemployment rate for a specific subgroup (Aboriginal people who completed postsecondary education). This is a direct lookup or calculation request.","scope_specification_analysis":"The computation, if needed, follows the standard unemployment rate formula (unemployed divided by labor force) applied to the filtered subgroup. No ambiguous ranking, grouping, or modeling choices are required; reasonable defaults apply.","domain_specification_analysis":"No geographical or organizational boundary is given. \"Aboriginal people\" could correspond to different countries’ Indigenous populations (e.g., Australia, Canada), each with its own datasets and definitions. A clear domain (e.g., country and possibly age\/labour-force definition) is required.","entity_specification_analysis":"Core entities are the population group \"Aboriginal people who completed postsecondary education\" and the metric \"unemployment rate.\" The term \"Aboriginal people\" is ambiguous without a domain, as it could refer to Indigenous peoples in Australia, Canada, or elsewhere, each with different classifications. Therefore, the target population cannot be uniquely identified.","temporal_specification_analysis":"The question uses past tense (\"was\") but provides no time frame. Unemployment rates are time-varying and require a specific period; a safe default cannot be assumed for past-tense queries."},"meta":{},"created_at":"2025-10-13T10:55:30.643661Z","updated_at":"2025-10-22T05:45:02.153833Z","inner_id":58,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1769,"annotations":[{"id":735,"completed_by":1,"result":[{"id":"SFE0YDyAEb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"T2yww3sXek","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"7hoKnZwZs8","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"XscZ4KyACm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"SATq2YTyGi","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:41:19.449115Z","updated_at":"2025-10-22T05:41:19.449128Z","draft_created_at":"2025-10-22T05:40:52.431304Z","lead_time":52.063,"prediction":{"id":209,"task":1769,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765277Z","updated_at":"2025-10-13T10:55:30.765280Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"16e70265-6392-4ded-a417-5f55a4967ca4","import_id":null,"last_action":null,"bulk_created":false,"task":1769,"project":9,"updated_by":1,"parent_prediction":209,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[209],"data":{"id":2328,"query":"what is the percentage of drug-related cases involving supply offences with a decision of guilt included custody as part of the sentence from 2008\/2009 to 2011\/2012?","dataset":"HiTab","task_specification_analysis":"The query requests a specific metric: the percentage of a defined subset of cases (drug-related supply offences with a decision of guilt) that resulted in custody. This is a clear computational task (compute a percentage).","scope_specification_analysis":"It is unclear whether the percentage should be reported for each fiscal year (2008\/09, 2009\/10, 2010\/11, 2011\/12) or as a single aggregated figure across the whole period. If aggregated, it is not specified whether to compute a pooled rate (sum of numerators over sum of denominators) or an unweighted average of yearly percentages. These choices materially affect the result.","domain_specification_analysis":"The query lacks a necessary jurisdictional boundary (e.g., country\/region such as England and Wales, Scotland, Canada, Australia, etc.), court population (adult vs. youth), and possibly court level. Without this, multiple plausible datasets exist and no universal default applies.","entity_specification_analysis":"Core entities are a well-defined class: drug-related court cases involving supply offences, focusing on those with a decision of guilt and whether custody was included in the sentence. While there could be methodological nuances (e.g., principal offence vs. any supply charge), the entities themselves are recognizable and standard across justice statistics.","temporal_specification_analysis":"Explicit fiscal-year range is given: from 2008\/2009 to 2011\/2012, which clearly defines the time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.643690Z","updated_at":"2025-10-22T05:41:19.494804Z","inner_id":59,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1770,"annotations":[{"id":772,"completed_by":1,"result":[{"id":"3Coo65ajlG","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"cgbZrEf6Aq","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"miynpTNHbl","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"dnysgB6uR1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"_2wAF1Omt6","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:09:59.746604Z","updated_at":"2025-10-22T06:09:59.746618Z","draft_created_at":"2025-10-22T06:09:32.854638Z","lead_time":43.614,"prediction":{"id":210,"task":1770,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765292Z","updated_at":"2025-10-13T10:55:30.765295Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"cca3c048-14db-48d5-bfb3-8227d74021c7","import_id":null,"last_action":null,"bulk_created":false,"task":1770,"project":9,"updated_by":1,"parent_prediction":210,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[210],"data":{"id":2338,"query":"where can the second lowest rates of municipal police strength be found?","dataset":"HiTab","task_specification_analysis":"The user asks to identify the location(s) with the second-lowest value of a specified metric, which is a clear ranking\/selection task.","scope_specification_analysis":"The procedure is to sort entities by the metric 'rate of municipal police strength' in ascending order and return the entity\/entities at rank 2. Handling of ties can default to returning all tied at second-lowest; missing values are typically excluded. No additional analytical parameters are required.","domain_specification_analysis":"The geographic\/organizational boundary is not stated—unclear whether this is within a specific country, region, or worldwide, or within a particular dataset—so the domain is ambiguous.","entity_specification_analysis":"The core entity \"rates of municipal police strength\" is vague: it doesn’t define the metric (officers per 100k? staffing ratio? headcount?), what qualifies as \"municipal police,\" nor the comparison set for \"second lowest\" (cities, states, countries). Hence the target entity\/measure and comparison group are not uniquely identifiable.","temporal_specification_analysis":"Police strength rates change over time, but no year or period is provided. The superlative \"second lowest\" depends on a specific timeframe, which is missing and cannot be safely defaulted."},"meta":{},"created_at":"2025-10-13T10:55:30.643717Z","updated_at":"2025-10-22T06:09:59.800614Z","inner_id":60,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1771,"annotations":[{"id":834,"completed_by":1,"result":[{"id":"em98W_CmSh","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"GHtw5cC8fm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"9O_YgBJ2IA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"AQF6wzbolR","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"ZaQ63lBSaY","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:56:13.370621Z","updated_at":"2025-10-22T14:56:13.370632Z","draft_created_at":"2025-10-22T14:55:52.863910Z","lead_time":47.837,"prediction":{"id":211,"task":1771,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765307Z","updated_at":"2025-10-13T10:55:30.765310Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"948c1600-3836-4d87-95b6-42db921e50c7","import_id":null,"last_action":null,"bulk_created":false,"task":1771,"project":9,"updated_by":1,"parent_prediction":211,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[211],"data":{"id":2857,"query":"What is the correlation between the proportion of generally unsafe air quality days according to the EPA and the amount of land affected by fires in 2024? Round to 2 decimal places.","dataset":"KramaBench","task_specification_analysis":"The user asks to compute a correlation and provide a rounded numeric result, which is a clear analytical action.","scope_specification_analysis":"Key analytical parameters are missing: the unit of analysis over which pairs are formed (e.g., states, counties, cities, days), the exact definition of \"generally unsafe\" air quality days (e.g., AQI ≥ 101 vs. AQI ≥ 151, which EPA categories to include), whether the proportion is out of all calendar days or only days with valid monitoring, and what constitutes \"amount of land affected by fires\" (wildfire burn area only vs. all fires; units; aggregated to which geography). While the correlation type could default to Pearson, the pairing scheme and metric definitions are not specified and could materially change results.","domain_specification_analysis":"Referencing the EPA reasonably constrains the domain to the United States. However, the subnational unit for aggregation remains unspecified (handled under Entity Specification).","entity_specification_analysis":"Core entities\/metrics are ambiguous. To compute a correlation, we need a pairing unit (e.g., states, counties, metros, or monitors) that is not specified. \"Generally unsafe air quality days\" is undefined (e.g., AQI ≥ USG vs. Unhealthy+), and how to compute the \"proportion\" (denominator, handling missing days) is unspecified. \"Amount of land affected by fires\" could mean wildfire area burned (acres\/hectares), prescribed burns, or total area impacted, and the data source\/aggregation level is not given.","temporal_specification_analysis":"The query explicitly requests data \"in 2024,\" providing a clear time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.643744Z","updated_at":"2025-10-22T14:56:13.414375Z","inner_id":61,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1772,"annotations":[{"id":818,"completed_by":1,"result":[{"id":"wzVep_fwnA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"w7T10WJpI_","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"urFf4o_dNz","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"JBGgGtp5L1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"p8UJlBnUiN","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:35:49.930012Z","updated_at":"2025-10-22T14:35:49.930027Z","draft_created_at":"2025-10-22T14:35:17.530745Z","lead_time":49.38,"prediction":{"id":212,"task":1772,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765322Z","updated_at":"2025-10-13T10:55:30.765325Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"c43dd2d6-6d85-4cef-95c5-9c2db0c227d6","import_id":null,"last_action":null,"bulk_created":false,"task":1772,"project":9,"updated_by":1,"parent_prediction":212,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[212],"data":{"id":2845,"query":"Which NIFC geographic area intersects with the most US states? Give the abbreviation of the geographic area","dataset":"KramaBench","task_specification_analysis":"Clear goal: identify the NIFC geographic area that intersects the largest number of US states and return its abbreviation. This is a max-by-count task over spatial relationships.","scope_specification_analysis":"Method is defined: for each NIFC geographic area, compute the count of US state polygons it spatially intersects (using the standard GIS 'intersects' predicate), then select the area with the maximum count and output its abbreviation. Ranking metric (count of intersections) is explicit. Any tie can reasonably default to returning all tied areas, though the query asks for a single abbreviation.","domain_specification_analysis":"The domain is inherently specified by referencing the NIFC (a U.S. federal context) and intersections with U.S. states. No additional geographic or organizational boundary is needed.","entity_specification_analysis":"Core entities are the NIFC Geographic Areas (GACCs) and U.S. states. \"NIFC geographic area\" refers to the standard set of NIFC\/GACC regions (e.g., CA, NW, NR, GB, RM, SW, S, E, AK), and asking for the area’s abbreviation implies the well-known NIFC codes. \"US states\" clearly refers to the 50 U.S. states. These are specific and resolvable entities.","temporal_specification_analysis":"No time frame is provided, but this is a definitional\/geospatial relationship (GACC boundaries vs. state boundaries) that is not typically time-dependent for the purpose of the query. A researcher can answer without a temporal filter."},"meta":{},"created_at":"2025-10-13T10:55:30.643771Z","updated_at":"2025-10-22T14:35:49.984454Z","inner_id":62,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1773,"annotations":[{"id":841,"completed_by":1,"result":[{"id":"No1N4G7Gap","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"cQ3rX4XtRc","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"K4GWW0Xtml","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Q7tJzASrmJ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"DhXd3eWj8f","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:59:32.856863Z","updated_at":"2025-10-22T14:59:32.856876Z","draft_created_at":"2025-10-22T14:59:10.350295Z","lead_time":32.353,"prediction":{"id":213,"task":1773,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765337Z","updated_at":"2025-10-13T10:55:30.765340Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"83ef106b-3448-47ab-985f-8ee52f6f8f3b","import_id":null,"last_action":null,"bulk_created":false,"task":1773,"project":9,"updated_by":1,"parent_prediction":213,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[213],"data":{"id":2783,"query":"What is the age of the patient with the lowest APP-Z score?","dataset":"KramaBench","task_specification_analysis":"Clear action: identify the minimum APP-Z score and report the corresponding patient’s age (a lookup after a min selection).","scope_specification_analysis":"Ranking metric and direction are explicit (lowest APP-Z). The attribute to return is explicit (age). Reasonable default if ties occur is to return the age(s) of all patients sharing the minimum.","domain_specification_analysis":"No contextual boundary is given (which study, hospital, EHR system, or population). The notion of \"lowest APP-Z score\" only makes sense within a specific dataset\/domain.","entity_specification_analysis":"The query refers to \"the patient with the lowest APP-Z score\" without specifying which dataset\/cohort\/clinic or even what APP-Z definition to use. Multiple plausible patients exist across many datasets, making the target entity unresolvable.","temporal_specification_analysis":"Age depends on a reference time (e.g., at enrollment, at measurement, or current). No timeframe or dataset convention is provided, so the intended time basis cannot be inferred."},"meta":{},"created_at":"2025-10-13T10:55:30.643798Z","updated_at":"2025-10-22T14:59:32.901769Z","inner_id":63,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1774,"annotations":[{"id":804,"completed_by":1,"result":[{"id":"hivBE2Catg","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"orOe0n--rR","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"gLF1u-YpC3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"44xTwyQfpd","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"UXuPRVAQsZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:27:24.930907Z","updated_at":"2025-10-22T14:27:24.930916Z","draft_created_at":"2025-10-22T14:27:09.192410Z","lead_time":43.054,"prediction":{"id":214,"task":1774,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765352Z","updated_at":"2025-10-13T10:55:30.765354Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"0b321995-6327-49d3-a089-7c5a9f2a7c1a","import_id":null,"last_action":null,"bulk_created":false,"task":1774,"project":9,"updated_by":1,"parent_prediction":214,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[214],"data":{"id":2825,"query":"How many total Identity Theft reports were there in 2024 from cross-state Metropolitan Statistical Areas?","dataset":"KramaBench","task_specification_analysis":"The user requests a single number: the total count of Identity Theft reports in 2024 from cross-state MSAs. This is a clear counting\/summing task.","scope_specification_analysis":"Method is clear: filter records to year=2024, filter geography to MSAs that span multiple states, filter category to Identity Theft, then sum the report counts (or count rows if microdata). No additional analytical parameters (ranking, per-capita adjustments, or model choice) are required.","domain_specification_analysis":"'Metropolitan Statistical Areas' implies the U.S. OMB-defined geography, and 'cross-state' further constrains the inclusion set. The domain boundary is clear and self-contained.","entity_specification_analysis":"Core entities are 'Identity Theft reports' and 'cross-state Metropolitan Statistical Areas (MSAs)'. MSAs are OMB-defined U.S. geographies, and identity theft reports at the MSA level are commonly published via the FTC Consumer Sentinel Network, making the target data identifiable.","temporal_specification_analysis":"The query explicitly specifies the year 2024, defining the time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.643825Z","updated_at":"2025-10-22T14:27:24.967827Z","inner_id":64,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1775,"annotations":[{"id":793,"completed_by":1,"result":[{"id":"_4I_z7F24K","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"qVSyx86B2O","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"556puSfUI0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"6DNnfYsPQb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"06YZX3g_KA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:15:50.567590Z","updated_at":"2025-10-22T14:15:50.567601Z","draft_created_at":"2025-10-22T14:15:40.231848Z","lead_time":27.204,"prediction":{"id":215,"task":1775,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765366Z","updated_at":"2025-10-13T10:55:30.765369Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"7044ae83-9720-453d-b456-0102cf1d672c","import_id":null,"last_action":null,"bulk_created":false,"task":1775,"project":9,"updated_by":1,"parent_prediction":215,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[215],"data":{"id":2769,"query":"What is the estimation error, in terms of MAE, of NOAA SWPC's 3-day forecast of AP from March 9, 2025 for the period March 10-12, 2025?","dataset":"KramaBench","task_specification_analysis":"The user asks to compute a specific error metric (MAE) comparing NOAA SWPC’s 3-day forecast (issued Mar 9, 2025) for AP against realized values over Mar 10–12, 2025. This is a clear, executable analytical task: calculate mean absolute error over the three forecasted vs observed daily values.","scope_specification_analysis":"All methodological parameters are provided: the metric (MAE), the variable (AP\/Ap index), the forecast issuance date (Mar 9, 2025), and the evaluation window (Mar 10–12, 2025). The procedure is straightforward: pair each day’s forecasted AP with the observed AP, compute absolute errors, then average. No ambiguous ranking, grouping, or model choices are required.","domain_specification_analysis":"The contextual boundary is clear: space weather\/geomagnetic indices from NOAA SWPC, specifically the global Ap index; no additional geographic or organizational scope needed.","entity_specification_analysis":"Entities are uniquely identified: NOAA SWPC as the source, the specific “3-day forecast” product, the Ap (planetary geomagnetic A index) variable, and the issuance on 2025-03-09 evaluated against 2025-03-10 to 2025-03-12.","temporal_specification_analysis":"Both the forecast issuance date (2025-03-09) and the evaluation period (2025-03-10 to 2025-03-12) are explicitly stated."},"meta":{},"created_at":"2025-10-13T10:55:30.643854Z","updated_at":"2025-10-22T14:15:50.608331Z","inner_id":65,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1776,"annotations":[{"id":756,"completed_by":1,"result":[{"id":"iTiJXgr5ns","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"_Ou0miaW0S","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"tNgEj4lbRd","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"2mlkuX1UUt","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"lasbyO_PBw","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:59:57.827194Z","updated_at":"2025-10-22T05:59:57.827203Z","draft_created_at":"2025-10-22T05:59:41.805179Z","lead_time":38.473,"prediction":{"id":216,"task":1776,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765381Z","updated_at":"2025-10-13T10:55:30.765384Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"759a4bb8-bfa7-4b4a-999d-a6d9ebc2bf41","import_id":null,"last_action":null,"bulk_created":false,"task":1776,"project":9,"updated_by":1,"parent_prediction":216,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[216],"data":{"id":2777,"query":"Determine the best lag (from 0 to 48 hours) between atmospheric drag--measured as semi-major axis change (in km) from TLE data of SATCAT 43180--and the OMNI AP index, that maximizes the r^2 correlation during May 1--30, 2024. TLE epoch times should be rounded to the nearest hour to align with AP measurements. Use hourly OMNI2 data. omni2 data format specification can be found at omni2.text file Use earth's gravitational paremeter mu = 398600.4418 km^3\/s^2.","dataset":"KramaBench","task_specification_analysis":"The user clearly asks to identify the lag (0–48 hours) that maximizes the coefficient of determination (r^2) between two time series over a defined period. This is an explicit optimization task over a specified lag range with a defined correlation metric and time window.","scope_specification_analysis":"The computational steps are sufficiently defined: derive semi-major axis a from TLEs using the provided mu (via a=(mu\/n^2)^(1\/3) with n from TLE mean motion converted to rad\/s), compute semi-major axis change Δa (successive differences in km), round TLE epochs to the nearest hour to align with hourly OMNI2 AP, and for each lag 0–48 h, shift AP relative to Δa and compute Pearson correlation, square to r^2, and select the lag with the maximum r^2 over May 1–30, 2024. Minor implementation defaults (e.g., handling multiple TLEs within the same rounded hour, pairwise-complete observations) can be reasonably assumed and do not materially affect the intended analysis.","domain_specification_analysis":"Domain boundaries are explicit: use TLE data for the specified satellite and the hourly OMNI2 dataset\/AP index (space physics\/Earth environment). No external organizational or geographic ambiguity remains.","entity_specification_analysis":"Core entities are uniquely identifiable: SATCAT 43180 (a specific NORAD catalog object) and the OMNI2 dataset’s AP\/ap index as defined in the OMNI2 documentation (omni2.text). Earth’s gravitational parameter is provided. These references enable locating the relevant TLE records and OMNI2 hourly tables without further clarification.","temporal_specification_analysis":"The timeframe is explicitly defined as May 1–30, 2024, with a lag search window of 0–48 hours and an instruction to round TLE epochs to the nearest hour for alignment."},"meta":{},"created_at":"2025-10-13T10:55:30.643881Z","updated_at":"2025-10-22T05:59:57.864650Z","inner_id":66,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1777,"annotations":[{"id":742,"completed_by":1,"result":[{"id":"ASkfrecFS-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"fP6hJL9Hmx","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"a-qco82bjH","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"eeA4zwCctv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"eVNAsCm2Oa","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:48:16.125631Z","updated_at":"2025-10-22T05:48:16.125644Z","draft_created_at":"2025-10-22T05:47:35.202179Z","lead_time":76.96,"prediction":{"id":217,"task":1777,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765396Z","updated_at":"2025-10-13T10:55:30.765398Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"1ac39f81-8101-401e-b84c-e35cc2f08b7e","import_id":null,"last_action":null,"bulk_created":false,"task":1777,"project":9,"updated_by":1,"parent_prediction":217,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[217],"data":{"id":2799,"query":"For Boston Harbor beaches located in communities with more than 90% environmental justice (EJ) populations, what is the Pearson correlation (to 3 decimal places) between rainfall in the past 3 days and Enterococcus levels? Estimate Enterococcus levels labeled as less than x as x\/2, and those labeled as greater than x as x.","dataset":"KramaBench","task_specification_analysis":"Compute a single Pearson correlation coefficient between two variables (rainfall in the past 3 days and Enterococcus levels) for a specified subset; report to 3 decimals.","scope_specification_analysis":"Scope is clear: filter to Boston Harbor beaches in communities with >90% EJ populations; for each sample, pair Enterococcus with rainfall accumulated over the prior 3 days; handle censored Enterococcus values (<x → x\/2; >x → x); compute Pearson correlation across all paired observations and round to 3 decimals. Standard default for “past 3 days” can be applied as the 72-hour or previous 3 calendar days sum prior to sampling.","domain_specification_analysis":"Geographic scope \"Boston Harbor\" is broadly identifiable, but the filter \"communities with >90% EJ populations\" leaves the domain boundary unclear: which jurisdictional unit defines \"communities,\" and what EJ criterion\/percentile and geography (state vs national) apply? The rainfall domain (which stations\/products covering the area) is also unspecified. Thus the contextual boundary needed to select\/join data is ambiguous.","entity_specification_analysis":"Core entities include: (1) \"Boston Harbor beaches\"—there are multiple authoritative lists (MassDPH, MWRA, Save the Harbor, etc.), not uniquely specified; (2) \"communities with more than 90% environmental justice (EJ) populations\"—\"communities\" (municipalities vs census tracts\/block groups) and the meaning of \">90% EJ populations\" are ambiguous (EJ is often a designation, not a proportion; could also mean EJScreen indices >90th percentile); (3) rainfall source\/location and spatial join method (nearest station, gridded product) are not specified; (4) Enterococcus dataset\/source not specified. These ambiguities prevent reliably identifying the exact tables required.","temporal_specification_analysis":"No time window is provided for the correlation (e.g., which years or bathing seasons). The phrase \"rainfall in the past 3 days\" is relative to each sampling date, but the overall study period is undefined and materially affects the result. This cannot default safely to \"most recent\" because the analysis spans historical observations."},"meta":{},"created_at":"2025-10-13T10:55:30.643908Z","updated_at":"2025-10-22T05:48:16.179989Z","inner_id":67,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1778,"annotations":[{"id":722,"completed_by":1,"result":[{"id":"hIYlbL4GlR","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"uLtWAjPxzY","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"30b2Et57pl","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"qghkH0hXaZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Ym17phZS3D","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:58:14.158792Z","updated_at":"2025-10-21T22:58:14.158805Z","draft_created_at":"2025-10-21T22:57:55.793617Z","lead_time":39.601,"prediction":{"id":218,"task":1778,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765410Z","updated_at":"2025-10-13T10:55:30.765413Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"40940011-434e-42f4-833c-1695cc98cc71","import_id":null,"last_action":null,"bulk_created":false,"task":1778,"project":9,"updated_by":1,"parent_prediction":218,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[218],"data":{"id":2842,"query":"For the freshwater beaches, what is the difference between the percentage exceedance rate in 2023 and the historic average from 2002 to 2022?","dataset":"KramaBench","task_specification_analysis":"The query requests a concrete computation: compute the numerical difference between the 2023 percentage exceedance rate and the historical average (2002–2022) for freshwater beaches.","scope_specification_analysis":"Operational steps are clear: filter to freshwater beaches, obtain the 2023 percentage exceedance rate, compute the historical average of the annual percentage exceedance rates across 2002–2022 (inclusive, using a standard unweighted arithmetic mean unless a precomputed historic average is provided), then subtract the historical average from the 2023 value to yield a signed difference in percentage points.","domain_specification_analysis":"No geographic or organizational boundary is provided (e.g., city, county, state, country, or monitoring program). \"The freshwater beaches\" presumes a known domain in context, which is ambiguous in an open-domain setting.","entity_specification_analysis":"The core entities are \"freshwater beaches\" and the metric \"percentage exceedance rate.\" Without a specified geography\/agency or dataset, \"freshwater beaches\" could refer to many different collections worldwide, and the exceedance metric may vary by program. Thus the target entity set cannot be uniquely identified.","temporal_specification_analysis":"The query clearly specifies the time frames: 2023 and the historic average from 2002–2022."},"meta":{},"created_at":"2025-10-13T10:55:30.643934Z","updated_at":"2025-10-21T22:58:14.209064Z","inner_id":68,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1779,"annotations":[{"id":778,"completed_by":1,"result":[{"id":"MXeMODz8YA","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"lGLavPR5u0","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"RpVQmaWxGi","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"_2O996cLKR","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"OAhp0Wk8UH","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:13:53.612491Z","updated_at":"2025-10-22T06:13:53.612501Z","draft_created_at":"2025-10-22T06:13:30.236843Z","lead_time":44.325,"prediction":{"id":219,"task":1779,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765425Z","updated_at":"2025-10-13T10:55:30.765428Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"3a62f600-4fa9-427f-98aa-2b01ae7e9f42","import_id":null,"last_action":null,"bulk_created":false,"task":1779,"project":9,"updated_by":1,"parent_prediction":219,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[219],"data":{"id":2858,"query":"According to NOAA, in Julies, are the size of a fire and the median wind speed in the area strongly correlated? The answer should be Yes or No.","dataset":"KramaBench","task_specification_analysis":"The user wants a binary (Yes\/No) determination of whether fire size and median wind speed are strongly correlated in July using NOAA data—i.e., compute a correlation and judge its strength.","scope_specification_analysis":"Key analytical parameters are missing: the correlation metric (Pearson, Spearman, etc.); the threshold defining “strongly” (e.g., |r| ≥ 0.7); whether strength is based on absolute value (strong negative vs positive); how to aggregate “median wind speed in the area” (spatial footprint\/radius, vertical level, data source) and over what temporal window (during the fire, on ignition day, entire July); and which fire size measure (final burned area vs daily size). These choices materially affect the outcome.","domain_specification_analysis":"While “According to NOAA” gives an organizational source, the geographical\/contextual boundary is missing (U.S. vs. global coverage; land vs. all detections). Different NOAA datasets cover different domains. The phrase “in the area” also implies a spatial boundary that is undefined.","entity_specification_analysis":"Core subjects are vague: “fire” is not restricted to wildfires vs. other fire events; NOAA has multiple fire-related products with different event definitions. The variables “size of a fire” and “median wind speed in the area” are undefined (e.g., acreage vs. perimeters; what spatial buffer constitutes “the area”). Without a specific dataset or event definition, a researcher cannot reliably select the correct tables.","temporal_specification_analysis":"“In Julies” is unclear—likely a typo for “in July,” but it still does not specify which year(s) (e.g., a particular July, all Julys across a range, or recent July). The correlation scope over time is ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.643961Z","updated_at":"2025-10-22T06:13:53.652723Z","inner_id":69,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1780,"annotations":[{"id":811,"completed_by":1,"result":[{"id":"80LPOvqr_T","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"81KI5kWx_d","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"2JZrPs8RHA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"XWbYGJQ8gS","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"S66oZqtFmy","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:30:46.420166Z","updated_at":"2025-10-22T14:30:46.420175Z","draft_created_at":"2025-10-22T14:30:37.439776Z","lead_time":28.3,"prediction":{"id":220,"task":1780,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765440Z","updated_at":"2025-10-13T10:55:30.765443Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"f5c7114a-2552-4899-8422-2ec1b5a9a250","import_id":null,"last_action":null,"bulk_created":false,"task":1780,"project":9,"updated_by":1,"parent_prediction":220,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[220],"data":{"id":2835,"query":"Which branch of the U.S. Military had the largest \"Median Fraud Loss\" reported by its service members in 2024?","dataset":"KramaBench","task_specification_analysis":"The query requests identifying the U.S. military branch with the largest Median Fraud Loss in 2024—i.e., a max selection over a clearly named metric.","scope_specification_analysis":"Steps are clear: filter to 2024 service-member reports, group by branch, compute\/use median fraud loss per branch, then select the branch(es) with the maximum median. If tied, return all tied branches as a reasonable default.","domain_specification_analysis":"The contextual boundary is the U.S. Military and its service members within the United States, which is globally unique and unambiguous.","entity_specification_analysis":"The entities are clear: the branches of the U.S. Military (Army, Navy, Air Force, Marine Corps, Coast Guard, Space Force) and the metric “Median Fraud Loss” reported by service members. This is a well-defined class and metric label likely present in public reports (e.g., FTC military consumer data).","temporal_specification_analysis":"Explicitly specified as “in 2024,” providing a clear time frame for the data."},"meta":{},"created_at":"2025-10-13T10:55:30.643987Z","updated_at":"2025-10-22T14:30:46.456753Z","inner_id":70,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1781,"annotations":[{"id":838,"completed_by":1,"result":[{"id":"vMHUJaf0ew","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"PDUq7yADEA","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"YeNHLtFeCB","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"4FC4LptWMZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"S4OzMuOnI6","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:58:01.412463Z","updated_at":"2025-10-22T14:58:01.412573Z","draft_created_at":"2025-10-22T14:57:47.715364Z","lead_time":28.202,"prediction":{"id":221,"task":1781,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765455Z","updated_at":"2025-10-13T10:55:30.765458Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"0fa975a1-ebcc-458e-b456-647f8f747e1d","import_id":null,"last_action":null,"bulk_created":false,"task":1781,"project":9,"updated_by":1,"parent_prediction":221,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[221],"data":{"id":3262,"query":"What is the dorm name and student capacity of dorms that allow pets and what is the average age of female students enrolled in major 600?","dataset":"MMQA","task_specification_analysis":"The query requests two explicit operations: (1) list dorm name and capacity for dorms that allow pets, and (2) compute the average age of female students in major 600. These are clear retrieval and aggregation tasks.","scope_specification_analysis":"Scope is clear: filter dorms by allows_pets = true and select [dorm_name, student_capacity]; then filter students by gender = female and major = 600 (and enrolled) and compute mean(age). The aggregation function (average) and filters are unambiguous with standard defaults.","domain_specification_analysis":"The query lacks a contextual boundary (which university\/college\/campus\/department). Housing and student records are domain-scoped; without this, the researcher cannot determine which institution’s data to query.","entity_specification_analysis":"Core entities include: dorms (residence halls) that allow pets, female students, and \"major 600.\" Without naming a specific institution, these entities cannot be uniquely identified in an open-domain corpus. Moreover, \"major 600\" is ambiguous (could refer to a course number or a program code that varies by school). Hence a researcher could not locate the correct tables without clarification.","temporal_specification_analysis":"No time frame is provided. Dorm pet policies and student enrollment\/ages change over terms\/years, and \"enrolled\" implies a specific academic period. Because the dataset\/time context is unknown, a safe default cannot be assumed."},"meta":{},"created_at":"2025-10-13T10:55:30.644013Z","updated_at":"2025-10-22T14:58:01.462917Z","inner_id":71,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1782,"annotations":[{"id":788,"completed_by":1,"result":[{"id":"uk2MSs1Ts8","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"jNzMbWME-H","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"IWZn0ELN-f","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"pmSDddryal","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"gBEBfV52eX","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:11:47.475874Z","updated_at":"2025-10-22T14:11:47.475884Z","draft_created_at":"2025-10-22T14:11:28.354172Z","lead_time":34.345,"prediction":{"id":222,"task":1782,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765470Z","updated_at":"2025-10-13T10:55:30.765472Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"875203d6-f73e-4fed-b5d8-3047bf9f12fa","import_id":null,"last_action":null,"bulk_created":false,"task":1782,"project":9,"updated_by":1,"parent_prediction":222,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[222],"data":{"id":3022,"query":"Find the distinct last names of all the students who have president votes and whose advisor is not 2192.","dataset":"MMQA","task_specification_analysis":"The user requests a specific retrieval: list the distinct last names of students meeting two conditions. This is a clear listing task.","scope_specification_analysis":"Filters are adequately defined: (1) students who have president votes (interpreted as having at least one associated vote record for the president position) and (2) advisor ID not equal to 2192. Output requires deduplication on last names. No ranking or additional metrics are needed.","domain_specification_analysis":"The organizational context is missing (which school\/university, which election). The presence of an internal ID ('2192') further indicates reliance on a particular, unspecified dataset.","entity_specification_analysis":"References to 'students', 'president votes', and an advisor identified as '2192' depend on a specific, unnamed database\/schema. The advisor ID is an internal identifier, and 'president votes' is ambiguous without a known table or context. These entities cannot be uniquely resolved in an open-domain setting.","temporal_specification_analysis":"No time frame is implied or required; this is a schema-bound database state query rather than a time-based metric."},"meta":{},"created_at":"2025-10-13T10:55:30.644039Z","updated_at":"2025-10-22T14:11:47.519640Z","inner_id":72,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1783,"annotations":[{"id":554,"completed_by":1,"result":[{"id":"Irm1itHWv-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Mhvmjp5BE9","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"NoqNNQR6rs","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"gwAqSILN5I","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-13T11:10:59.735762Z","updated_at":"2025-10-13T11:10:59.735773Z","draft_created_at":"2025-10-13T11:10:01.925708Z","lead_time":74.472,"prediction":{"id":223,"task":1783,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765484Z","updated_at":"2025-10-13T10:55:30.765487Z","created_ago":"15 minutes","mislabeling":0.0,"model_version":"gpt-5"},"result_count":4,"unique_id":"6d0f6186-2775-4e7c-a950-1541d5894051","import_id":null,"last_action":null,"bulk_created":false,"task":1783,"project":9,"updated_by":1,"parent_prediction":223,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[223],"data":{"id":3319,"query":"Which 'Bad Customer' has made the largest total payments, and what is the total payment amount?","dataset":"MMQA","task_specification_analysis":"The user requests an explicit operation: identify the customer (within the 'Bad Customer' subset) with the largest total payments and report both the customer and the summed amount. This is a clear ranking-after-aggregation task.","scope_specification_analysis":"Method is clear: filter to 'Bad Customer', group by customer, sum payment amounts per customer, order by the sum descending, select the top entry, and return the customer identifier\/name and the total. Reasonable defaults apply (sum for 'total payments'); tie-breaking is not specified but is a minor edge case.","domain_specification_analysis":"No organizational or geographical boundary is provided (e.g., which company\/database\/system defines \"Bad Customer\"). The question is not sensible without a specific domain context.","entity_specification_analysis":"\"Bad Customer\" is a dataset-specific label and \"customer\"\/\"payments\" lack any company, dataset, or market context. The core entities cannot be uniquely identified in an open domain.","temporal_specification_analysis":"The query seeks a superlative cumulative total (largest total payments), which reasonably implies an all-time aggregation within the intended dataset."},"meta":{},"created_at":"2025-10-13T10:55:30.644066Z","updated_at":"2025-10-13T11:10:59.776948Z","inner_id":73,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1784,"annotations":[{"id":555,"completed_by":1,"result":[{"id":"GkyaC1Y0Ml","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"UZEeg6LiIM","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"-9g_siKJ7z","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"FSJDxSQ7Qv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"s2W0Q944LT","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-13T11:12:11.589679Z","updated_at":"2025-10-13T11:12:11.589690Z","draft_created_at":"2025-10-13T11:11:24.429283Z","lead_time":70.966,"prediction":{"id":224,"task":1784,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765499Z","updated_at":"2025-10-13T10:55:30.765502Z","created_ago":"16 minutes","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"66acfba3-6a85-49c4-a37f-f7dce3e19e7a","import_id":null,"last_action":null,"bulk_created":false,"task":1784,"project":9,"updated_by":1,"parent_prediction":224,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[224],"data":{"id":2948,"query":"What are the different first names for customers from Brazil who have also had an invoice?","dataset":"MMQA","task_specification_analysis":"The user requests a list of unique values (first names) meeting clear filter conditions (customers from Brazil who have at least one invoice). This is a straightforward listing task.","scope_specification_analysis":"The query specifies filters (country = Brazil, existence of an invoice) and the operation (return different\/distinct first names). A reasonable default for \"had an invoice\" is at least one associated invoice. No ambiguous metric or method is required beyond a DISTINCT\/EXISTS join.","domain_specification_analysis":"The query lacks the organizational\/contextual boundary (which company\/database\/system). Interpreting it as \"all customers in the world\" is not a sensible universal default for invoicing data, so the domain is ambiguous despite specifying Brazil as a geographic filter.","entity_specification_analysis":"Core entities are \"customers\" and their \"invoices.\" Without specifying the dataset\/organization\/system, \"customers\" is undefined; multiple plausible populations exist. \"Brazil\" is a well-defined country, but it does not resolve which customers’ records to use.","temporal_specification_analysis":"The intent is \"ever had an invoice\" (existence at any time), which functions as an all-time filter; no explicit time frame is required to answer within a given dataset."},"meta":{},"created_at":"2025-10-13T10:55:30.644092Z","updated_at":"2025-10-13T11:12:11.627450Z","inner_id":74,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1785,"annotations":[{"id":748,"completed_by":1,"result":[{"id":"Ezb7MBPCtA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Y9ZR3ZiOrf","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"RurgqpYpnd","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"FD-K2Fhc-H","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"p3t5rVld33","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:53:41.846842Z","updated_at":"2025-10-22T05:53:41.846854Z","draft_created_at":"2025-10-22T05:53:04.741709Z","lead_time":59.882,"prediction":{"id":225,"task":1785,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765514Z","updated_at":"2025-10-13T10:55:30.765516Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a7371508-be95-425f-9f02-2f9351cd8996","import_id":null,"last_action":null,"bulk_created":false,"task":1785,"project":9,"updated_by":1,"parent_prediction":225,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[225],"data":{"id":3364,"query":"List the names of products priced at $150 or more manufactured by companies headquartered in Austin.","dataset":"MMQA","task_specification_analysis":"The user asks to list product names given clear filter conditions; it’s a straightforward retrieval task.","scope_specification_analysis":"Filters are fully defined: price ≥ $150 and manufacturer HQ location = Austin; the output field (product names) is explicit. No additional metrics or methods are required.","domain_specification_analysis":"The domain boundary is geographic: companies headquartered in Austin. In common usage this implies Austin, Texas, USA (dominant interpretation), reinforced by USD pricing. Thus the domain is effectively specified.","entity_specification_analysis":"Core entities are a broad but well-defined class: manufactured products from companies headquartered in Austin, with a clear price threshold (USD $150+). Broad classes are acceptable; \"manufactured\" narrows to physical goods. No single ambiguous entity reference.","temporal_specification_analysis":"No time frame given for prices, which can change. The most sensible default is current\/latest pricing (e.g., current MSRP\/retail). Lacking explicit timing makes it under-specified but with a reasonable recency assumption."},"meta":{},"created_at":"2025-10-13T10:55:30.644118Z","updated_at":"2025-10-22T05:53:41.894767Z","inner_id":75,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1786,"annotations":[{"id":770,"completed_by":1,"result":[{"id":"uPD6iN6PZR","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"hNvD8nIU-n","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"aw4Cq_08fy","type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"5zsOX0sG_9","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"prvL0oyR0V","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:08:20.573250Z","updated_at":"2025-10-22T06:08:20.573264Z","draft_created_at":"2025-10-22T06:08:15.784804Z","lead_time":18.099,"prediction":{"id":226,"task":1786,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765528Z","updated_at":"2025-10-13T10:55:30.765531Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"6712c928-613a-4641-8b5b-2a6a0049158b","import_id":null,"last_action":null,"bulk_created":false,"task":1786,"project":9,"updated_by":1,"parent_prediction":226,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[226],"data":{"id":3307,"query":"What are the full names of actors who had roles in more than 30 films?","dataset":"MMQA","task_specification_analysis":"The user requests a list of entities (actors) satisfying a clear threshold condition—having roles in more than 30 films—so the action is a straightforward filter\/list operation.","scope_specification_analysis":"The calculation is clear: group by actor, count films, filter where count > 30, and return full names. Reasonable default is to count distinct films per actor; no additional ranking or modeling is required.","domain_specification_analysis":"No geographic or organizational boundary is given; the sensible default is the global film industry (worldwide).","entity_specification_analysis":"The query targets the broad, well-defined class of \"actors\" and counts of their film roles; this is resolvable using global filmography sources (e.g., IMDb). No ambiguous entity references.","temporal_specification_analysis":"The intent is an all-time career total (cumulative number of films), so no explicit timeframe is required."},"meta":{},"created_at":"2025-10-13T10:55:30.644144Z","updated_at":"2025-10-22T06:08:20.626106Z","inner_id":76,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1787,"annotations":[{"id":816,"completed_by":1,"result":[{"id":"XqOW_u0Q3H","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"YgZYWM6pzQ","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"WEM92grvau","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"KFwrdl90yt","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"sdcHSy4ppv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:34:30.187224Z","updated_at":"2025-10-22T14:34:30.187237Z","draft_created_at":"2025-10-22T14:34:18.791244Z","lead_time":65.514,"prediction":{"id":227,"task":1787,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765543Z","updated_at":"2025-10-13T10:55:30.765545Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"b35c0d3a-fe1c-473c-8370-d6d6c6318902","import_id":null,"last_action":null,"bulk_created":false,"task":1787,"project":9,"updated_by":1,"parent_prediction":227,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[227],"data":{"id":3187,"query":"Where is the railway located that was built by Stephenson and operates a train named 'Andaman Exp'?","dataset":"MMQA","task_specification_analysis":"The query asks to return the location of a specific railway entity. This is a direct lookup task with clear intent: identify the railway meeting the given filters and report its location.","scope_specification_analysis":"The method is straightforward: filter railways by (a) built by Stephenson and (b) operates a train named 'Andaman Exp', then retrieve the entity’s location attribute. No ranking, aggregation, or modeling is required; a standard location field (e.g., place\/country\/region) can be returned.","domain_specification_analysis":"No geographic or organizational boundary is given. While \"Andaman Exp\" suggests Indian Railways, this is not explicitly stated, and \"railway\" could refer to any global railway company\/network. Hence the domain context is missing.","entity_specification_analysis":"Core entities are ambiguous. \"the railway\" is not uniquely identified; many railways exist. \"built by Stephenson\" does not specify which Stephenson (George, Robert, company, etc.). \"Andaman Exp\" is an abbreviated train name likely referring to \"Andaman Express\" in India, but this is not explicit and could have variants. The conjunction of these constraints does not resolve to a single entity.","temporal_specification_analysis":"The question asks for a location (a definitional attribute) and does not imply a specific time frame. No temporal filtering is necessary."},"meta":{},"created_at":"2025-10-13T10:55:30.644171Z","updated_at":"2025-10-22T14:34:30.235049Z","inner_id":77,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1788,"annotations":[{"id":846,"completed_by":1,"result":[{"id":"DxLb0P3vfE","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"NXmFge1Oq3","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"5gxr5PQ5AP","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"iSozrMXBdY","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Btl-DNndpu","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:03:19.405770Z","updated_at":"2025-10-22T15:03:19.405784Z","draft_created_at":"2025-10-22T15:03:11.096279Z","lead_time":18.773,"prediction":{"id":228,"task":1788,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765558Z","updated_at":"2025-10-13T10:55:30.765561Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"5b9fb76e-57ec-4164-8916-9648f90d3fb9","import_id":null,"last_action":null,"bulk_created":false,"task":1788,"project":9,"updated_by":1,"parent_prediction":228,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[228],"data":{"id":2969,"query":"What are the names of tourist attractions accessible by shuttle in location 868 and what type of shops are available there?","dataset":"MMQA","task_specification_analysis":"The user requests a filtered lookup and retrieval: identify tourist attractions meeting two conditions (in location 868 and accessible by shuttle) and return two attributes (attraction names and associated shop types). This is a clear list\/retrieve task.","scope_specification_analysis":"Scope is clear: apply filters (location = 868; accessibility by shuttle = true) and project fields (attraction name, shop type). No ambiguous metrics, ranking, or aggregation required. If multiple shop types exist per attraction, list them; this does not introduce analytical ambiguity.","domain_specification_analysis":"The query depends on a specific dataset\/system (e.g., a transit\/attractions database) but does not name it or a geographic\/organizational boundary; \"location 868\" is meaningless without that domain.","entity_specification_analysis":"References an opaque identifier \"location 868\" that is not globally resolvable and likely dataset-internal; \"tourist attractions accessible by shuttle\" and \"shops\" also depend on an unspecified context. Multiple plausible entities exist with no path to disambiguate.","temporal_specification_analysis":"No time frame provided; present-tense inventory-style question implies the latest\/current listings would be intended."},"meta":{},"created_at":"2025-10-13T10:55:30.644206Z","updated_at":"2025-10-22T15:03:19.461964Z","inner_id":78,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1789,"annotations":[{"id":740,"completed_by":1,"result":[{"id":"4P0EwaMiH_","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"i7R8W5ontI","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"QxSypbfyxq","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"wWyoqf30Er","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"P7ui9UqcUr","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:45:51.811324Z","updated_at":"2025-10-22T05:45:51.811338Z","draft_created_at":"2025-10-22T05:45:13.207046Z","lead_time":49.006,"prediction":{"id":229,"task":1789,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765573Z","updated_at":"2025-10-13T10:55:30.765575Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"f5ab2252-240d-47ba-b89e-d562aa9b02b3","import_id":null,"last_action":null,"bulk_created":false,"task":1789,"project":9,"updated_by":1,"parent_prediction":229,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[229],"data":{"id":3117,"query":"Which regions have been affected by storms with a maximum speed below 980 and had more than 20 cities affected?","dataset":"MMQA","task_specification_analysis":"The user wants a list of regions that meet certain criteria—this is a clear retrieval\/filtering task.","scope_specification_analysis":"Key analytical parameters are ambiguous: (1) Whether “had more than 20 cities affected” applies per storm (i.e., select storms with >20 cities affected, then return the regions those storms impacted) or at the region level (i.e., for each region, count affected cities across qualifying storms). (2) Whether to count unique cities or total city-impacts across multiple storms (deduplication rules). (3) The meaning\/metric of “maximum speed below 980” (unit\/field), though procedurally it’s a filter threshold; still, the attachment of the city-count condition materially changes outcomes. These ambiguities affect grouping and filtering logic and would require clarification.","domain_specification_analysis":"No geographic or organizational boundary is defined. It is unclear whether the scope is global or limited to a particular country\/region. The notion of \"affected\" and the count of cities are highly dataset-dependent, requiring a specified domain or source.","entity_specification_analysis":"Core entities are ambiguous. \"Regions\" is undefined (could be countries, states, administrative regions, or dataset-specific regions). \"Storms\" is unspecified (tropical cyclones vs. other storm types). The attribute \"maximum speed below 980\" lacks units and appears unrealistic for wind speed, suggesting possible confusion with pressure—this further undermines resolvability. \"Cities affected\" depends on a dataset-specific definition of \"affected.\"","temporal_specification_analysis":"No time frame is given. The condition \"had more than 20 cities affected\" could be cumulative over all time, per storm season, or a specific period; there is no safe default."},"meta":{},"created_at":"2025-10-13T10:55:30.644236Z","updated_at":"2025-10-22T05:45:51.867828Z","inner_id":79,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1790,"annotations":[{"id":760,"completed_by":1,"result":[{"id":"qzKPDryOoO","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"CdVOPu8k_Y","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"pua_EYheJ8","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"JbqUmAmIbj","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"P6xBaKuBfv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:02:03.891677Z","updated_at":"2025-10-22T06:02:03.891687Z","draft_created_at":"2025-10-22T06:01:33.417602Z","lead_time":44.408,"prediction":{"id":230,"task":1790,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765587Z","updated_at":"2025-10-13T10:55:30.765590Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"baff4666-a50c-4396-b551-60e92c6e0bac","import_id":null,"last_action":null,"bulk_created":false,"task":1790,"project":9,"updated_by":1,"parent_prediction":230,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[230],"data":{"id":2865,"query":"What is the average cost of treatments of type 'WALK' for dogs of breed 'ESK'?","dataset":"MMQA","task_specification_analysis":"The user requests a single statistic: compute the average (mean) cost. This is a clear, executable analytical action.","scope_specification_analysis":"Filters are fully defined (treatment type = 'WALK', breed = 'ESK'); the metric is specified (cost) and the aggregation function is the arithmetic mean. No additional parameters are needed.","domain_specification_analysis":"The query lacks a contextual boundary (which clinic, provider network, city\/country, or specific database). These codes imply an organizational schema; without naming the domain, relevant tables cannot be located.","entity_specification_analysis":"The core entities rely on dataset-specific codes: treatment type 'WALK' and breed code 'ESK' are not globally standardized identifiers. Without a named dataset or coding system, these entities cannot be uniquely resolved.","temporal_specification_analysis":"No time frame is provided. An average cost requires a period (e.g., last month, 2024, all-time); prices can change over time, so there is no safe default."},"meta":{},"created_at":"2025-10-13T10:55:30.644263Z","updated_at":"2025-10-22T06:02:03.929342Z","inner_id":80,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1791,"annotations":[{"id":791,"completed_by":1,"result":[{"id":"PmUVaWKUK7","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"w2A3n2KNO9","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"BvHdyL7nBP","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"BRQFMBAuco","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"cR-K516Ks0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:14:17.691653Z","updated_at":"2025-10-22T14:14:17.691673Z","draft_created_at":"2025-10-22T14:14:06.662989Z","lead_time":27.232,"prediction":{"id":231,"task":1791,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765602Z","updated_at":"2025-10-13T10:55:30.765604Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"c0471716-8466-44a7-95b1-23d0170cb138","import_id":null,"last_action":null,"bulk_created":false,"task":1791,"project":9,"updated_by":1,"parent_prediction":231,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[231],"data":{"id":3707,"query":"How many million people live in the Australian state where the Royal National Park is ?","dataset":"OTT-QA","task_specification_analysis":"The user requests a specific numeric value: the population (expressed in millions) of the Australian state containing the Royal National Park. This is a clear lookup\/quantification task.","scope_specification_analysis":"Procedure is straightforward: identify the state where the Royal National Park is located, obtain that state's population, and express it in millions. No ambiguous metric, ranking, or model choice is required. Any timing of the population estimate pertains to data selection, not the analytical method.","domain_specification_analysis":"The query explicitly constrains the context to an Australian state; the geographic domain is clear and unique.","entity_specification_analysis":"\"Royal National Park\" uniquely refers to the well-known national park in Australia, which resolves to the state of New South Wales; the target entity is that specific Australian state. Entities are clearly identifiable.","temporal_specification_analysis":"No time frame is given for the population, which is time-dependent; the sensible default is the most recent available estimate."},"meta":{},"created_at":"2025-10-13T10:55:30.644290Z","updated_at":"2025-10-22T14:14:17.769713Z","inner_id":81,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1792,"annotations":[{"id":789,"completed_by":1,"result":[{"id":"Sb8IUAzwEr","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"poCc68zRH2","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"ZTvxF7A3fd","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"8OfLYwsBUh","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"3BbxdLK5l-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:12:49.016337Z","updated_at":"2025-10-22T14:12:49.016352Z","draft_created_at":"2025-10-22T14:12:21.039556Z","lead_time":60.852,"prediction":{"id":232,"task":1792,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765616Z","updated_at":"2025-10-13T10:55:30.765619Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"af6f289a-758a-4cd4-96dd-0ce5b92fba2f","import_id":null,"last_action":null,"bulk_created":false,"task":1792,"project":9,"updated_by":1,"parent_prediction":232,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[232],"data":{"id":3502,"query":"The sponsor from Swords in Dublin sponsors a Cheltenham Festival race with this type of jump . ?","dataset":"OTT-QA","task_specification_analysis":"The query asks for a specific fact: the type of jump used in the Cheltenham Festival race sponsored by the sponsor based in Swords, Dublin. This is a direct lookup\/identify task.","scope_specification_analysis":"No calculations or modeling are required. The procedure is straightforward: identify the sponsor from Swords (e.g., Ryanair), locate the Cheltenham Festival race they sponsor, and retrieve the race’s jump type (e.g., hurdles vs fences). No ambiguous metrics or parameters are involved.","domain_specification_analysis":"Context is the Cheltenham Festival (horse racing) and a sponsor based in Swords, Dublin—this is a globally unique, well-known sporting domain with clear geographic reference.","entity_specification_analysis":"Core entities include the sponsor \"from Swords in Dublin\" and a Cheltenham Festival race. Within Cheltenham Festival sponsors, the Swords (Dublin) clue uniquely resolves to Ryanair, pointing to the Ryanair Chase; hence the type of jump (steeplechase fences) is identifiable. Entities are reasonably resolvable via world knowledge.","temporal_specification_analysis":"No time frame is given. Sponsorships can vary by year, but the likely intent is the current\/typical sponsor; moreover, the race’s jump type is stable over time. Default to latest available information."},"meta":{},"created_at":"2025-10-13T10:55:30.644316Z","updated_at":"2025-10-22T14:12:49.069598Z","inner_id":82,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1793,"annotations":[{"id":822,"completed_by":1,"result":[{"id":"GZ15pvW3aR","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"vOJ7uynfBT","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"JfkvlhbNnD","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"vP7_QcHqLv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"6DmJ6HEK4z","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:39:10.805490Z","updated_at":"2025-10-22T14:39:10.805501Z","draft_created_at":"2025-10-22T14:38:53.830375Z","lead_time":41.281,"prediction":{"id":233,"task":1793,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765631Z","updated_at":"2025-10-13T10:55:30.765633Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2e641376-dc47-49c7-8d08-0a7953c456d5","import_id":null,"last_action":null,"bulk_created":false,"task":1793,"project":9,"updated_by":1,"parent_prediction":233,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[233],"data":{"id":3451,"query":"Who was the place listed in the Carlifornia historical landmarks in Coalinga named after ?","dataset":"OTT-QA","task_specification_analysis":"The user asks a direct lookup question: identify the person after whom the specified landmark\/place was named. This is a simple factual retrieval (“who is it named after?”).","scope_specification_analysis":"No calculations or complex methods are needed. The operation is to locate the relevant landmark entry and extract its naming origin (the person’s name). This is a straightforward lookup without ranking, aggregation, or modeling.","domain_specification_analysis":"The domain is clearly bounded to California Historical Landmarks within Coalinga, a specific geographic and list-based context.","entity_specification_analysis":"The core entity is referred to only as \"the place\" among California Historical Landmarks in Coalinga. Without the landmark’s name or number, this cannot be uniquely identified, and multiple landmarks may exist.","temporal_specification_analysis":"The question asks about the eponym (who a place is named after), which is a stable historical fact and not tied to a specific time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.644342Z","updated_at":"2025-10-22T14:39:10.849261Z","inner_id":83,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1794,"annotations":[{"id":719,"completed_by":1,"result":[{"id":"lFIRK2_37M","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Fx5XyP9770","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"cGfshl4zRM","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"nelCoUD2Xm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"N_lpR4xmK4","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:56:09.115967Z","updated_at":"2025-10-21T22:56:09.115977Z","draft_created_at":"2025-10-21T22:55:45.963458Z","lead_time":60.772,"prediction":{"id":234,"task":1794,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765645Z","updated_at":"2025-10-13T10:55:30.765648Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"c02385d4-4622-4502-9371-d5d887ccec9f","import_id":null,"last_action":null,"bulk_created":false,"task":1794,"project":9,"updated_by":1,"parent_prediction":234,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[234],"data":{"id":3802,"query":"What other roller coaster is the newest at Gotham City Six Flags America similar to ?","dataset":"OTT-QA","task_specification_analysis":"The query asks for a specific identification: which other roller coaster the newest coaster in Gotham City at Six Flags America is similar to. This is an information lookup\/identification task.","scope_specification_analysis":"The term \"similar to\" is not operationalized (e.g., same manufacturer\/model, identical layout\/clone, same ride type, theme, or other criterion). Different similarity criteria could lead to different answers, so the method is not clearly defined.","domain_specification_analysis":"The domain is clearly bounded to the Gotham City area within the specific park Six Flags America (Maryland). This is a unique, self-contained context.","entity_specification_analysis":"Core entities: (1) \"the newest roller coaster at Gotham City [section of] Six Flags America\" and (2) \"the other roller coaster it is similar to.\" The identity of the \"newest\" coaster depends on a date, and \"similar to\" is undefined (clone\/model\/layout\/manufacturer\/theme), yielding multiple plausible matches. Thus, a researcher could not uniquely identify the target coaster or the comparison coaster without clarification.","temporal_specification_analysis":"\"Newest\" is a time-relative qualifier with no date or as-of point provided. Different years yield different \"newest\" coasters for that park\/section, so the timeframe is essential and missing."},"meta":{},"created_at":"2025-10-13T10:55:30.644369Z","updated_at":"2025-10-21T22:56:09.156264Z","inner_id":84,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1795,"annotations":[{"id":776,"completed_by":1,"result":[{"id":"TYD_3RbzO5","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"2iaAPJJ4j4","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"f7tXqlSBkc","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"ZoORnUpR5i","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"C9PIAqfqxK","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:12:47.110118Z","updated_at":"2025-10-22T06:12:47.110127Z","draft_created_at":"2025-10-22T06:12:28.016106Z","lead_time":34.074,"prediction":{"id":235,"task":1795,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765660Z","updated_at":"2025-10-13T10:55:30.765662Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"87d58b4d-1c84-4b8a-8326-de3d0b3ca40e","import_id":null,"last_action":null,"bulk_created":false,"task":1795,"project":9,"updated_by":1,"parent_prediction":235,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[235],"data":{"id":3801,"query":"The laureate affiliated with Washington University who won the prize for work on viruses was born in which state ?","dataset":"OTT-QA","task_specification_analysis":"Direct lookup: identify the specified laureate and return their state of birth.","scope_specification_analysis":"Filtering criteria are clear (affiliation with Washington University; prize for work on viruses). Operation is a simple attribute retrieval (birth state). No additional methodological choices required.","domain_specification_analysis":"The organizational boundary \"Washington University\" refers to Washington University in St. Louis (distinct from the University of Washington). The requested \"which state\" clearly implies U.S. states. The domain context is unambiguous.","entity_specification_analysis":"\"Laureate\" commonly implies a Nobel laureate; paired with \"won the prize for work on viruses\" this strongly points to the Nobel Prize in Physiology or Medicine. Among Washington University (St. Louis) affiliates, this uniquely identifies Charles M. Rice, making the target entity resolvable.","temporal_specification_analysis":"Asking the birthplace state is a stable biographical fact and does not require a time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.644395Z","updated_at":"2025-10-22T06:12:47.146395Z","inner_id":85,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1796,"annotations":[{"id":845,"completed_by":1,"result":[{"id":"xhFAxjTEY0","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"XyHoCjFB5t","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"7mtBFU0cyd","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"fExbStemJs","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"xt7Bkwh6zf","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:02:59.919508Z","updated_at":"2025-10-22T15:02:59.919518Z","draft_created_at":"2025-10-22T15:02:48.551641Z","lead_time":31.774,"prediction":{"id":236,"task":1796,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765674Z","updated_at":"2025-10-13T10:55:30.765677Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"4165ec51-90d5-4888-bc66-db8e1ea3b217","import_id":null,"last_action":null,"bulk_created":false,"task":1796,"project":9,"updated_by":1,"parent_prediction":236,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[236],"data":{"id":3597,"query":"How many buildings is part of the historic location which is in the city of Belleview in Marion County , Florida ?","dataset":"OTT-QA","task_specification_analysis":"The query requests a simple count of items: the number of buildings that are part of a specified historic location in Belleview, Marion County, Florida. The analytical action (count) is clear and executable.","scope_specification_analysis":"The method is straightforward: filter to the historic location in the given city and count associated buildings. No ranking, aggregation choices, or modeling parameters are required beyond a basic count. Any ambiguity about which specific historic location is a data identification issue, not a procedural one.","domain_specification_analysis":"The geographical boundary is clearly stated: the city of Belleview in Marion County, Florida. This provides a specific, unambiguous domain context.","entity_specification_analysis":"The query references \"the historic location\" in Belleview, Marion County, Florida without naming it. Belleview likely has multiple historic places\/listings; and \"historic location\" could mean a district, site, or listing (e.g., the Lake Lillian Neighborhood Historic District). This leaves the target entity ambiguous.","temporal_specification_analysis":"No time frame is given. The count of buildings associated with a historic district\/listing can differ between initial listing and current status or subsequent amendments, so the intended \"as of\" time is unclear."},"meta":{},"created_at":"2025-10-13T10:55:30.644421Z","updated_at":"2025-10-22T15:02:59.955666Z","inner_id":86,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1797,"annotations":[{"id":832,"completed_by":1,"result":[{"id":"K3_9IDfNw6","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"JQnhxIZX3w","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"8NeQu-G7SO","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"_nOtzb2V9_","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"wetC3ZiPoA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:55:08.107700Z","updated_at":"2025-10-22T14:55:08.107715Z","draft_created_at":"2025-10-22T14:54:37.062425Z","lead_time":59.773,"prediction":{"id":237,"task":1797,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765689Z","updated_at":"2025-10-13T10:55:30.765692Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a7df9028-e66c-4d96-8008-1e8622308e34","import_id":null,"last_action":null,"bulk_created":false,"task":1797,"project":9,"updated_by":1,"parent_prediction":237,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[237],"data":{"id":3864,"query":"In which year did one of the Loreto schools come second at the Riverview Gold Cup and the school located at Moore Park come 3rd ?","dataset":"OTT-QA","task_specification_analysis":"The query asks to identify the year(s) when two conditions were simultaneously true: (a) the 2nd-place finisher was any Loreto school, and (b) the 3rd-place finisher was the school located at Moore Park. This is a direct filter-and-return operation on a results-by-year dataset.","scope_specification_analysis":"The method is straightforward: filter records where position=2 is in the set of Loreto schools and position=3 equals the school whose location is Moore Park, then return the year(s). No ambiguous ranking metric or aggregation is required; membership in “Loreto schools” and the Moore Park school identity are data lookups, not methodological gaps.","domain_specification_analysis":"The contextual boundary is the Riverview Gold Cup regatta, which is a specific, commonly known event; this provides a clear domain.","entity_specification_analysis":"Core entities are ambiguous: (1) \"one of the Loreto schools\" refers to multiple campuses (e.g., Loreto Kirribilli, Loreto Normanhurst) without specifying which; (2) \"the school located at Moore Park\" is not uniquely identifiable (at least Sydney Boys High School and Sydney Girls High School are plausible); (3) the specific race\/category within the Riverview Gold Cup is not stated (the regatta has multiple events), so the target result table is unclear.","temporal_specification_analysis":"The query asks for the year when a particular outcome occurred; it does not require a time-frame filter. This is a historical lookup rather than a request constrained to a specific period."},"meta":{},"created_at":"2025-10-13T10:55:30.644448Z","updated_at":"2025-10-22T14:55:08.163277Z","inner_id":87,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1798,"annotations":[{"id":837,"completed_by":1,"result":[{"id":"JdJGAm03O-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"rRxim7UF15","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"wKPwQj1oXu","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"sPXdpg0kQ_","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"828WB1xTI0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:57:32.557988Z","updated_at":"2025-10-22T14:57:32.557999Z","draft_created_at":"2025-10-22T14:57:12.915239Z","lead_time":32.717,"prediction":{"id":238,"task":1798,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765704Z","updated_at":"2025-10-13T10:55:30.765707Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"548b5f6c-0314-4e8b-ba3d-d82b8c0ab25f","import_id":null,"last_action":null,"bulk_created":false,"task":1798,"project":9,"updated_by":1,"parent_prediction":238,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[238],"data":{"id":3768,"query":"In what year was the 1974 graduate of Utah State University born ?","dataset":"OTT-QA","task_specification_analysis":"A direct lookup question: retrieve the birth year of the specified individual (the 1974 graduate of Utah State University).","scope_specification_analysis":"No calculations or methodological choices needed; simply select the birth year attribute for the identified entity.","domain_specification_analysis":"The contextual boundary includes a specific, unique institution (Utah State University), which is unambiguous as a domain reference.","entity_specification_analysis":"The target entity is described only as \"the 1974 graduate of Utah State University,\" which could refer to many individuals. Without a name or additional identifiers, the person cannot be uniquely resolved.","temporal_specification_analysis":"The query asks for a birth year (a stable biographical fact). The mention of 1974 is part of the entity description (graduation year), not a timeframe for the data. No retrieval timeframe is needed."},"meta":{},"created_at":"2025-10-13T10:55:30.644474Z","updated_at":"2025-10-22T14:57:32.598695Z","inner_id":88,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1799,"annotations":[{"id":813,"completed_by":1,"result":[{"id":"2DjQSq1tWp","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"BdldkmML6A","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"UuJ6NfqK_P","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"EV44gKa8t1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"lAVqcbWi_H","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:32:13.755945Z","updated_at":"2025-10-22T14:32:13.755957Z","draft_created_at":"2025-10-22T14:31:26.673655Z","lead_time":58.688,"prediction":{"id":239,"task":1799,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765719Z","updated_at":"2025-10-13T10:55:30.765722Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"fcf82840-4b70-46a0-9563-ba3a2583c99a","import_id":null,"last_action":null,"bulk_created":false,"task":1799,"project":9,"updated_by":1,"parent_prediction":239,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[239],"data":{"id":3509,"query":"What type of OLAP server licence does the company founded by Praveen Kankariya require ?","dataset":"OTT-QA","task_specification_analysis":"This is a direct fact lookup: identify the type of OLAP server license required by the company founded by Praveen Kankariya. No computations or modeling are requested.","scope_specification_analysis":"No calculations, ranking, or aggregation are needed. The scope is to retrieve a single attribute (license type) for the specified entity. Any ambiguity about the company is a data identification issue, not a procedural one.","domain_specification_analysis":"The query’s boundary is organizational (the company in question). If the company were uniquely identified, no further geographic or sector boundary is needed.","entity_specification_analysis":"Core entity \"the company founded by Praveen Kankariya\" is ambiguous (he is associated with\/has founded more than one organization). Moreover, the object of the question—\"OLAP server licence type\"—likely pertains to a specific product or vendor context that isn’t identified. A researcher couldn’t reliably pick the correct company or licensing context.","temporal_specification_analysis":"No time frame is given. Licensing requirements can change; the present-tense phrasing implies the latest\/current requirement, but this is not explicit."},"meta":{},"created_at":"2025-10-13T10:55:30.644501Z","updated_at":"2025-10-22T14:32:13.837320Z","inner_id":89,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1800,"annotations":[{"id":817,"completed_by":1,"result":[{"id":"GEhGU7Car1","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"jxnLlsDNDw","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Erhx2i1cGJ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"VdDE4ARmRh","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"s6crXsVsO-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:34:59.761841Z","updated_at":"2025-10-22T14:34:59.761863Z","draft_created_at":"2025-10-22T14:34:50.949594Z","lead_time":28.816,"prediction":{"id":240,"task":1800,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765734Z","updated_at":"2025-10-13T10:55:30.765736Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"78bfb8ed-4216-4a17-bc50-ab7b642c9999","import_id":null,"last_action":null,"bulk_created":false,"task":1800,"project":9,"updated_by":1,"parent_prediction":240,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[240],"data":{"id":3414,"query":"How many municipalities are in the metro area of the city in Latin America with a population of 14,440,345 ?","dataset":"OTT-QA","task_specification_analysis":"The query asks for a single numeric value: the count of municipalities in the metro area of a specific city. This is a clear lookup\/count operation.","scope_specification_analysis":"The method is straightforward: identify the city (given by population) and count municipalities within its metro area as defined in the dataset. No additional parameters (ranking, grouping, model choice) are required.","domain_specification_analysis":"The geographic boundary \"Latin America\" is clear and commonly understood. While \"metro area\" definitions vary by country\/agency, the broader domain\/context is still the Latin American region, which is sufficiently specified for locating relevant data once the city is known.","entity_specification_analysis":"Core entity is \"the city in Latin America with a population of 14,440,345.\" Identifying a city solely by an exact population count is ambiguous without a year and scope (city proper vs metro vs urban agglomeration). Multiple Latin American cities could match that number under different sources\/years, and the subsequent reference to the city's \"metro area\" adds definitional ambiguity (which metro delineation). Therefore the target city cannot be uniquely resolved.","temporal_specification_analysis":"No time frame is provided for the population value used to identify the city, and metro-area compositions can change over time. Because the query uses a past-tense-like identifier (a specific population figure) rather than a general current-state statistic, recency cannot be safely assumed."},"meta":{},"created_at":"2025-10-13T10:55:30.644527Z","updated_at":"2025-10-22T14:34:59.850536Z","inner_id":90,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1801,"annotations":[{"id":715,"completed_by":1,"result":[{"id":"Q0iLXgvBM0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Gsg9OyErz-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Pwv3mOsY_L","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"98QzO2Lpbm","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"AxD5zXTBVu","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:51:41.415474Z","updated_at":"2025-10-21T22:51:41.415494Z","draft_created_at":"2025-10-21T22:51:20.418249Z","lead_time":47.424,"prediction":{"id":241,"task":1801,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765748Z","updated_at":"2025-10-13T10:55:30.765751Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"1ff62084-a4f3-47e9-8111-0e1e9153cd13","import_id":null,"last_action":null,"bulk_created":false,"task":1801,"project":9,"updated_by":1,"parent_prediction":241,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[241],"data":{"id":4290,"query":"When is the highest first elected, when is the result \"re-elected,\" and when is \"Lincoln Davis\" the incumbent for the United States House of Representatives election, 2006, Tennessee?","dataset":"OpenWikiTables","task_specification_analysis":"The query requests concrete operations: (1) find the record(s) with the maximum value of the 'First elected' field; (2) filter records where 'Result' equals 're-elected'; (3) filter records where 'Incumbent' equals 'Lincoln Davis'. These are identifiable actions (max and equality filters).","scope_specification_analysis":"Key procedural details are ambiguous: (a) The prompt uses “When is…” but does not specify what to return (e.g., district number, row index, date\/year, or the full row). (b) “Highest first elected” could mean most recent year (max) versus earliest (min), and tie-handling is unspecified. (c) For the filters, it’s unclear whether to return all matching rows and which fields to include. These choices can materially change the output.","domain_specification_analysis":"Domain boundary is the specific election context: Tennessee’s U.S. House races in 2006. This is a globally unique, self-contained scope (commonly the Wikipedia page \"2006 United States House of Representatives elections in Tennessee\").","entity_specification_analysis":"Core entities are uniquely identifiable: the event \"United States House of Representatives election, 2006, Tennessee\" (i.e., the 2006 U.S. House elections in Tennessee) and the person \"Lincoln Davis.\" References to table fields like \"First elected\" and \"Result\" map to standard columns on that event’s tables. Although phrasing like \"highest first elected\" is awkward, it still implies the max year in the 'First elected' column within this specific event’s table.","temporal_specification_analysis":"Explicitly tied to the 2006 election; no additional time frame needed."},"meta":{},"created_at":"2025-10-13T10:55:30.644555Z","updated_at":"2025-10-21T22:51:41.456466Z","inner_id":91,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1802,"annotations":[{"id":769,"completed_by":1,"result":[{"id":"HYVSPiV-b0","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"VK_UTEORxb","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Ie310m7j9Y","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"X33ZVrMLdT","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Jutqqt_kxZ","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:08:01.727700Z","updated_at":"2025-10-22T06:08:01.727711Z","draft_created_at":"2025-10-22T06:07:27.885208Z","lead_time":51.794,"prediction":{"id":242,"task":1802,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765763Z","updated_at":"2025-10-13T10:55:30.765766Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"66c3edcc-6035-473e-bfb4-2fa206a45b05","import_id":null,"last_action":null,"bulk_created":false,"task":1802,"project":9,"updated_by":1,"parent_prediction":242,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[242],"data":{"id":4074,"query":"How do the 96 stories rank among non-current projects in Shanghai's list of tallest buildings?","dataset":"OpenWikiTables","task_specification_analysis":"The user requests a ranking position (i.e., determine how 96 stories ranks among a subset), which is a clear analytical action.","scope_specification_analysis":"Key parameters are missing: (1) the ranking metric—by building height or by number of stories; (2) how ties should be handled; (3) the operational definition of \"non-current projects\" (e.g., proposed, on-hold, cancelled). These choices materially affect the result.","domain_specification_analysis":"The geographic\/contextual boundary is Shanghai and the scope is the city’s tallest buildings list, which is a well-known domain. The ambiguity lies in entities\/categories rather than the domain boundary.","entity_specification_analysis":"The subject is unclear: “96 stories” refers to a floor count, not a specific building, and there may be multiple 96‑story entries. “Non-current projects” is not a uniquely defined set (could mean cancelled, on hold, proposed). The ranking basis (by height vs by floor count) is also unclear.","temporal_specification_analysis":"No time frame is given and rankings\/project statuses change over time. The likely intent is to use the latest available version of the list."},"meta":{},"created_at":"2025-10-13T10:55:30.644582Z","updated_at":"2025-10-22T06:08:01.770895Z","inner_id":92,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1803,"annotations":[{"id":779,"completed_by":1,"result":[{"id":"dKWJUCZEIi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Iv9THp4M-i","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"3cv8ermuZr","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"GDPA6wvdyF","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"w39FtYsgjC","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:14:51.646757Z","updated_at":"2025-10-22T06:14:51.646766Z","draft_created_at":"2025-10-22T06:14:14.711791Z","lead_time":57.379,"prediction":{"id":243,"task":1803,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765778Z","updated_at":"2025-10-13T10:55:30.765781Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2dc41391-3b45-4970-beaf-7717c3ba0d75","import_id":null,"last_action":null,"bulk_created":false,"task":1803,"project":9,"updated_by":1,"parent_prediction":243,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[243],"data":{"id":4281,"query":"What is the number of points for the Dallas Cowboys' 1971 season on October 11?","dataset":"OpenWikiTables","task_specification_analysis":"This is a direct lookup request: retrieve the number of points scored by the Dallas Cowboys on a specific date within the 1971 season.","scope_specification_analysis":"No calculation is required beyond identifying the game on Oct 11, 1971 and reading the points scored by Dallas. The metric ('points for' the team in that game) is standard and unambiguous for such queries.","domain_specification_analysis":"Domain is unambiguous: professional American football (NFL), Dallas Cowboys. No additional organizational or geographic boundary is needed.","entity_specification_analysis":"Core entities are clear: the Dallas Cowboys NFL team and their 1971 season. This uniquely identifies the team-season context. Note: the metric \"number of points\" could mean points scored in the Oct 11 game vs. cumulative season points as of that date, but this is a metric ambiguity, not an entity ambiguity.","temporal_specification_analysis":"The query specifies \"October 11\" within the 1971 season context, which reasonably resolves to October 11, 1971 (a specific game date)."},"meta":{},"created_at":"2025-10-13T10:55:30.644611Z","updated_at":"2025-10-22T06:14:51.684367Z","inner_id":93,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1804,"annotations":[{"id":844,"completed_by":1,"result":[{"id":"B9pKJtAxM0","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"jfRVRD5_cN","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"gZXWhtc0_p","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"VqC7NeBJ8q","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Ehr5i2q4st","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:02:27.464987Z","updated_at":"2025-10-22T15:02:27.464999Z","draft_created_at":"2025-10-22T15:01:36.635157Z","lead_time":73.186,"prediction":{"id":244,"task":1804,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765858Z","updated_at":"2025-10-13T10:55:30.765862Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"8c1e96c0-447b-4b5d-bbfe-8ef69d6fa9b4","import_id":null,"last_action":null,"bulk_created":false,"task":1804,"project":9,"updated_by":1,"parent_prediction":244,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[244],"data":{"id":4064,"query":"What are the average tries with less than 0 goals for the 2008 Bulldogs RLFC season?","dataset":"OpenWikiTables","task_specification_analysis":"Compute an average (mean) value: the user asks for the average number of tries under a stated condition.","scope_specification_analysis":"Method is clear: filter 2008 Bulldogs RLFC season records to those with goals < 0, then take the arithmetic mean of tries (implicitly per match). While the condition is likely to yield no rows in practice, the procedural steps are unambiguous.","domain_specification_analysis":"The contextual boundary (league\/competition and geography) is missing—NRL vs other rugby leagues is not specified. Without a unique club\/competition, the domain cannot be bounded.","entity_specification_analysis":"\"Bulldogs RLFC\" is ambiguous—multiple rugby league clubs are nicknamed\/termed \"Bulldogs\" and use RLFC. It’s unclear which club is meant (e.g., Canterbury-Bankstown Bulldogs vs other Bulldogs clubs). Additionally, it’s unclear whether the aggregation is over matches or players, which affects how \"tries\" and \"goals\" are interpreted.","temporal_specification_analysis":"\"2008 ... season\" provides a clear, resolvable time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.644638Z","updated_at":"2025-10-22T15:02:27.509814Z","inner_id":94,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1805,"annotations":[{"id":754,"completed_by":1,"result":[{"id":"eUbjlof5Ez","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"EEj13gj5Nm","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"QAMOgSYr52","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"nAFaxVFZlX","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"GSaNaIGJ5S","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:58:46.879813Z","updated_at":"2025-10-22T05:58:46.879824Z","draft_created_at":"2025-10-22T05:57:47.892412Z","lead_time":77.862,"prediction":{"id":245,"task":1805,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765876Z","updated_at":"2025-10-13T10:55:30.765879Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"5c681844-802c-4d04-852d-acee56c2c09b","import_id":null,"last_action":null,"bulk_created":false,"task":1805,"project":9,"updated_by":1,"parent_prediction":245,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[245],"data":{"id":4119,"query":"Which country has an IATA from gzt under Atlasjet?","dataset":"OpenWikiTables","task_specification_analysis":"The intent is unclear: \"IATA from gzt under Atlasjet\" could mean (a) the country where the airport with IATA code GZT is located; (b) the destination countries served by Atlasjet from GZT; or (c) something about Atlasjet’s own IATA code. These alternatives would yield different answers and require different lookups. Hence the analytical action is not clearly specified.","scope_specification_analysis":"No complex calculations are implied; the request appears to be a simple lookup\/filter. However, because the task itself is ambiguous, the computation steps cannot be executed until the intended interpretation is clarified.","domain_specification_analysis":"The context is global aviation; mapping an IATA airport code to its country is unambiguous and does not require an additional boundary. Mention of Atlasjet does not introduce domain ambiguity.","entity_specification_analysis":"The entities are resolvable: 'gzt' is the unique IATA airport code for Gaziantep Airport, and 'Atlasjet' uniquely refers to the Turkish airline (Atlasglobal). The target attribute is the country of that airport.","temporal_specification_analysis":"This is a stable, definitional mapping (airport code to country) and does not require a time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.644664Z","updated_at":"2025-10-22T05:58:46.924363Z","inner_id":95,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1806,"annotations":[{"id":782,"completed_by":1,"result":[{"id":"IMroNmSFWs","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"6w92UlbzKy","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"WV2XNN3-mx","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"u6JRru1bQ-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"DFKvmoQO_G","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:06:02.627300Z","updated_at":"2025-10-22T14:06:02.627315Z","draft_created_at":"2025-10-22T14:05:43.279139Z","lead_time":47.78,"prediction":{"id":246,"task":1806,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765891Z","updated_at":"2025-10-13T10:55:30.765894Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a516cd09-d019-4357-83d9-d1f9b2cac919","import_id":null,"last_action":null,"bulk_created":false,"task":1806,"project":9,"updated_by":1,"parent_prediction":246,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[246],"data":{"id":3983,"query":"What elimination number is listed as Sonjay Dutt's elimination in Genesis (2009)?","dataset":"OpenWikiTables","task_specification_analysis":"The query requests a single factual lookup: retrieve the elimination number associated with Sonjay Dutt at the event Genesis (2009). No additional interpretation or computation is required.","scope_specification_analysis":"This is a direct attribute lookup from the identified event\/participant record; no ranking, aggregation, or calculation parameters are needed.","domain_specification_analysis":"The domain is the specific pro wrestling event (TNA\/Impact Wrestling Genesis 2009). Mention of Sonjay Dutt disambiguates from other uses of \"Genesis\" (e.g., Smash tournaments).","entity_specification_analysis":"Entities are uniquely identifiable: Sonjay Dutt (the professional wrestler) and the event \"Genesis (2009)\" (TNA\/Impact Wrestling PPV). The query seeks the order of his elimination in that event, which is a resolvable attribute tied to that specific event\/match.","temporal_specification_analysis":"Time is inherent in the event name \"Genesis (2009)\", referencing a specific dated event; no additional timeframe needed."},"meta":{},"created_at":"2025-10-13T10:55:30.644691Z","updated_at":"2025-10-22T14:06:02.683511Z","inner_id":96,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1807,"annotations":[{"id":796,"completed_by":1,"result":[{"id":"XTdWNz0cO8","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"B0dpNnhnNi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"U9DQhCwYom","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"N9q95Xuh4S","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"IzGmHe_LmY","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:21:04.551768Z","updated_at":"2025-10-22T14:21:04.551780Z","draft_created_at":"2025-10-22T14:20:48.694349Z","lead_time":108.675,"prediction":{"id":247,"task":1807,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765906Z","updated_at":"2025-10-13T10:55:30.765909Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"1f183705-cf8e-4d9c-b654-193e23771808","import_id":null,"last_action":null,"bulk_created":false,"task":1807,"project":9,"updated_by":1,"parent_prediction":247,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[247],"data":{"id":4301,"query":"What was Mike Hailwood's highest score among full Formula One results with a Ford engine and a Brooke Bond Oxo Team Surtees competitor in 1972?","dataset":"OpenWikiTables","task_specification_analysis":"The query asks for a single value: the maximum “score” achieved by Mike Hailwood in a filtered subset of his 1972 Formula One race results. This is a clear max\/lookup task.","scope_specification_analysis":"The method is clear: filter results to 1972 Formula One entries where the engine is Ford and the entrant\/team is Brooke Bond Oxo Team Surtees, then take the maximum of the score\/points column across those races. “Highest score” reasonably maps to points in standard F1 race result tables. No additional analytical parameters are needed.","domain_specification_analysis":"The domain is clearly the Formula One World Championship results context, which is globally unique and well-bounded.","entity_specification_analysis":"The entities are uniquely identifiable: Mike Hailwood (F1 driver), the Brooke Bond Oxo Team Surtees entrant\/team in 1972, and cars with a Ford engine (Ford-Cosworth DFV). \"Full Formula One results\" maps to standard \"Complete Formula One World Championship results\" tables for the driver. While \"score\" likely means race points, this does not hinder locating the relevant data tables.","temporal_specification_analysis":"The query explicitly specifies the year 1972, referring to that F1 season's races."},"meta":{},"created_at":"2025-10-13T10:55:30.644717Z","updated_at":"2025-10-22T14:21:04.596597Z","inner_id":97,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1808,"annotations":[{"id":830,"completed_by":1,"result":[{"id":"jVtOqFNdcn","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"iHhFZP8yYW","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"uNLClGZ-fm","type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"jTA3sLxHIs","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"ZjDmSImd9L","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:53:25.880954Z","updated_at":"2025-10-22T14:53:25.880964Z","draft_created_at":"2025-10-22T14:53:18.073925Z","lead_time":22.166,"prediction":{"id":248,"task":1808,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765921Z","updated_at":"2025-10-13T10:55:30.765924Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"c81bcb85-cfaf-412b-ba37-eaa65f05e3c6","import_id":null,"last_action":null,"bulk_created":false,"task":1808,"project":9,"updated_by":1,"parent_prediction":248,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[248],"data":{"id":4219,"query":"How many competitions has Dietmar Mögenburg completed in total?","dataset":"OpenWikiTables","task_specification_analysis":"The request asks for a single aggregate: the total number of competitions completed by Dietmar Mögenburg. This is a clear count operation.","scope_specification_analysis":"Method is straightforward: count the number of competition entries for the athlete that are considered completed (commonly those with a recorded result\/mark, excluding DNS\/DNF\/DSQ). No grouping, ranking, or additional parameters are required; “in total” implies across the full available career data.","domain_specification_analysis":"\"Competitions\" is not bounded: it’s unclear whether to include only international championships, all sanctioned meets, national\/club meets, indoor\/outdoor, youth categories, qualifiers vs finals, and what \"completed\" means (entered, finished, or recorded a valid result). Different sources (World Athletics, federation records, databases like Tilastopaja) would yield different counts. Without a defined scope, a researcher cannot select the correct tables.","entity_specification_analysis":"The subject \"Dietmar Mögenburg\" is a uniquely identifiable retired German high jumper; a capable system can resolve this person unambiguously.","temporal_specification_analysis":"The phrase \"in total\" implies an all-time, career-cumulative count, so no further time frame is needed."},"meta":{},"created_at":"2025-10-13T10:55:30.644744Z","updated_at":"2025-10-22T14:53:25.917265Z","inner_id":98,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1809,"annotations":[{"id":714,"completed_by":1,"result":[{"id":"IH-ibpke20","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"03p2-ljC05","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"bFSN7dlxgp","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"eFA45IWINt","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"WCxGzhJqn3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:50:53.245577Z","updated_at":"2025-10-21T22:50:53.245590Z","draft_created_at":"2025-10-21T22:49:57.769485Z","lead_time":72.719,"prediction":{"id":249,"task":1809,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765936Z","updated_at":"2025-10-13T10:55:30.765939Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"e396ca43-b9cc-4fbe-9f32-3d0d0b925902","import_id":null,"last_action":null,"bulk_created":false,"task":1809,"project":9,"updated_by":1,"parent_prediction":249,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[249],"data":{"id":4013,"query":"From the list of multiplayer browser games, how many games has Jagex released?","dataset":"OpenWikiTables","task_specification_analysis":"The user asks for a count of games attributable to Jagex from a defined list. This is a clear counting task.","scope_specification_analysis":"The method is to filter the list for entries released\/published by Jagex and count the resulting games (distinct titles). This is a straightforward filter-and-count with a reasonable default interpretation of \"released\" as \"published.\"","domain_specification_analysis":"The query relies on 'the list of multiplayer browser games' without naming a specific source (e.g., a particular Wikipedia page, category, or curated list). Multiple plausible lists exist with differing inclusion criteria, so the boundary is unclear.","entity_specification_analysis":"Core entity 'Jagex' is uniquely identifiable. The class 'multiplayer browser games' is intended as the population defined by the referenced list; ambiguity about the list itself is treated under domain, not entity.","temporal_specification_analysis":"No explicit timeframe given. The phrasing 'has released' implies a cumulative count up to the present, so the reasonable intent is to use the most recent available version of the list\/source."},"meta":{},"created_at":"2025-10-13T10:55:30.644770Z","updated_at":"2025-10-21T22:50:53.293828Z","inner_id":99,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1810,"annotations":[{"id":716,"completed_by":1,"result":[{"id":"e6pvmpVlm4","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"s0nWP8kaKi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"e3XV0ITMX-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"59CSfXrDpS","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"},{"id":"fJQN2vbPhz","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:53:12.358804Z","updated_at":"2025-10-21T22:53:12.358815Z","draft_created_at":"2025-10-21T22:52:31.123470Z","lead_time":90.352,"prediction":{"id":250,"task":1810,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765951Z","updated_at":"2025-10-13T10:55:30.765954Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"f0dd49c8-2e67-4c09-a2bd-b9e476be5226","import_id":null,"last_action":null,"bulk_created":false,"task":1810,"project":9,"updated_by":1,"parent_prediction":250,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[250],"data":{"id":4323,"query":"What is the total number of dates if partner \"Carl-Uwe Steeb\" for 6 doubles titles is Karel Nováček?","dataset":"OpenWikiTables","task_specification_analysis":"The query requests a count (“total number of dates”) subject to a condition about a partner, which is an identifiable analytical action (a count).","scope_specification_analysis":"Key parameters are unclear: what exactly constitutes a “date” in this context (tournament start date, final date, title date, etc.); whether to count distinct dates or all rows; how to interpret “for 6 doubles titles” (restrict to the six titles of Steeb overall, or specifically the titles where the partner is Nováček, or verify that there are six); and which date field to use if multiple exist. These choices materially affect the result.","domain_specification_analysis":"The contextual boundary is missing: it is unclear whether to use ATP Tour only, include Grand Slams, Challengers\/ITF, Davis Cup, or a specific dataset schema (which defines the \"dates\" field). Without specifying the competition level\/data source, a researcher cannot confidently select the correct tables or inclusion rules.","entity_specification_analysis":"The query names two unique professional tennis players, \"Karel Nováček\" and \"Carl-Uwe Steeb,\" which are unambiguous entities. While \"6 doubles titles\" likely refers to Nováček’s career doubles titles, the core subjects (the players) are still uniquely identifiable.","temporal_specification_analysis":"The question implies an all-time\/career aggregation (counting dates associated with titles), so a time frame is implicitly the player’s entire career and no additional temporal filter is needed."},"meta":{},"created_at":"2025-10-13T10:55:30.644803Z","updated_at":"2025-10-21T22:53:12.403765Z","inner_id":100,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1811,"annotations":[{"id":726,"completed_by":1,"result":[{"id":"mz7hK_9Toc","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"--XLYbTYYn","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"f1d3XfibdB","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"I-R3yZzcQX","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"APipW3lgjy","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:33:14.314684Z","updated_at":"2025-10-22T05:33:14.314697Z","draft_created_at":"2025-10-21T23:00:23.645901Z","lead_time":23581.93,"prediction":{"id":251,"task":1811,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765966Z","updated_at":"2025-10-13T10:55:30.765969Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"e482a3ef-fb97-4b35-8590-61bf3adfd497","import_id":null,"last_action":null,"bulk_created":false,"task":1811,"project":9,"updated_by":1,"parent_prediction":251,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[251],"data":{"id":4691,"query":"What was the total amount of elements greater than 200 for 2012-2017?","dataset":"QTSumm","task_specification_analysis":"The user is asking for an aggregate over items exceeding a threshold within a time range—i.e., compute a total for elements > 200 from 2012 to 2017.","scope_specification_analysis":"Key analytical parameters are ambiguous: \"total amount\" could mean sum of values or count of elements; also unclear if the total is across the whole 2012–2017 period or reported per year. These choices materially change the result.","domain_specification_analysis":"No dataset, organization, geography, or context is given; the domain boundary is missing and cannot be inferred.","entity_specification_analysis":"\"elements greater than 200\" does not identify what the elements are (values in which column\/measure), and \"total amount\" is ambiguous (count vs sum). No subject (e.g., sales, temperatures, cases) is given.","temporal_specification_analysis":"The timeframe \"2012-2017\" is explicitly provided."},"meta":{},"created_at":"2025-10-13T10:55:30.644830Z","updated_at":"2025-10-22T05:33:14.358132Z","inner_id":101,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1812,"annotations":[{"id":842,"completed_by":1,"result":[{"id":"xaJZYUziah","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"b17JGuMR8y","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"aIlmGlpSeL","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"BtgBaHHzPD","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"-4f4fG520K","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:00:09.297448Z","updated_at":"2025-10-22T15:00:09.297461Z","draft_created_at":"2025-10-22T14:59:48.195414Z","lead_time":35.707,"prediction":{"id":252,"task":1812,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765980Z","updated_at":"2025-10-13T10:55:30.765983Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"ac7444ce-c8e0-45eb-82e3-fd3d27112e0c","import_id":null,"last_action":null,"bulk_created":false,"task":1812,"project":9,"updated_by":1,"parent_prediction":252,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[252],"data":{"id":4628,"query":"based on the black-scholes option pricing model what was the percent of the change in the option prices from 2003 to 2004","dataset":"QTSumm","task_specification_analysis":"The user requests a percent change in option prices between 2003 and 2004, which is an explicit calculation (e.g., (2004 value − 2003 value) \/ 2003 value × 100%).","scope_specification_analysis":"Key analytical parameters are missing: which options (underlying, call\/put, strike, maturity), whether this refers to a specific contract or an aggregate across many contracts, and if aggregated, what statistic or weighting to use (mean\/median, volume- or open-interest-weighted). The temporal reference within each year (end-of-year, annual average, specific date) is unspecified. For Black–Scholes pricing, required inputs (risk-free rate term, dividend yield, volatility estimation method\/window, time-to-maturity alignment) are not defined. These choices materially affect results.","domain_specification_analysis":"No market, exchange, geography, asset class, or product scope is provided (e.g., SPX options on CBOE, options on a specific stock, or an index basket). The domain boundary is missing and multiple plausible domains exist.","entity_specification_analysis":"\"Option prices\" is not tied to any identifiable set of options. The query does not specify underlying asset(s), option type (call\/put), strike(s), maturities, moneyness, or input assumptions (volatility, rates, dividends) needed for Black‑Scholes pricing. Therefore the core subject is ambiguous and cannot be located in data without clarification.","temporal_specification_analysis":"The time frame \"from 2003 to 2004\" is explicit, indicating a comparison between those years."},"meta":{},"created_at":"2025-10-13T10:55:30.644856Z","updated_at":"2025-10-22T15:00:09.351560Z","inner_id":102,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1813,"annotations":[{"id":745,"completed_by":1,"result":[{"id":"GGaZLcZX8U","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"LmMN3_s93f","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"T5seZsrsNZ","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Y5OdK5QTqf","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"rVLJAjBQyU","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:51:19.790051Z","updated_at":"2025-10-22T05:51:19.790063Z","draft_created_at":"2025-10-22T05:50:53.751592Z","lead_time":42.94,"prediction":{"id":253,"task":1813,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.765995Z","updated_at":"2025-10-13T10:55:30.765997Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"36ba22aa-5d1d-4fd2-8905-f7086794f2a2","import_id":null,"last_action":null,"bulk_created":false,"task":1813,"project":9,"updated_by":1,"parent_prediction":253,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[253],"data":{"id":4756,"query":"What's the average of Interest cost in pension of U.S plans in 2014 and 2015? (in million)","dataset":"QTSumm","task_specification_analysis":"The user requests a clear analytical action: compute the average (arithmetic mean) of a specified metric for a subset and time period.","scope_specification_analysis":"The metric is “Interest cost in pension,” filtered to U.S. plans, for years 2014 and 2015. A standard, unweighted per-year mean can be assumed, returning values in millions. Group by year and compute mean; no additional methodological parameters are required.","domain_specification_analysis":"While a geographic hint (U.S) is provided, the necessary organizational boundary (which company\/industry or national aggregate) is missing. Different datasets would apply depending on interpretation, making the domain ambiguous.","entity_specification_analysis":"Core entities are \"Interest cost in pension\" and \"U.S plans.\" The metric is a standard pension cost component, but the subject (which plans) is not identified. It could mean all U.S defined benefit plans in aggregate or the U.S plans of a specific company; without a sponsor\/company or dataset, multiple plausible targets exist.","temporal_specification_analysis":"Explicit years (2014 and 2015) are given; the query asks for the average across these two years."},"meta":{},"created_at":"2025-10-13T10:55:30.644883Z","updated_at":"2025-10-22T05:51:19.835299Z","inner_id":103,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1814,"annotations":[{"id":799,"completed_by":1,"result":[{"id":"Pjkb4umKPM","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"5nOt0ZmJNV","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"zGlSjWX3I-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"i0CDAVAaqc","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"2mBdGWlPVo","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:23:45.129689Z","updated_at":"2025-10-22T14:23:45.129700Z","draft_created_at":"2025-10-22T14:23:25.752590Z","lead_time":37.705,"prediction":{"id":254,"task":1814,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766010Z","updated_at":"2025-10-13T10:55:30.766012Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"6a738e10-84a2-470a-ac31-9028531f71bc","import_id":null,"last_action":null,"bulk_created":false,"task":1814,"project":9,"updated_by":1,"parent_prediction":254,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[254],"data":{"id":4850,"query":"what is the roi of an investment in loews common stock from 2011 to 2012?","dataset":"QTSumm","task_specification_analysis":"The user asks to compute the ROI for an investment in Loews common stock over a specified period (2011 to 2012). This is a clear metric calculation request (compute return over a period).","scope_specification_analysis":"Key analytical parameters are missing: the precise ROI definition (price return vs. total return including dividends), whether dividends are reinvested, exact start\/end dates within 2011–2012 (e.g., first\/last trading day, calendar year closes), whether to report cumulative return or annualized (CAGR), and whether to use split\/adjusted prices. These choices materially change the result.","domain_specification_analysis":"The context is the public equity market for Loews Corporation; no additional geographic or organizational boundary is needed.","entity_specification_analysis":"\"Loews common stock\" reasonably resolves to Loews Corporation (NYSE: L) common equity; the generic \"an investment\" can be evaluated per-dollar so no additional entity detail is required.","temporal_specification_analysis":"The time frame \"from 2011 to 2012\" provides a clear period (interpretable as the 2011–2012 calendar years); precise endpoints (first\/last trading day) can be operationalized without further clarification."},"meta":{},"created_at":"2025-10-13T10:55:30.644910Z","updated_at":"2025-10-22T14:23:45.170415Z","inner_id":104,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1815,"annotations":[{"id":847,"completed_by":1,"result":[{"id":"73hEetQt6T","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"0SstjDaIBS","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"LJnIeUC0Al","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"K60sL1o-0t","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"AF9fORpFX1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:06:29.395163Z","updated_at":"2025-10-22T15:06:29.395176Z","draft_created_at":"2025-10-22T15:03:38.009885Z","lead_time":188.912,"prediction":{"id":255,"task":1815,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766024Z","updated_at":"2025-10-13T10:55:30.766027Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"336cdef8-280a-44a2-b457-adf77dafdb77","import_id":null,"last_action":null,"bulk_created":false,"task":1815,"project":9,"updated_by":1,"parent_prediction":255,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[255],"data":{"id":4504,"query":"What's the average of the Gain (loss) on sales of real estate, net in the years where NAREIT FFO is positive?","dataset":"QTSumm","task_specification_analysis":"Compute an average of a specified metric under a filter condition; this is a clear aggregation request.","scope_specification_analysis":"Filter to years with NAREIT FFO > 0, then take the arithmetic mean of 'Gain (loss) on sales of real estate, net' over those years. Standard unweighted mean is implied; no additional parameters required.","domain_specification_analysis":"No organizational\/geographical boundary is provided (e.g., specific REIT, all US REITs, global REITs). Mention of NAREIT suggests a context but does not uniquely define the domain.","entity_specification_analysis":"The query names metrics\/line items ('Gain (loss) on sales of real estate, net' and 'NAREIT FFO') but does not specify the subject entity (which REIT\/company) or whether it should be across all REITs, so the core subject is ambiguous.","temporal_specification_analysis":"It filters on 'years where NAREIT FFO is positive' but provides no overall timeframe; the relevant years depend on the unspecified dataset\/entity, and there is no clear recency or all-time scope."},"meta":{},"created_at":"2025-10-13T10:55:30.644936Z","updated_at":"2025-10-22T15:06:29.443402Z","inner_id":105,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1816,"annotations":[{"id":823,"completed_by":1,"result":[{"id":"f5XbvNoUmU","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"ngr6NFtQSv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"M5xA3Koi0K","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"NkQje0h7b-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"ZcH4E1X_0g","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:39:38.211811Z","updated_at":"2025-10-22T14:39:38.211825Z","draft_created_at":"2025-10-22T14:39:27.594368Z","lead_time":26.794,"prediction":{"id":256,"task":1816,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766039Z","updated_at":"2025-10-13T10:55:30.766041Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"b8487827-3c0f-408e-b3c1-e52a0f07385c","import_id":null,"last_action":null,"bulk_created":false,"task":1816,"project":9,"updated_by":1,"parent_prediction":256,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[256],"data":{"id":4672,"query":"Does Planned generation (TWh) keeps increasing each year between 2005 and 2006?","dataset":"QTSumm","task_specification_analysis":"The user requests a yes\/no determination: whether Planned generation increases year-over-year across the specified period. This is a clear, executable check.","scope_specification_analysis":"The method is implicit: verify a strictly increasing trend by comparing the value in 2006 to 2005 (since the range yields a single year-over-year step). No additional aggregation or ranking needed.","domain_specification_analysis":"No geographical, organizational, or sectoral boundary is provided (global vs. country, grid operator, utility, fuel type, etc.). The concept of \"planned generation\" depends on a planning authority\/source, so a universal default cannot be assumed.","entity_specification_analysis":"\"Planned generation (TWh)\" appears to be a metric\/column label from an unspecified dataset. It lacks identification of what is being planned (e.g., electricity generation by fuel), for which entity (country, utility, region), or which data source. Multiple plausible interpretations exist, so the core subject cannot be uniquely resolved.","temporal_specification_analysis":"The timeframe \"between 2005 and 2006\" is explicit (effectively checking change from 2005 to 2006). Although the phrasing \"each year\" is awkward for a two-year span, the time window is clear."},"meta":{},"created_at":"2025-10-13T10:55:30.644963Z","updated_at":"2025-10-22T14:39:38.265295Z","inner_id":106,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1817,"annotations":[{"id":767,"completed_by":1,"result":[{"id":"4KDC9ZSCsE","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"DXuuAV3t6F","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"ZLBlxXDfRe","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"C5ccc7UTG9","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"gQ9kZ1eh7-","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:06:29.795401Z","updated_at":"2025-10-22T06:06:29.795411Z","draft_created_at":"2025-10-22T06:06:08.738150Z","lead_time":36.128,"prediction":{"id":257,"task":1817,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766053Z","updated_at":"2025-10-13T10:55:30.766056Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"5cb79722-937c-47cc-aee0-b3f668781cd2","import_id":null,"last_action":null,"bulk_created":false,"task":1817,"project":9,"updated_by":1,"parent_prediction":257,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[257],"data":{"id":4415,"query":"What's the average of Service and Maintenance in 2014? (in thousand)","dataset":"QTSumm","task_specification_analysis":"The user asks to compute an average, which is a clear analytical action (compute the mean), and specifies the year filter (2014) and the output unit (thousand).","scope_specification_analysis":"It is unclear what items are being averaged and in what sequence. Ambiguities include: (1) whether “Service” and “Maintenance” are two separate categories to average their 2014 values, or a single combined category; (2) if data has multiple periods within 2014 (e.g., monthly), whether to average across months first per category, then average across categories, or average all observations together; (3) whether any weighting is intended. These choices can materially change the result.","domain_specification_analysis":"No contextual boundary is provided (e.g., which organization, geography, industry, or dataset). The unit cue \"(in thousand)\" does not define a domain. The query relies on an unknown table-specific context.","entity_specification_analysis":"The entities \"Service\" and \"Maintenance\" are undefined labels likely referring to columns or categories in some dataset. There is no indication of the subject (company, government, sector) or the metric (costs, revenues, incidents), so multiple plausible interpretations exist and the data cannot be located unambiguously.","temporal_specification_analysis":"The time frame \"in 2014\" is explicit and unambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.644992Z","updated_at":"2025-10-22T06:06:29.832272Z","inner_id":107,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1818,"annotations":[{"id":821,"completed_by":1,"result":[{"id":"Z4XtYyXRnc","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"eECNhXDk3l","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"qZEfL6By4m","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"rof2XoBa4j","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"JivGk2o54R","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:38:28.814782Z","updated_at":"2025-10-22T14:38:28.814795Z","draft_created_at":"2025-10-22T14:38:16.829283Z","lead_time":31.201,"prediction":{"id":258,"task":1818,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766068Z","updated_at":"2025-10-13T10:55:30.766071Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"14321bb0-79ae-4627-9bb6-8b9d5397cdef","import_id":null,"last_action":null,"bulk_created":false,"task":1818,"project":9,"updated_by":1,"parent_prediction":258,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[258],"data":{"id":4854,"query":"In the year ended June 30 where Interest cost for Pension Benefits is the lowest, what's the Interest cost for Other Retiree Benefits?","dataset":"QTSumm","task_specification_analysis":"The query asks to identify the year (among years ended June 30) with the minimum Interest cost for Pension Benefits and then report the Interest cost for Other Retiree Benefits for that same year. This is a clear find-min-then-lookup operation.","scope_specification_analysis":"The ranking metric is explicit (Interest cost for Pension Benefits). The selection domain is defined (years ended June 30). The output field is explicit (Interest cost for Other Retiree Benefits). No additional methodological choices are required; in case of ties, a standard default would be to return all tied years.","domain_specification_analysis":"No organizational or geographical boundary is provided. \"Year ended June 30\" applies to many companies and governments; without naming the filer\/report, the contextual boundary is missing.","entity_specification_analysis":"The entities \"Interest cost for Pension Benefits\" and \"Interest cost for Other Retiree Benefits\" refer to line items within a specific organization’s pension\/OPEB footnote, but no company, plan, currency, or dataset is named. Many entities report such items, so the target is not uniquely identifiable.","temporal_specification_analysis":"\"In the year ended June 30 where ... is the lowest\" depends on the (unspecified) reporting entity’s sequence of fiscal years. This implicitly relies on whatever years exist in a particular dataset, which is unknown; no concrete time frame or all-time scope is given."},"meta":{},"created_at":"2025-10-13T10:55:30.645020Z","updated_at":"2025-10-22T14:38:28.864995Z","inner_id":108,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1819,"annotations":[{"id":824,"completed_by":1,"result":[{"id":"YCeMai10ZM","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"YZlIQGYmcZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"itLrT5N4GS","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"P-_gXxSKGG","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"UBSEo04nv9","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:40:04.354303Z","updated_at":"2025-10-22T14:40:04.354314Z","draft_created_at":"2025-10-22T14:39:59.935291Z","lead_time":25.383,"prediction":{"id":259,"task":1819,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766083Z","updated_at":"2025-10-13T10:55:30.766086Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"aa2a9050-70ae-4778-90bd-e8d88d18be21","import_id":null,"last_action":null,"bulk_created":false,"task":1819,"project":9,"updated_by":1,"parent_prediction":259,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[259],"data":{"id":4748,"query":"If Food Packaging for Balance at End of Period develops with the same growth rate in 2004, what will it reach in 2005?","dataset":"QTSumm","task_specification_analysis":"The user requests a one-step forecast: compute 2005 by applying the same growth rate observed in 2004. This is a clear analytical action (project next value using prior period growth).","scope_specification_analysis":"Method is implicit and standard: compute 2004 growth rate as year-over-year percentage change (2004 vs 2003), then apply it multiplicatively to the 2004 value to obtain 2005 (Value_2005 = Value_2004 * (1 + GrowthRate_2004)). No additional parameters are required.","domain_specification_analysis":"No domain boundary is given (no company, country, sector classification system, or dataset). The context could be corporate finance, inventory, or national\/industry statistics, making the domain ambiguous.","entity_specification_analysis":"Core entities like \"Food Packaging\" and the metric \"Balance at End of Period\" are not uniquely identifiable without a dataset, organization, or indicator definition. Multiple plausible datasets\/contexts exist (company inventory, national statistics, sector accounts), so the target entity cannot be resolved.","temporal_specification_analysis":"The query explicitly references 2004 (growth rate) and asks for the value in 2005, implying a projection using the 2004 growth. The time frame is clear."},"meta":{},"created_at":"2025-10-13T10:55:30.645046Z","updated_at":"2025-10-22T14:40:04.394750Z","inner_id":109,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1820,"annotations":[{"id":783,"completed_by":1,"result":[{"id":"B9vHG6EL7W","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"uoYHpVSFG4","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"akopqMvf0k","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"mvCxJs0tC0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Em_wAbud2O","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:07:43.180934Z","updated_at":"2025-10-22T14:07:43.180946Z","draft_created_at":"2025-10-22T14:07:22.166068Z","lead_time":99.881,"prediction":{"id":260,"task":1820,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766098Z","updated_at":"2025-10-13T10:55:30.766101Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"472d744a-674b-49ba-899a-20728dd16ca4","import_id":null,"last_action":null,"bulk_created":false,"task":1820,"project":9,"updated_by":1,"parent_prediction":260,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[260],"data":{"id":4693,"query":"What was the average value of Residential\/Religious(a), Commercial\/Industrial, Retail access customers in Revenues in Millions for Twelve Months Ended in 2011? (in million)","dataset":"QTSumm","task_specification_analysis":"Compute a single average (mean) value across three specified categories’ revenues for 2011.","scope_specification_analysis":"The query names the exact categories (Residential\/Religious(a), Commercial\/Industrial, Retail access customers), the metric (Revenues in Millions for Twelve Months Ended), the year (2011), and the unit (millions). The calculation is a straightforward unweighted arithmetic mean of the three values.","domain_specification_analysis":"No organizational or geographic boundary is provided (e.g., which utility\/company), so the context for the revenues is unclear and cannot be uniquely bounded.","entity_specification_analysis":"The entities ('Residential\/Religious(a)', 'Commercial\/Industrial', 'Retail access customers' revenue categories) appear to reference a specific utility’s customer classes, but no company\/utility or dataset is identified; multiple plausible sources exist.","temporal_specification_analysis":"“Twelve Months Ended in 2011” provides a clear annual time frame (i.e., the 2011 fiscal\/calendar year)."},"meta":{},"created_at":"2025-10-13T10:55:30.645073Z","updated_at":"2025-10-22T14:07:43.226882Z","inner_id":110,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1821,"annotations":[{"id":728,"completed_by":1,"result":[{"id":"sUPrOSBSPJ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"UF7bDd5MZW","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"hipLHyVWT7","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Cq4t1e1vqO","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"dHF1Q7-6_I","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:35:12.165471Z","updated_at":"2025-10-22T05:35:12.165522Z","draft_created_at":"2025-10-22T05:34:26.734545Z","lead_time":62.981,"prediction":{"id":261,"task":1821,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766113Z","updated_at":"2025-10-13T10:55:30.766115Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"d26d7b74-da60-448b-a831-4b7a659c00fd","import_id":null,"last_action":null,"bulk_created":false,"task":1821,"project":9,"updated_by":1,"parent_prediction":261,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[261],"data":{"id":5271,"query":"What is the title of the book written by George Orwell that has the lowest sale price?","dataset":"Spider","task_specification_analysis":"The user asks to identify (return) the title of the book by George Orwell with the lowest sale price—i.e., filter by author, find the minimum sale price, and report the corresponding title(s).","scope_specification_analysis":"The calculation is clear: filter to George Orwell, compute the minimum of the sale price field, and select the title(s) at that minimum. The ranking metric (sale price) and direction (lowest) are specified; reasonable default is to return all ties if any.","domain_specification_analysis":"\"Sale price\" lacks a necessary boundary: retailer\/marketplace (e.g., Amazon, bookstores), geography\/currency, condition (new\/used), and edition. Without a domain, a researcher cannot choose the relevant tables or sources.","entity_specification_analysis":"Core entity is \"books written by George Orwell\"—a well-defined, resolvable set using world knowledge; \"George Orwell\" is uniquely identifiable. Edition-level variation affects pricing but not the identification of the author or the class of works.","temporal_specification_analysis":"No time frame is provided. For price questions, a reasonable default is the most recent\/current prices, but prices are time-variant."},"meta":{},"created_at":"2025-10-13T10:55:30.645099Z","updated_at":"2025-10-22T05:35:12.257466Z","inner_id":111,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1822,"annotations":[{"id":762,"completed_by":1,"result":[{"id":"qN5e8K8RZa","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"h_vjQX9SOi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"0I6WiEd-6S","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"qDb0Hg3QcL","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"OJO8we_3dl","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:03:25.542826Z","updated_at":"2025-10-22T06:03:25.542836Z","draft_created_at":"2025-10-22T06:03:05.737501Z","lead_time":30.088,"prediction":{"id":262,"task":1822,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766127Z","updated_at":"2025-10-13T10:55:30.766130Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"611e1d28-4366-4543-9e36-f6a99afd60f3","import_id":null,"last_action":null,"bulk_created":false,"task":1822,"project":9,"updated_by":1,"parent_prediction":262,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[262],"data":{"id":5132,"query":"What are the statuses, dates, and shipment dates for all invoices?","dataset":"Spider","task_specification_analysis":"The user requests a straightforward retrieval\/listing of fields (statuses, dates, shipment dates) for all invoices. This is a clear extract\/list task.","scope_specification_analysis":"No calculations, ranking, or modeling are required—just selecting specified attributes for all records. Any ambiguity about which exact 'date' column is a data-field choice, not a procedural method issue.","domain_specification_analysis":"No organizational, geographical, or system boundary is provided. \"All invoices\" cannot sensibly default to a universal domain; a specific company\/system or dataset is required.","entity_specification_analysis":"Core entity is \"invoices,\" but the query does not specify whose or which system's invoices (company, vendor platform, dataset). Field names like \"statuses,\" \"dates,\" and \"shipment dates\" are also ambiguous (e.g., invoice date vs. due date vs. created date). Without an owning context, the target tables cannot be confidently identified.","temporal_specification_analysis":"The request is for \"all invoices,\" implying no time filter (an all-time scope over the chosen domain). No specific period is required to answer once the domain is known."},"meta":{},"created_at":"2025-10-13T10:55:30.645125Z","updated_at":"2025-10-22T06:03:25.580203Z","inner_id":112,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1823,"annotations":[{"id":798,"completed_by":1,"result":[{"id":"YOkrKf-vqM","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"6lm4PcPQc9","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"DZ1yozVx6w","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"wvxwsbmAh1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Zm12KjxppI","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:23:06.651488Z","updated_at":"2025-10-22T14:23:06.651504Z","draft_created_at":"2025-10-22T14:22:50.920229Z","lead_time":25.854,"prediction":{"id":263,"task":1823,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766142Z","updated_at":"2025-10-13T10:55:30.766145Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"6459be67-3c02-4b07-af8f-6ea0aa4ad3c3","import_id":null,"last_action":null,"bulk_created":false,"task":1823,"project":9,"updated_by":1,"parent_prediction":263,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[263],"data":{"id":5072,"query":"What are the pairs of teachers and students who are in the same class? Give me the pairs of their details.","dataset":"Spider","task_specification_analysis":"The user asks to list all teacher–student pairs who are in the same class, i.e., enumerate paired records of teachers and students that share class membership.","scope_specification_analysis":"The method is clear: join teacher-to-class and student-to-class relationships on a common class identifier and output one record per teacher–student pair (per class). No ranking or aggregation is requested. “Details” can reasonably default to all available attributes for each teacher and student.","domain_specification_analysis":"No organizational or geographical boundary is given (which school, university, district, or system). The context is necessary to locate relevant tables.","entity_specification_analysis":"Core entities ('teachers', 'students', 'classes', and 'their details') are generic and dataset-dependent; no specific institution or dataset is identified, so the subjects cannot be uniquely resolved.","temporal_specification_analysis":"Class membership is time-bound (term\/semester). The query provides no timeframe, so it’s unclear whether to use current, a specific term, or historical data."},"meta":{},"created_at":"2025-10-13T10:55:30.645152Z","updated_at":"2025-10-22T14:23:06.705242Z","inner_id":113,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1824,"annotations":[{"id":553,"completed_by":1,"result":[{"id":"I_bJHJ8QBe","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"vSaznNvozz","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"hjJRN6Dw-r","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"7y1-Vad9jS","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Zk6KRWbf9t","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-13T11:09:44.578595Z","updated_at":"2025-10-13T11:09:44.578609Z","draft_created_at":"2025-10-13T11:09:07.202874Z","lead_time":146.337,"prediction":{"id":264,"task":1824,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766157Z","updated_at":"2025-10-13T10:55:30.766160Z","created_ago":"14 minutes","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"97e388d3-a8af-4103-b777-6862f2541da0","import_id":null,"last_action":null,"bulk_created":false,"task":1824,"project":9,"updated_by":1,"parent_prediction":264,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[264],"data":{"id":4916,"query":"Find the name and age of pilots who have a plane in Austin.","dataset":"Spider","task_specification_analysis":"The query requests a straightforward retrieval: list the name and age of pilots satisfying a condition. This is a clear lookup operation.","scope_specification_analysis":"The scope involves filtering pilots by whether they have an associated plane located in Austin—implementable via a join and WHERE condition. No ranking, aggregation, or model choice is required.","domain_specification_analysis":"A clear geographic boundary is provided: Austin, which by dominant interpretation refers to Austin, Texas. No additional organizational boundary is required to make the query meaningful.","entity_specification_analysis":"Core entities are a broad but well-defined class: pilots and aircraft, filtered by location (Austin). The intent is to retrieve all pilots who are associated with an aircraft in Austin. While “have” (own\/operate\/based) is semantically ambiguous, the entities themselves are identifiable as classes, so this does not create entity ambiguity.","temporal_specification_analysis":"No time frame is given; the query is in present tense (“have”), so the reasonable default is the most recent\/currently available data (e.g., current registries\/rosters)."},"meta":{},"created_at":"2025-10-13T10:55:30.645179Z","updated_at":"2025-10-13T11:09:44.633365Z","inner_id":114,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1825,"annotations":[{"id":731,"completed_by":1,"result":[{"id":"ms-zWKHLEK","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"swg057yUv9","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"tz5muTgi6n","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"HYFxG6u07Y","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"WDxw2SnNdO","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:37:28.846901Z","updated_at":"2025-10-22T05:37:28.846929Z","draft_created_at":"2025-10-22T05:37:01.712992Z","lead_time":39.224,"prediction":{"id":265,"task":1825,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766171Z","updated_at":"2025-10-13T10:55:30.766174Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"b89e271b-8d68-43dc-9b90-fbe420f1f49c","import_id":null,"last_action":null,"bulk_created":false,"task":1825,"project":9,"updated_by":1,"parent_prediction":265,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[265],"data":{"id":5123,"query":"What is the name and age of the sailor with maximum age?","dataset":"Spider","task_specification_analysis":"The query requests identifying the sailor with the maximum age and returning their name and age—a clear retrieval task involving a max operation.","scope_specification_analysis":"Method is clear: compute max(age) and select name and age for the sailor(s) with that age. Any tie-handling is a reasonable default and does not materially change the intent.","domain_specification_analysis":"No domain boundary is provided. The query could refer to a particular dataset\/table (common in SQL exercises), a specific organization (e.g., a navy), a country, or the world. Without a boundary, relevant tables cannot be confidently identified.","entity_specification_analysis":"\"Sailor\" is a generic class with no dataset or named entity specified. \"The sailor with maximum age\" depends on which population of sailors is in scope (a specific table, club, navy, country, or the world). No unique entity can be resolved.","temporal_specification_analysis":"Age requires a reference time. It is unclear whether the intent is \"oldest currently living\" (as of when?) or \"oldest ever\" (historical). No timeframe or as-of date is given."},"meta":{},"created_at":"2025-10-13T10:55:30.645210Z","updated_at":"2025-10-22T05:37:28.958385Z","inner_id":115,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1826,"annotations":[{"id":713,"completed_by":1,"result":[{"id":"3aOcEVOQle","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"DEu7wyWll3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"DsAbVT9tD6","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"U1eNLmpp-Y","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"0d7e0awoAc","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:49:39.896796Z","updated_at":"2025-10-21T22:49:39.896809Z","draft_created_at":"2025-10-21T22:49:26.310722Z","lead_time":28.846,"prediction":{"id":266,"task":1826,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766189Z","updated_at":"2025-10-13T10:55:30.766207Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2acee04d-2f0f-46a1-a421-edd529b2d742","import_id":null,"last_action":null,"bulk_created":false,"task":1826,"project":9,"updated_by":1,"parent_prediction":266,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[266],"data":{"id":5160,"query":"What are years of founding for orchestras that have had more than a single performance?","dataset":"Spider","task_specification_analysis":"The user requests a list\/return of founding years for a subset of orchestras—those with more than one performance. This is a clear retrieval task after filtering.","scope_specification_analysis":"Scope is clear: compute (or use) the count of performances per orchestra, filter where count > 1, and select the founding_year for those orchestras. No ranking or additional aggregation needed; default interpretation is per-orchestra (no dedup) unless otherwise requested.","domain_specification_analysis":"No geographic or organizational boundary is provided; the sensible interpretation is worldwide (any orchestra).","entity_specification_analysis":"The entities are orchestras as a broad, well-defined class, with a subset defined by a clear predicate (having more than one performance). This is resolvable without needing a specific dataset reference.","temporal_specification_analysis":"The filter \"have had more than a single performance\" is naturally interpreted as an all-time cumulative condition (ever up to now). Founding years are historical attributes. Thus the temporal scope is implicitly all-time."},"meta":{},"created_at":"2025-10-13T10:55:30.645237Z","updated_at":"2025-10-21T22:49:39.952208Z","inner_id":116,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1827,"annotations":[{"id":777,"completed_by":1,"result":[{"id":"C-GxgqE7ji","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"3OwEju2jiA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"2cFaPFop1e","type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"EOLmzHRzO4","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"PsFsrtCLlv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:13:08.683102Z","updated_at":"2025-10-22T06:13:08.683111Z","draft_created_at":"2025-10-22T06:12:57.520382Z","lead_time":20.969,"prediction":{"id":267,"task":1827,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766220Z","updated_at":"2025-10-13T10:55:30.766235Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"e5dfab39-f36e-4563-ac9b-a2c5de9cf01c","import_id":null,"last_action":null,"bulk_created":false,"task":1827,"project":9,"updated_by":1,"parent_prediction":267,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[267],"data":{"id":4898,"query":"How many concerts occurred in 2014 or 2015?","dataset":"Spider","task_specification_analysis":"The user requests a simple aggregate: count the number of concerts that occurred within two specified years. This is a clear counting task.","scope_specification_analysis":"The method is clear: filter events to those with dates in 2014 or 2015 (inclusive) and count them. No additional metrics, grouping, or modeling choices are needed.","domain_specification_analysis":"No geographic or organizational boundary is provided (e.g., country, city, venue, artist, platform). A reasonable default is to interpret this as a global count across all concerts.","entity_specification_analysis":"The core entity is the broad class \"concerts,\" which is a clear, well-understood event type to count. No specific artist, venue, or organizer is referenced, but counting all concerts is a valid broad-class target.","temporal_specification_analysis":"The years 2014 and 2015 are explicitly specified, providing a clear time frame."},"meta":{},"created_at":"2025-10-13T10:55:30.645263Z","updated_at":"2025-10-22T06:13:08.720192Z","inner_id":117,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1828,"annotations":[{"id":729,"completed_by":1,"result":[{"id":"vjwBktSxHl","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"l6I8lWDjtD","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Z0Mnz0xgDJ","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"7TU43G2FTA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"cvZjsWT155","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:35:35.178170Z","updated_at":"2025-10-22T05:35:35.178181Z","draft_created_at":"2025-10-22T05:35:22.913421Z","lead_time":22.288,"prediction":{"id":268,"task":1828,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766246Z","updated_at":"2025-10-13T10:55:30.766249Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"10a12352-7a93-444a-becf-d1fa7a5ebcc3","import_id":null,"last_action":null,"bulk_created":false,"task":1828,"project":9,"updated_by":1,"parent_prediction":268,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[268],"data":{"id":5263,"query":"How many documents do we have?","dataset":"Spider","task_specification_analysis":"The user requests a simple count of items (documents). This is an explicit counting task.","scope_specification_analysis":"No additional parameters are needed; perform a total count of all documents in the identified dataset (e.g., COUNT of records). No filtering, grouping, or special metrics implied.","domain_specification_analysis":"The boundary \"we\" is entirely undefined (which organization, repository, platform, or dataset). Without a domain, a researcher cannot identify which tables to query.","entity_specification_analysis":"The core entity \"documents\" is undefined—no type (e.g., PDFs, contracts, support tickets) or source system is given, and \"we\" does not resolve to a specific organization or dataset. Multiple plausible entities exist with no path to resolution.","temporal_specification_analysis":"Present-tense \"do we have\" implies a current count; if a system had a clear domain, the latest available tally would be the natural default."},"meta":{},"created_at":"2025-10-13T10:55:30.645289Z","updated_at":"2025-10-22T05:35:35.252410Z","inner_id":118,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1829,"annotations":[{"id":848,"completed_by":1,"result":[{"id":"82jfiLThNt","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"guWrP-Vp11","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"_wGiaLlPs_","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Fd4W-yHOnb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"fhoGYOa2p0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:06:57.348045Z","updated_at":"2025-10-22T15:06:57.348054Z","draft_created_at":"2025-10-22T15:06:44.607233Z","lead_time":26.543,"prediction":{"id":269,"task":1829,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766261Z","updated_at":"2025-10-13T10:55:30.766264Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"0fd5fd44-b37b-4b07-935b-3cc5a5d2c049","import_id":null,"last_action":null,"bulk_created":false,"task":1829,"project":9,"updated_by":1,"parent_prediction":269,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[269],"data":{"id":5354,"query":"List the first name, middle name and last name, and log in name of all the seller users, whose seller value is 1.","dataset":"Spider","task_specification_analysis":"The user requests a straightforward retrieval: list specific fields (first, middle, last name, and login name) for a subset of users defined by a condition.","scope_specification_analysis":"The calculation\/transform is a simple filter where seller_value = 1 and a projection of four fields. No aggregation, ranking, or modeling required. Ordering is not requested, so default (unspecified) ordering is acceptable.","domain_specification_analysis":"The organizational\/system boundary (which company, platform, or database) is not provided. Without a domain, a researcher cannot know which user base to query.","entity_specification_analysis":"Core entities like \"seller users,\" the attribute \"seller value,\" and fields such as \"log in name\" depend on a specific database\/schema that is not identified. Multiple plausible tables and column names could exist; hence the subjects cannot be uniquely resolved in an open domain.","temporal_specification_analysis":"This is a static listing request over user records without any time dimension; no timeframe is implied or required."},"meta":{},"created_at":"2025-10-13T10:55:30.645315Z","updated_at":"2025-10-22T15:06:57.385648Z","inner_id":119,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1830,"annotations":[{"id":766,"completed_by":1,"result":[{"id":"nJk_7lyApO","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"k_sJTbamZd","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"QqYyecqOHB","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"s05cagicEZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"GKCYcI3gU_","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:05:52.962910Z","updated_at":"2025-10-22T06:05:52.962924Z","draft_created_at":"2025-10-22T06:05:17.258705Z","lead_time":50.925,"prediction":{"id":270,"task":1830,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766276Z","updated_at":"2025-10-13T10:55:30.766278Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"467d9e73-4858-4177-b934-adba93381d24","import_id":null,"last_action":null,"bulk_created":false,"task":1830,"project":9,"updated_by":1,"parent_prediction":270,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[270],"data":{"id":5333,"query":"Show the earnings of players from country \"Australia\" or \"Zimbabwe\".","dataset":"Spider","task_specification_analysis":"The user requests a retrieval: display the earnings for players who are from Australia or Zimbabwe. This is a clear filter-and-show action.","scope_specification_analysis":"Scope is clear: filter players by country in {Australia, Zimbabwe} and return the earnings field. No aggregation, ranking, or additional calculations are requested; a simple lookup suffices assuming an 'earnings' column exists.","domain_specification_analysis":"The domain boundary (sport\/league or competitive domain) is missing. \"Players\" requires a context such as ATP\/WTA, ICC cricket, FIFA soccer, PGA golf, esports, poker, etc. Without this, a researcher cannot select the relevant corpus.","entity_specification_analysis":"Core entity \"players\" is ambiguous—no sport\/league or player population is specified. Countries (Australia, Zimbabwe) are clear, but without specifying which players, multiple plausible datasets exist (tennis, cricket, soccer, esports, poker, golf, etc.). \"Earnings\" is tied to the entity type and remains undefined (salary, prize money, endorsements).","temporal_specification_analysis":"No time frame is given. \"Earnings\" could mean current year, latest season, specific tournament, or career-to-date as of a date. Without a timeframe (and without a clear sport context), the temporal scope is unclear."},"meta":{},"created_at":"2025-10-13T10:55:30.645342Z","updated_at":"2025-10-22T06:05:53.019033Z","inner_id":120,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1831,"annotations":[{"id":851,"completed_by":1,"result":[{"id":"qKlvcD4v7E","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"MnTXFCzykT","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"w8B4aP4gFq","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"qiyYCYMUjB","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Gwr4qCYY6u","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:09:16.632264Z","updated_at":"2025-10-22T15:09:16.632274Z","draft_created_at":"2025-10-22T15:09:00.931066Z","lead_time":43.13,"prediction":{"id":271,"task":1831,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766290Z","updated_at":"2025-10-13T10:55:30.766293Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"f2a738f3-6d7f-4014-9486-14a44ea8231b","import_id":null,"last_action":null,"bulk_created":false,"task":1831,"project":9,"updated_by":1,"parent_prediction":271,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[271],"data":{"id":5820,"query":"In the first episode of a reality show, contestants had to spin two wheels of fate. Spinning the first wheel determined the remote location where contestants would reside for the duration of the season. Spinning the second wheel determined which \"bonus survival tool\" they would be allowed to bring, along with a few other necessary items. What is the probability that a randomly selected participant spun the first wheel and landed on desert and spun the second wheel and landed on a tent? Simplify any fractions.","dataset":"TabMWP","task_specification_analysis":"Compute the probability of a joint event: a participant landed on desert (first wheel) and tent (second wheel). This is a clear, executable operation.","scope_specification_analysis":"Use empirical joint probability: count of participants with wheel1 = desert AND wheel2 = tent divided by total participants; express as a simplified fraction. No additional parameters (e.g., independence assumptions) are required.","domain_specification_analysis":"No organizational\/geographical boundary is given (which show\/network\/season). The contextual domain needed to find the relevant tables is missing.","entity_specification_analysis":"Core entities (the specific reality show\/season, the wheel outcome sets and their probabilities or counts) are not identified. Without knowing which show or how many options each wheel has, multiple plausible datasets exist and no unique data can be targeted.","temporal_specification_analysis":"Reference to “the first episode” is relative and unresolvable without the specific show\/season; the probability could differ by episode\/season."},"meta":{},"created_at":"2025-10-13T10:55:30.645372Z","updated_at":"2025-10-22T15:09:16.670679Z","inner_id":121,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1832,"annotations":[{"id":797,"completed_by":1,"result":[{"id":"b_8AR7TzI0","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"v7-ocCc6h2","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"2QzhD62Tgy","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"9MZJ8tuXUC","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"jpbBdYtVoA","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:22:39.685465Z","updated_at":"2025-10-22T14:22:39.685486Z","draft_created_at":"2025-10-22T14:22:26.559286Z","lead_time":94.398,"prediction":{"id":272,"task":1832,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766305Z","updated_at":"2025-10-13T10:55:30.766307Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"65a11226-441a-4095-bce7-4bbf91bc5785","import_id":null,"last_action":null,"bulk_created":false,"task":1832,"project":9,"updated_by":1,"parent_prediction":272,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[272],"data":{"id":5823,"query":"A personal trainer recorded how many laps each of her clients ran last week. How many clients ran fewer than 2 laps?","dataset":"TabMWP","task_specification_analysis":"The query asks for a count of clients meeting a specific condition (laps run fewer than 2). This is a clear counting task.","scope_specification_analysis":"The calculation method is explicit: filter records where laps < 2 (strictly fewer than 2) and count those clients. No additional metrics, grouping, or ambiguous parameters are required.","domain_specification_analysis":"The domain boundary is the private context of a particular trainer’s client set. Without identifying the trainer or organization, the domain is ambiguous and cannot be located in an open corpus.","entity_specification_analysis":"The entities are \"a personal trainer\" and \"her clients,\" which are not uniquely identifiable. There is no way to know which trainer or client set is meant, nor which dataset contains these records.","temporal_specification_analysis":"The query specifies the time frame as \"last week,\" which is a clear, resolvable relative period even though it depends on a reference date."},"meta":{},"created_at":"2025-10-13T10:55:30.645399Z","updated_at":"2025-10-22T14:22:39.729415Z","inner_id":122,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1833,"annotations":[{"id":826,"completed_by":1,"result":[{"id":"LxATom3n4c","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"NYVOokY7MP","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"CViNKbi9Og","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"jhwJUOZeVw","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"btK2eIOhSN","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:41:01.467052Z","updated_at":"2025-10-22T14:41:01.467070Z","draft_created_at":"2025-10-22T14:40:51.297377Z","lead_time":30.808,"prediction":{"id":273,"task":1833,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766319Z","updated_at":"2025-10-13T10:55:30.766322Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2a030037-8207-416a-8431-f6e5c7660ca2","import_id":null,"last_action":null,"bulk_created":false,"task":1833,"project":9,"updated_by":1,"parent_prediction":273,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[273],"data":{"id":5421,"query":"A car dealership tracked the number of cars sold each month. In which month did the dealership sell the most cars?","dataset":"TabMWP","task_specification_analysis":"The query asks to identify the month with the highest number of cars sold—i.e., find the argmax month based on monthly sales counts.","scope_specification_analysis":"Method is clear: compare monthly sales counts and select the month(s) with the maximum value. Ranking metric is explicitly the number of cars sold. Any tie can be handled by returning all tied months, a reasonable default.","domain_specification_analysis":"The contextual boundary is an internal dataset for an unspecified dealership. Without the dealership name or location, there is no clear organizational\/geographical domain to target.","entity_specification_analysis":"The core entity is \"a car dealership\"\/\"the dealership,\" which is not identified by name or any unique descriptor, making it impossible to resolve to a specific entity or dataset in an open-domain setting.","temporal_specification_analysis":"\"Which month\" is asked without a defined time frame (which year or range, or whether it’s all-time within the dataset). This relies on an unspecified dataset scope, so the temporal context is ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.645431Z","updated_at":"2025-10-22T14:41:01.535676Z","inner_id":123,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1834,"annotations":[{"id":839,"completed_by":1,"result":[{"id":"1Fq27zvmj4","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"-tXAE5BTT-","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"7Iu7wTWWsb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"8OerMYx1ps","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"SozjUTbMKt","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:58:23.250205Z","updated_at":"2025-10-22T14:58:23.250218Z","draft_created_at":"2025-10-22T14:58:13.349238Z","lead_time":20.717,"prediction":{"id":274,"task":1834,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766334Z","updated_at":"2025-10-13T10:55:30.766336Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"2dd6f5df-49e9-4d25-978d-cc9552414316","import_id":null,"last_action":null,"bulk_created":false,"task":1834,"project":9,"updated_by":1,"parent_prediction":274,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[274],"data":{"id":5747,"query":"A juice shop tracked how many smoothies it sold in the past 4 days. On which day did the shop sell the most smoothies?","dataset":"TabMWP","task_specification_analysis":"Clear goal: identify the day with the highest number of smoothies sold (argmax).","scope_specification_analysis":"Metric is daily smoothie count over the 4 days; operation is to find the maximum and report the corresponding day(s). Tie-handling can default to reporting all max days.","domain_specification_analysis":"The domain is implicitly the internal sales records of a particular juice shop, but the shop\/organization and location are not given, so the boundary is missing.","entity_specification_analysis":"Core entity is \"a juice shop\" with no identifying details; the days are unspecified within that shop's records. No way to locate a specific dataset in an open domain.","temporal_specification_analysis":"\"Past 4 days\" is a relative timeframe with no anchor date; also the specific day labels (e.g., dates or weekday names) are unknown."},"meta":{},"created_at":"2025-10-13T10:55:30.645458Z","updated_at":"2025-10-22T14:58:23.295302Z","inner_id":124,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1835,"annotations":[{"id":819,"completed_by":1,"result":[{"id":"kl1LzDV7Xz","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"2nQEp60L1S","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"c9ye_SrZV3","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"b71T-S9v8-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"wDlcePorG5","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:36:22.248243Z","updated_at":"2025-10-22T14:36:22.248258Z","draft_created_at":"2025-10-22T14:36:06.412191Z","lead_time":31.647,"prediction":{"id":275,"task":1835,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766348Z","updated_at":"2025-10-13T10:55:30.766351Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"645f5d7d-0b14-4b88-9437-b1450c814a7f","import_id":null,"last_action":null,"bulk_created":false,"task":1835,"project":9,"updated_by":1,"parent_prediction":275,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[275],"data":{"id":5473,"query":"How much money does Tyrone need to buy 8 tickets for a South American cruise?","dataset":"TabMWP","task_specification_analysis":"The query requests a single numeric outcome: the total amount of money required to purchase 8 tickets. This is a clear, executable operation (compute the total cost for a quantity).","scope_specification_analysis":"The calculation is straightforward: use the per-ticket price for the specified South American cruise and multiply by 8 (or equivalently sum 8 ticket prices if itemized). No ranking, grouping, or modeling is required. Any variation in fare type, taxes\/fees inclusion, or discounts pertains to data selection rather than the analytical method; the default method is total = 8 × (applicable per-ticket price).","domain_specification_analysis":"No organizational or market boundary (e.g., which cruise line\/agency) is provided; scope could be any cruise globally, and currency is unspecified, making the domain ambiguous.","entity_specification_analysis":"The key entity is the cruise product; \"South American cruise\" covers many itineraries, cruise lines, ports, cabin classes, and fare types. \"Tyrone\" does not help to disambiguate. No specific product or currency is identified, so a price cannot be uniquely determined.","temporal_specification_analysis":"Cruise prices vary by sailing date\/season and booking date; no travel date or \"as of\" time is given, so there is no defensible default timeframe."},"meta":{},"created_at":"2025-10-13T10:55:30.645484Z","updated_at":"2025-10-22T14:36:22.305411Z","inner_id":125,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1836,"annotations":[{"id":732,"completed_by":1,"result":[{"id":"N_KuH4xFe7","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"VnFGHCWc2B","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"jgUqJ0ynwu","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"IRkzW5cuPK","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"0YTIfqAtw1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:38:34.881948Z","updated_at":"2025-10-22T05:38:34.881959Z","draft_created_at":"2025-10-22T05:38:05.041556Z","lead_time":65.127,"prediction":{"id":276,"task":1836,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766363Z","updated_at":"2025-10-13T10:55:30.766365Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"eab5f6a4-d3da-4655-b8dd-c9f47e3ead58","import_id":null,"last_action":null,"bulk_created":false,"task":1836,"project":9,"updated_by":1,"parent_prediction":276,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[276],"data":{"id":5533,"query":"The employees at Estelle's Construction monitored the number of cracked window panes in each shipment they received. How many shipments had at least 30 cracked window panes but less than 80 cracked window panes?","dataset":"TabMWP","task_specification_analysis":"The query asks to count the number of shipments that meet a specified condition. This is a clear, executable operation: compute a count.","scope_specification_analysis":"Filter shipments where the number of cracked window panes is >= 30 and < 80, then count those shipments. The bounds are explicitly defined (inclusive lower, exclusive upper). No additional metrics, grouping, or methods are needed.","domain_specification_analysis":"The intended boundary is organizational—shipments received by Estelle’s Construction—so the contextual scope is clear if the entity were known.","entity_specification_analysis":"References a specific company (“Estelle’s Construction”) and its shipments, but this entity is not uniquely identifiable in an open domain and the particular dataset of shipments is not provided. A researcher could not locate the exact table without further details.","temporal_specification_analysis":"No time frame is given for the shipments; the count would depend on which period of shipments is considered. There is no safe default period to assume."},"meta":{},"created_at":"2025-10-13T10:55:30.645510Z","updated_at":"2025-10-22T05:38:34.920640Z","inner_id":126,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1837,"annotations":[{"id":803,"completed_by":1,"result":[],"was_cancelled":true,"ground_truth":false,"created_at":"2025-10-22T14:26:41.138943Z","updated_at":"2025-10-22T14:26:41.138956Z","draft_created_at":null,"lead_time":101.512,"prediction":{"id":277,"task":1837,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766377Z","updated_at":"2025-10-13T10:55:30.766380Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":0,"unique_id":"1d33fbdc-7b09-4c9b-804c-2074de9fdd6b","import_id":null,"last_action":null,"bulk_created":false,"task":1837,"project":9,"updated_by":1,"parent_prediction":277,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[277],"data":{"id":5826,"query":"Each tin has 5 muffins. How many muffins are in 4 tins?","dataset":"TabMWP","task_specification_analysis":"The query asks to compute the total number of muffins given a per-tin count and a number of tins—i.e., perform a straightforward calculation to find the total.","scope_specification_analysis":"The method is clear: multiply 5 muffins per tin by 4 tins. No additional parameters or choices are needed.","domain_specification_analysis":"No geographical or organizational boundary is needed; the scenario is self-contained and purely mathematical.","entity_specification_analysis":"Core entities are abstract classes ('tins' and 'muffins') with explicit quantities given in the problem; no external identification needed.","temporal_specification_analysis":"This is a timeless arithmetic word problem; no timeframe is relevant or implied."},"meta":{},"created_at":"2025-10-13T10:55:30.645537Z","updated_at":"2025-10-22T14:26:41.187355Z","inner_id":127,"total_annotations":0,"cancelled_annotations":1,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1838,"annotations":[{"id":810,"completed_by":1,"result":[{"id":"J8MCbkQUA8","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"NF9x9DvkQZ","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"xyupb0n8La","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"um28YeIwje","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Ha4W9Ib0Iy","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:30:17.577634Z","updated_at":"2025-10-22T14:30:17.577643Z","draft_created_at":"2025-10-22T14:30:03.240807Z","lead_time":26.335,"prediction":{"id":278,"task":1838,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766392Z","updated_at":"2025-10-13T10:55:30.766395Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"e663e4b8-0a7c-4922-847a-268a7e5f6693","import_id":null,"last_action":null,"bulk_created":false,"task":1838,"project":9,"updated_by":1,"parent_prediction":278,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[278],"data":{"id":5723,"query":"Nicholas asked 80 students if they prefer to watch the Summer Olympics or the Winter Olympics. This table shows the relative frequencies from the survey. How many more boys than girls prefer to watch the Winter Olympics?","dataset":"TabMWP","task_specification_analysis":"The query asks for a single numeric result: the difference in counts between boys and girls who prefer the Winter Olympics. This is a clear, executable operation (compute a count difference).","scope_specification_analysis":"Method is clear: obtain the number (or compute from relative frequencies) of boys preferring Winter and girls preferring Winter, convert relative frequencies to counts using the total of 80 if needed, then subtract (boys minus girls). No ambiguous choice of metric or model is required.","domain_specification_analysis":"The contextual boundary is the specific (unnamed) survey\/table referenced as “this table.” Without the table or an identifiable dataset\/source, the domain is unclear and cannot be resolved in an open-domain setting.","entity_specification_analysis":"The core data entity is “this table” of relative frequencies from Nicholas’s survey, which is not provided or uniquely identifiable. The problem depends on specific counts by gender and preference that are missing, and “Nicholas” is not a resolvable public entity.","temporal_specification_analysis":"This is a self-contained word-problem about a particular survey; the answer does not depend on a time frame. No temporal filtering is relevant."},"meta":{},"created_at":"2025-10-13T10:55:30.645564Z","updated_at":"2025-10-22T14:30:17.615692Z","inner_id":128,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1839,"annotations":[{"id":738,"completed_by":1,"result":[{"id":"nByYj68Mpp","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"2OVs2XEU8G","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"rkICaFDspa","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"qOS1SeH571","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"FDi3-vCjpX","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:43:15.980933Z","updated_at":"2025-10-22T05:43:15.980944Z","draft_created_at":"2025-10-22T05:43:01.529071Z","lead_time":43.684,"prediction":{"id":279,"task":1839,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766407Z","updated_at":"2025-10-13T10:55:30.766410Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"47658581-010f-458c-bdc4-abdbdc8b7c37","import_id":null,"last_action":null,"bulk_created":false,"task":1839,"project":9,"updated_by":1,"parent_prediction":279,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[{"id":330,"user":"daniel.gomm@cwi.nl","created_username":"daniel.gomm@cwi.nl, 1","created_ago":"9 hours, 26 minutes","result":[{"id":"nByYj68Mpp","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"}],"lead_time":18.668,"was_postponed":false,"import_id":null,"created_at":"2025-10-22T05:43:01.524077Z","updated_at":"2025-10-22T05:43:01.524099Z","task":1839,"annotation":null}],"predictions":[279],"data":{"id":5780,"query":"Look at the following schedule. Which stop does the train depart from at 2.45 A.M.?","dataset":"TabMWP","task_specification_analysis":"The query requests a specific lookup: identify the stop corresponding to a given departure time (2:45 A.M.) in the schedule.","scope_specification_analysis":"This is a straightforward retrieval using the departure time column; match 2:45 A.M. (02:45) and return the associated stop. No additional metrics, ranking, or calculations are required.","domain_specification_analysis":"The domain is tied to a specific, unstated data container (“the following schedule”), making the contextual boundary (which system\/railway\/route) unclear.","entity_specification_analysis":"The query depends on an unspecified table (“the following schedule”) and an unidentified train. Without knowing which schedule\/train, the entity cannot be resolved in an open-domain setting.","temporal_specification_analysis":"A precise time filter is given (“2.45 A.M.”), which is sufficient for a timetable lookup."},"meta":{},"created_at":"2025-10-13T10:55:30.645590Z","updated_at":"2025-10-22T05:43:16.021292Z","inner_id":129,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1840,"annotations":[{"id":825,"completed_by":1,"result":[{"id":"gu07ant8-9","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"WZcAGiB6bh","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"mSV-mz1eSd","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"yz_2-8OiVF","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"a5je6eZDym","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:40:30.083383Z","updated_at":"2025-10-22T14:40:30.083393Z","draft_created_at":"2025-10-22T14:40:17.675678Z","lead_time":24.851,"prediction":{"id":280,"task":1840,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766422Z","updated_at":"2025-10-13T10:55:30.766425Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"07dffd54-f8d3-4302-ba87-e9a44c6a123f","import_id":null,"last_action":null,"bulk_created":false,"task":1840,"project":9,"updated_by":1,"parent_prediction":280,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[280],"data":{"id":5511,"query":"Kendra spent a week at the beach and recorded the number of shells she found each day. According to the table, what was the rate of change between Tuesday and Wednesday?","dataset":"TabMWP","task_specification_analysis":"The query asks to compute the rate of change between two specific days, which is a clear and executable calculation objective.","scope_specification_analysis":"Rate of change here follows the standard slope formula between consecutive days: (Wednesday count − Tuesday count) \/ 1 day, yielding shells per day. No additional parameters are needed.","domain_specification_analysis":"The query relies on \"the table\" without any contextual boundary (dataset, source, organization). This makes the domain undefined and ambiguous.","entity_specification_analysis":"References to \"Kendra,\" \"the table,\" and daily shell counts are not uniquely identifiable. The specific table is unknown, so the core entity cannot be resolved in an open-domain setting.","temporal_specification_analysis":"The time frame is given only relative to the (unknown) dataset—\"between Tuesday and Wednesday\"—which depends on the unspecified table. Per guidance, container-based temporal reference is ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.645617Z","updated_at":"2025-10-22T14:40:30.121514Z","inner_id":130,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1841,"annotations":[{"id":800,"completed_by":1,"result":[{"id":"hHT4hq4OIB","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"1-pvLAgqQr","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"fueaYLMFTX","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"U4VuDp8bDV","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"GTK2BbD4Hu","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:24:08.670747Z","updated_at":"2025-10-22T14:24:08.670760Z","draft_created_at":"2025-10-22T14:23:56.530215Z","lead_time":22.859,"prediction":{"id":281,"task":1841,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766437Z","updated_at":"2025-10-13T10:55:30.766439Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"74873131-fb01-4c6f-aa4e-59ea3ccf253b","import_id":null,"last_action":null,"bulk_created":false,"task":1841,"project":9,"updated_by":1,"parent_prediction":281,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[281],"data":{"id":5916,"query":"What is the correlation between the 'size (cents)' and 'error' columns in the table? Provide the correlation coefficient as evidence.","dataset":"TableBench","task_specification_analysis":"Compute the correlation between the two specified columns and report the coefficient as evidence. The action is clear: calculate and output a correlation coefficient.","scope_specification_analysis":"Correlation type is not named, but a standard default (Pearson product-moment) can be assumed. Typical handling would use numeric values from 'size (cents)' and 'error', exclude rows with missing\/non-numeric values, and compute a single Pearson r. No further parameters (e.g., p-value, CI) are requested.","domain_specification_analysis":"The contextual boundary is \"in the table,\" which is an unknown data container in an open-domain setting; cannot locate the relevant dataset.","entity_specification_analysis":"References two column names but does not identify which table\/dataset they belong to; \"the table\" is undefined, so the core entities cannot be resolved.","temporal_specification_analysis":"Computing a correlation within a table is not time-bound; no temporal frame is required."},"meta":{},"created_at":"2025-10-13T10:55:30.645644Z","updated_at":"2025-10-22T14:24:08.726845Z","inner_id":131,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1842,"annotations":[{"id":724,"completed_by":1,"result":[{"id":"foDBHZJ-1P","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"KTCOVhkXpu","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"2k47eBY5TY","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Z6wKcmtaWZ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"_jTpflN-Vu","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T22:59:35.937366Z","updated_at":"2025-10-21T22:59:35.937382Z","draft_created_at":"2025-10-21T22:59:19.869388Z","lead_time":32.697,"prediction":{"id":282,"task":1842,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766451Z","updated_at":"2025-10-13T10:55:30.766454Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"38230189-8b1b-4d99-9489-12fb4f85cde4","import_id":null,"last_action":null,"bulk_created":false,"task":1842,"project":9,"updated_by":1,"parent_prediction":282,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[282],"data":{"id":6058,"query":"How many countries have at least one semifinalist?","dataset":"TableBench","task_specification_analysis":"The user requests a count of entities (countries) satisfying a clear condition (having ≥1 semifinalist). This is a straightforward aggregation task.","scope_specification_analysis":"Method is clear: filter to semifinalists, map to their countries, count distinct countries. The threshold ‘at least one’ removes ambiguity.","domain_specification_analysis":"The contextual boundary (which competition\/league\/contest) is missing. Many domains have semifinals (e.g., World Cup, Wimbledon, Eurovision), leading to multiple plausible datasets.","entity_specification_analysis":"Core entities are ambiguous. \"Semifinalist\" implies a specific competition\/event (sports tournament, contest, award, etc.) that is not named; without the event, the dataset cannot be identified. \"Countries\" as a class is fine, but the set of semifinalists to count against is undefined.","temporal_specification_analysis":"No time frame is given. The intent could be a particular edition\/season\/year or all-time; there is no safe default to assume recency for an event-specific question."},"meta":{},"created_at":"2025-10-13T10:55:30.645670Z","updated_at":"2025-10-21T22:59:35.994185Z","inner_id":132,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1843,"annotations":[{"id":758,"completed_by":1,"result":[{"id":"LbWClFIhWn","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"I7SeWyMwmq","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"bZiGdgM7GQ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Si6O7NPQS1","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"LHELlHEcs7","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:00:43.482429Z","updated_at":"2025-10-22T06:00:43.482443Z","draft_created_at":"2025-10-22T06:00:36.278385Z","lead_time":20.993,"prediction":{"id":283,"task":1843,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766466Z","updated_at":"2025-10-13T10:55:30.766469Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"227eaa14-ea4b-4af0-947d-250bd1869e40","import_id":null,"last_action":null,"bulk_created":false,"task":1843,"project":9,"updated_by":1,"parent_prediction":283,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[283],"data":{"id":5970,"query":"How many regions in Italy have a population of more than 4 million?","dataset":"TableBench","task_specification_analysis":"The user requests a count of items (regions) meeting a condition, which is a clear, executable operation.","scope_specification_analysis":"The filtering criterion is explicit (population > 4,000,000), and the aggregation is a simple count of regions after filtering. No additional parameters or methods are needed.","domain_specification_analysis":"Geographical domain explicitly limited to Italy; boundary is clear and unique.","entity_specification_analysis":"\"Regions in Italy\" refers to the 20 first-level administrative regions (regioni). The population metric and threshold (>4 million) are clear. No ambiguous entities.","temporal_specification_analysis":"No time frame provided; population is time-varying. Present-tense phrasing implies the latest available population figures."},"meta":{},"created_at":"2025-10-13T10:55:30.645697Z","updated_at":"2025-10-22T06:00:43.535766Z","inner_id":133,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1844,"annotations":[{"id":771,"completed_by":1,"result":[{"id":"na8VJJKkW6","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"XYUZAA5rGp","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"uQwNq4FQQF","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"6uX23WS7WC","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"AY63kOSzQS","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:09:15.449732Z","updated_at":"2025-10-22T06:09:15.449747Z","draft_created_at":"2025-10-22T06:08:47.743248Z","lead_time":54.244,"prediction":{"id":284,"task":1844,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766481Z","updated_at":"2025-10-13T10:55:30.766484Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"32f5308b-5e7b-4c12-a726-e1b5370b8288","import_id":null,"last_action":null,"bulk_created":false,"task":1844,"project":9,"updated_by":1,"parent_prediction":284,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[284],"data":{"id":6265,"query":"What is the difference in Depth (km) between the earthquake with the highest Magnitude and the earthquake with the lowest Magnitude?","dataset":"TableBench","task_specification_analysis":"Compute a single numeric value: the difference in Depth (km) between the earthquakes with the maximum and minimum Magnitude.","scope_specification_analysis":"Procedure is clear: identify the records with highest and lowest Magnitude, extract their Depth (km), and compute the (typically absolute) difference. Ranking metric (Magnitude) and comparison attribute (Depth in km) are specified; reasonable default is absolute difference.","domain_specification_analysis":"No contextual boundary is given (e.g., which catalog such as USGS, what region, or which table). The column-style phrasing suggests a particular dataset, but it is not identified, making the domain ambiguous in an open-domain setting.","entity_specification_analysis":"The earthquakes referenced (highest and lowest magnitude) are only defined relative to some specific catalog\/table containing columns like “Depth (km)” and “Magnitude.” Without a specified dataset, region, or catalog, multiple plausible candidates exist—especially for the “lowest magnitude,” which is catalog-dependent—so the entities cannot be uniquely identified.","temporal_specification_analysis":"The query asks for extremes (highest\/lowest) which implies an all-time scope within whatever dataset\/domain is intended; no explicit time filter is needed beyond that."},"meta":{},"created_at":"2025-10-13T10:55:30.645724Z","updated_at":"2025-10-22T06:09:15.505130Z","inner_id":134,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1845,"annotations":[{"id":785,"completed_by":1,"result":[{"id":"43T2RTkAth","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"mSak76sS-Q","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"9iEyzIthj3","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Ih2qcwaX7y","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"IUKpIYGxKF","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:09:10.884801Z","updated_at":"2025-10-22T14:09:10.884810Z","draft_created_at":"2025-10-22T14:08:56.932426Z","lead_time":27.736,"prediction":{"id":285,"task":1845,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766496Z","updated_at":"2025-10-13T10:55:30.766498Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"15faeb44-b2e3-4074-a8f0-f02ef47f52f3","import_id":null,"last_action":null,"bulk_created":false,"task":1845,"project":9,"updated_by":1,"parent_prediction":285,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[285],"data":{"id":6210,"query":"Can you identify any songs within the table whose values significantly deviate from the patterns observed in other songs?","dataset":"TableBench","task_specification_analysis":"The user is asking to identify outliers among songs in a table—i.e., detect items that deviate from others. This is a clear analytical action (outlier detection).","scope_specification_analysis":"Key methodological details are missing: which columns\/metrics constitute the “values,” whether to evaluate deviation univariately or multivariately, how to scale\/normalize features, the outlier criterion (e.g., z-score threshold, IQR rule, Mahalanobis distance, isolation forest), the significance\/threshold, and whether to detect high-only, low-only, or two-sided deviations. These choices materially affect results.","domain_specification_analysis":"The domain boundary is given only as \"in the table,\" but the table\/dataset is unknown in an open-domain setting. Without identifying the dataset, no relevant tables can be located.","entity_specification_analysis":"The core entity is \"songs within the table,\" but the specific table is not identified. Additionally, the attributes\/metrics (\"values\" and the \"patterns\" to compare against) are undefined, so the set of songs and features cannot be resolved.","temporal_specification_analysis":"No timeframe is relevant or implied; the task is an intra-table outlier detection independent of time."},"meta":{},"created_at":"2025-10-13T10:55:30.645750Z","updated_at":"2025-10-22T14:09:10.923925Z","inner_id":135,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1846,"annotations":[{"id":775,"completed_by":1,"result":[{"id":"pmybj-Jbmq","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"II3Yy-CVib","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Rr3SMz4_xT","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"Lw_5TVhsbW","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"-pzQjRD_9h","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:12:12.278545Z","updated_at":"2025-10-22T06:12:12.278556Z","draft_created_at":"2025-10-22T06:11:55.956110Z","lead_time":32.253,"prediction":{"id":286,"task":1846,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766510Z","updated_at":"2025-10-13T10:55:30.766513Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"564dcded-02ec-4483-acce-9e30c40c7c76","import_id":null,"last_action":null,"bulk_created":false,"task":1846,"project":9,"updated_by":1,"parent_prediction":286,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[286],"data":{"id":6108,"query":"What is the impact of a company's sales on its market value among the companies listed in the same industry?","dataset":"TableBench","task_specification_analysis":"The intent is to quantify the effect\/relationship of company sales on market value within an industry—an explicit analytical goal (estimate impact\/relationship).","scope_specification_analysis":"Key methodological choices are missing: what measure of 'impact' (correlation vs regression effect vs causal estimate), functional form (levels vs log-log\/elasticity), whether to analyze cross-section or panel and over what period, how to handle industry structure (separate per industry vs pooled with industry fixed effects), controls\/covariates, treatment of outliers\/scale, and whether to target association or causal inference.","domain_specification_analysis":"The contextual boundary is unclear: geography\/exchanges (global vs. specific country\/market) are not specified, and \"same industry\" lacks a defined classification system. Currency and reporting standards would also need a domain boundary. Without this, relevant tables cannot be reliably targeted.","entity_specification_analysis":"Core subjects are \"companies listed in the same industry,\" but no specific industry or industry taxonomy (e.g., GICS\/NAICS\/SIC) is provided. The query does not identify a particular set of companies, and the measures (sales, market value) are not tied to concrete definitions. A researcher could not isolate the correct entities without further specification.","temporal_specification_analysis":"No time frame is given. Sales are period-specific (annual, quarterly, TTM) and market value is date-specific; the relationship depends on alignment (e.g., FY2023 revenue vs. market cap on a given date). This cannot be safely defaulted to recency because the intent (impact analysis) could be cross-sectional or panel over time."},"meta":{},"created_at":"2025-10-13T10:55:30.645780Z","updated_at":"2025-10-22T06:12:12.319557Z","inner_id":136,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1847,"annotations":[{"id":850,"completed_by":1,"result":[{"id":"yB7Zx8xH65","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"tiudSVC5JJ","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"gnBwTEq6jJ","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"j7XXckdyyn","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"n2r3lXP7A3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:08:32.759296Z","updated_at":"2025-10-22T15:08:32.759310Z","draft_created_at":"2025-10-22T15:08:15.977362Z","lead_time":26.194,"prediction":{"id":287,"task":1847,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766525Z","updated_at":"2025-10-13T10:55:30.766528Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"daf40f9c-57fe-4b6f-beac-acb86603bd79","import_id":null,"last_action":null,"bulk_created":false,"task":1847,"project":9,"updated_by":1,"parent_prediction":287,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[287],"data":{"id":6097,"query":"Can you rank the models from highest to lowest power output (hp)?","dataset":"TableBench","task_specification_analysis":"The user requests a ranking operation: order models based on a specific metric (power output). This is a clear analytical action.","scope_specification_analysis":"The ranking metric (power output in hp) and sort direction (highest to lowest) are explicitly stated. Any minor tie-handling can follow standard defaults.","domain_specification_analysis":"No domain boundary is stated (e.g., which manufacturer, vehicle category, market\/region, or dataset). The ranking set cannot be delimited.","entity_specification_analysis":"\"models\" is undefined—no product type, brand, or specific model names are provided. Multiple plausible interpretations (car models, motorcycle models, generator models, etc.) make the core entities unresolvable.","temporal_specification_analysis":"Power output (hp) often varies by model year\/trim; no time frame or model-year context is given, and no safe default applies."},"meta":{},"created_at":"2025-10-13T10:55:30.645807Z","updated_at":"2025-10-22T15:08:32.815080Z","inner_id":137,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1848,"annotations":[{"id":790,"completed_by":1,"result":[{"id":"djk8Dj9fBq","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Pb_tF1WKSo","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"wLOp72Cvlf","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"In1eBXFLiV","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"6CsmcEwA0U","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:13:49.754273Z","updated_at":"2025-10-22T14:13:49.754283Z","draft_created_at":"2025-10-22T14:13:07.203962Z","lead_time":60.002,"prediction":{"id":288,"task":1848,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766540Z","updated_at":"2025-10-13T10:55:30.766542Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"3d37814b-2e8d-4805-afef-4610e2563a6f","import_id":null,"last_action":null,"bulk_created":false,"task":1848,"project":9,"updated_by":1,"parent_prediction":288,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[288],"data":{"id":6107,"query":"In which year did the value in `bötzow` increase the most compared to the previous year?","dataset":"TableBench","task_specification_analysis":"Identify the year(s) in which the value for `bötzow` experienced the largest year-over-year increase. This implies computing changes relative to the immediately preceding year and selecting the year corresponding to the maximal increase.","scope_specification_analysis":"Compute Δ = value(year) − value(year−1) for all years with a prior year; find the maximum Δ; report the year(s) associated with that maximum increase. If multiple years tie, return all. The first year (without a prior year) is excluded from comparison.","domain_specification_analysis":"No domain boundary is given (e.g., population, economy, weather, sales), and the backticked label implies reliance on an unknown table\/context. Thus, the contextual boundary needed to locate relevant data is missing.","entity_specification_analysis":"The query references a generic 'value' and a backticked label 'bötzow' that appears dataset-specific. 'Bötzow' itself could refer to multiple real-world entities (e.g., a locality in Germany, a brewery\/brand) or a column\/row label in an unknown table. The target metric ('value') is undefined. Therefore, the core entities cannot be uniquely identified.","temporal_specification_analysis":"The question asks for the year with the largest year-over-year increase but provides no time frame, and the relevant years would be defined by an unspecified dataset. Without a specified period or dataset, the temporal scope is ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.645833Z","updated_at":"2025-10-22T14:13:49.807341Z","inner_id":138,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1849,"annotations":[{"id":829,"completed_by":1,"result":[{"id":"A6LTvU0yMv","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"lC9ZKH6Ylz","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"xFDsw1BYSN","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"LqFY2LJ96G","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"sACCnM6rOZ","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:53:03.128090Z","updated_at":"2025-10-22T14:53:03.128099Z","draft_created_at":"2025-10-22T14:52:46.657832Z","lead_time":27.987,"prediction":{"id":289,"task":1849,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766554Z","updated_at":"2025-10-13T10:55:30.766557Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"1557feb8-66f8-4051-9b40-cb480e40b9ba","import_id":null,"last_action":null,"bulk_created":false,"task":1849,"project":9,"updated_by":1,"parent_prediction":289,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[289],"data":{"id":6078,"query":"Which factors in the table, such as 'season', 'season premiere', 'season finale', 'tv season', or 'rank', significantly influence the 'viewers (in millions)' for each TV season? If none have an effect, please reply 'no clear impact'.","dataset":"TableBench","task_specification_analysis":"The user wants to identify which listed factors have a statistically significant impact on the continuous outcome 'viewers (in millions)'. This is a clear analytical action: assess significance of predictors for an outcome and report which factors matter or that none do.","scope_specification_analysis":"Key methodological details are missing and materially affect results: (1) the statistical method to assess influence\/significance (e.g., multiple linear regression vs. univariate correlations\/ANOVA); (2) whether to test jointly controlling for other variables or separately; (3) the significance threshold and any multiple-comparison correction; (4) handling\/encoding of categorical variables (e.g., 'tv season'), and treatment of potentially non-numeric fields like 'season premiere'\/'season finale'; (5) whether analysis is across all seasons or per-season stratified ('for each TV season' is ambiguous). These omissions prevent an unambiguous, reproducible procedure.","domain_specification_analysis":"The contextual boundary (which TV show, network, or dataset) is missing. \"In the table\" does not define a unique domain, leading to ambiguity about which data to analyze.","entity_specification_analysis":"The query depends on an unspecified \"table\" with generic column names; no specific TV show\/series or dataset is identified. Many tables could match, so the core entity is not uniquely resolvable.","temporal_specification_analysis":"No time frame is given; the seasons\/time scope would be defined by the unspecified table. Per guidance, referencing the data container for timing makes this ambiguous."},"meta":{},"created_at":"2025-10-13T10:55:30.645859Z","updated_at":"2025-10-22T14:53:03.167192Z","inner_id":139,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1850,"annotations":[{"id":737,"completed_by":1,"result":[{"id":"E1UfEaAdHi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"rqIFJ54FSS","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"fiMIk_yl_W","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"fGLPU3NEIO","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"KVLjdb2Ju8","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:42:31.468651Z","updated_at":"2025-10-22T05:42:31.468660Z","draft_created_at":"2025-10-22T05:42:09.141848Z","lead_time":36.058,"prediction":{"id":290,"task":1850,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766569Z","updated_at":"2025-10-13T10:55:30.766572Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"365065a9-571b-424b-932c-e2f658326c78","import_id":null,"last_action":null,"bulk_created":false,"task":1850,"project":9,"updated_by":1,"parent_prediction":290,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[290],"data":{"id":6289,"query":"In which season did Liverpool score the highest total number of goals in the Premier League?","dataset":"TableBench","task_specification_analysis":"Identify the season where Liverpool achieved the maximum total Premier League goals—this is a straightforward max\/argmax retrieval task.","scope_specification_analysis":"Metric is total PL goals for Liverpool per season; group by season and select the season(s) with the highest total. If tied, returning all tied seasons is a reasonable default.","domain_specification_analysis":"The contextual boundary is the English Premier League competition, which is globally unique and explicitly stated.","entity_specification_analysis":"Entities are clearly identifiable: \"Liverpool\" refers to Liverpool FC's men's first team, and the \"Premier League\" specifies the English top division since 1992–93. The context removes ambiguity with other Liverpool teams or competitions.","temporal_specification_analysis":"The query seeks the all-time peak season across all Premier League seasons, which is a superlative over the full historical range; no additional timeframe is needed."},"meta":{},"created_at":"2025-10-13T10:55:30.645886Z","updated_at":"2025-10-22T05:42:31.505982Z","inner_id":140,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1851,"annotations":[{"id":807,"completed_by":1,"result":[{"id":"m3RWmeZYDi","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"4kueHQWs3K","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"xrPiVz_Omh","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"nFLh8WJCpJ","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"6_vwJcySVY","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:28:53.825220Z","updated_at":"2025-10-22T14:28:53.825229Z","draft_created_at":"2025-10-22T14:28:26.765831Z","lead_time":38.733,"prediction":{"id":291,"task":1851,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766584Z","updated_at":"2025-10-13T10:55:30.766586Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"cae99fb4-1226-4008-85aa-62549b8404dc","import_id":null,"last_action":null,"bulk_created":false,"task":1851,"project":9,"updated_by":1,"parent_prediction":291,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[291],"data":{"id":6697,"query":"any person can petition matters 2 and 17. true or false?","dataset":"WikiTableQuestions","task_specification_analysis":"The user requests a binary truth evaluation (“true or false”) of a specific statement, which is a clear, executable action.","scope_specification_analysis":"Fulfilling the request involves a straightforward lookup\/verification against the identified rules or provisions for matters 2 and 17; no additional metrics, models, or calculations are required.","domain_specification_analysis":"No jurisdiction or institutional context is given (e.g., country, court, agency, association). Numbered \"matters 2 and 17\" only make sense within a specific domain, so the boundary is missing.","entity_specification_analysis":"The entities \"matters 2 and 17\" and \"any person\" are not tied to any specific statute, rulebook, constitution, court rules, or regulatory scheme. Many legal\/organizational systems number provisions as 2 and 17, so the reference cannot be resolved to a unique set of entities.","temporal_specification_analysis":"The query is in the present tense but legal permissions depend on jurisdiction and can change over time. Without knowing the governing instrument, there is no safe default timeframe to assume."},"meta":{},"created_at":"2025-10-13T10:55:30.645912Z","updated_at":"2025-10-22T14:28:53.863497Z","inner_id":141,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1852,"annotations":[{"id":833,"completed_by":1,"result":[{"id":"y_vp1gvp4r","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Ugk1DGLlNU","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"fKScOsnfg0","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"kuCxdZiLtF","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"PkwYKDmuv-","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:55:24.831164Z","updated_at":"2025-10-22T14:55:24.831173Z","draft_created_at":"2025-10-22T14:55:21.415391Z","lead_time":15.933,"prediction":{"id":292,"task":1852,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766598Z","updated_at":"2025-10-13T10:55:30.766601Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"6bedd9a0-7720-4f64-94e1-94cf636d8eb1","import_id":null,"last_action":null,"bulk_created":false,"task":1852,"project":9,"updated_by":1,"parent_prediction":292,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[292],"data":{"id":6793,"query":"which teams faced off at miller park?","dataset":"WikiTableQuestions","task_specification_analysis":"The query asks to identify (list) the teams that played each other at a specific venue. This is a straightforward retrieval\/listing task without complex analysis.","scope_specification_analysis":"Operationally: filter events by venue = \"Miller Park\" and extract the two opponent teams per event, returning a deduplicated list of opponent pairs (\"which teams\" implies unique entities; \"faced off\" implies matchups). No additional metrics, ranking, or modeling required.","domain_specification_analysis":"The contextual boundary is clearly the venue \"Miller Park,\" which is a specific, globally identifiable location, constraining the domain to events held there.","entity_specification_analysis":"\"Miller Park\" is a uniquely identifiable venue (Milwaukee Brewers’ stadium), but \"which teams faced off\" does not specify a particular game or event. Many different teams have played there (regular season, postseason, All-Star Game), so the teams cannot be uniquely determined.","temporal_specification_analysis":"No date or event is given. The phrasing suggests a specific matchup in the past, but without a timeframe there is no safe default (cannot assume the most recent or all-time)."},"meta":{},"created_at":"2025-10-13T10:55:30.645939Z","updated_at":"2025-10-22T14:55:24.871072Z","inner_id":142,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1853,"annotations":[{"id":730,"completed_by":1,"result":[{"id":"IpE2muSBAa","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"XtWIOTEvyP","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"Eu69R9BUxV","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"3G-xAZqj47","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"Pg6zpkdyFV","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:36:48.683910Z","updated_at":"2025-10-22T05:36:48.683924Z","draft_created_at":"2025-10-22T05:35:49.003505Z","lead_time":72.21,"prediction":{"id":293,"task":1853,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766613Z","updated_at":"2025-10-13T10:55:30.766615Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"a83c963c-4f7d-427e-899f-01ce1771c07f","import_id":null,"last_action":null,"bulk_created":false,"task":1853,"project":9,"updated_by":1,"parent_prediction":293,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[293],"data":{"id":6742,"query":"what is the average population density in mainland cities?","dataset":"WikiTableQuestions","task_specification_analysis":"The user asks to compute an average for a specific metric (population density) over a filtered subset (mainland cities). This is a clear aggregation task.","scope_specification_analysis":"Key analytical parameters are missing. It is unclear whether to take an unweighted mean of city-level densities, a population-weighted mean, an area-weighted mean, or to compute an overall density as total population divided by total area across mainland cities. These choices materially change the result. Units and handling of missing\/zero-area cases are also unspecified, though less critical.","domain_specification_analysis":"\"Mainland\" is a relative geographic boundary that requires a reference region\/country; multiple plausible interpretations exist, making the domain ambiguous.","entity_specification_analysis":"The core entity \"mainland cities\" is undefined without specifying which mainland (e.g., mainland China, mainland France, mainland U.S., or globally non-island cities). The set of cities cannot be determined.","temporal_specification_analysis":"No time frame is given; for population statistics a reasonable default would be the most recent available data, but this is not stated."},"meta":{},"created_at":"2025-10-13T10:55:30.645965Z","updated_at":"2025-10-22T05:36:48.727210Z","inner_id":143,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1854,"annotations":[{"id":751,"completed_by":1,"result":[{"id":"oUf4O5CeiG","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"AdEXHr-jaR","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"3WkAPDm1Qg","type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"M8pi4vkcy3","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"buIOOQNf_6","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:55:01.564377Z","updated_at":"2025-10-22T05:55:01.564388Z","draft_created_at":"2025-10-22T05:54:51.197854Z","lead_time":32.104,"prediction":{"id":294,"task":1854,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766627Z","updated_at":"2025-10-13T10:55:30.766630Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"3f43c2d5-4b56-4e94-a18d-037680d72b54","import_id":null,"last_action":null,"bulk_created":false,"task":1854,"project":9,"updated_by":1,"parent_prediction":294,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[294],"data":{"id":6812,"query":"how many times was korzeniowski disqualified from a competition?","dataset":"WikiTableQuestions","task_specification_analysis":"The user requests a count of a specific outcome (“disqualified”) for a specific individual, which is a clear counting task.","scope_specification_analysis":"Method is straightforward: filter records for the person named Korzeniowski with outcome\/status marked as disqualified and count occurrences. No additional parameters (ranking, grouping, models) required.","domain_specification_analysis":"No federation\/league\/sport boundary is given; the sensible default is across all competitions the (intended) person participated in, i.e., a universal domain.","entity_specification_analysis":"\"Korzeniowski\" is ambiguous (e.g., Robert Korzeniowski the racewalker, among others). Without a given name or additional context, the target individual cannot be uniquely identified.","temporal_specification_analysis":"The query seeks a total count of disqualifications, implying an all-time cumulative measure rather than a specific period."},"meta":{},"created_at":"2025-10-13T10:55:30.645992Z","updated_at":"2025-10-22T05:55:01.605301Z","inner_id":144,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1855,"annotations":[{"id":741,"completed_by":1,"result":[{"id":"AJK30qLGp9","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"4mwtLYq1JN","type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"li85DAXyy1","type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"BHOmbbzACT","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"H-aVcyTQVC","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T05:46:58.553248Z","updated_at":"2025-10-22T05:46:58.553259Z","draft_created_at":"2025-10-22T05:46:13.728575Z","lead_time":65.939,"prediction":{"id":295,"task":1855,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Recency)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766642Z","updated_at":"2025-10-13T10:55:30.766645Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"cab43818-35e7-4b41-b4f1-caf77263a7e1","import_id":null,"last_action":null,"bulk_created":false,"task":1855,"project":9,"updated_by":1,"parent_prediction":295,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[295],"data":{"id":6695,"query":"how many networks have more affiliates than canal de las estrellas?","dataset":"WikiTableQuestions","task_specification_analysis":"The query requests a count of items: how many networks have an affiliate count greater than that of Canal de las Estrellas. This is a clear counting task.","scope_specification_analysis":"Method is clear: compute affiliate counts per network, determine the affiliate count for Canal de las Estrellas, then count networks with strictly greater affiliate counts. No ambiguous ranking metric or aggregation choice is required beyond a simple count and comparison.","domain_specification_analysis":"The query does not specify a geographical or organizational boundary. A sensible widest interpretation is all television networks worldwide. While one could infer a Mexico-only comparison, that boundary is not stated. Thus, defaulting to a universal\/global domain is reasonable.","entity_specification_analysis":"Core entities: (1) 'Canal de las Estrellas' is a uniquely identifiable TV network (now branded 'Las Estrellas' in Mexico), so the reference can be resolved with world knowledge. (2) 'networks' is a broad class but contextually constrained to television broadcast networks by the use of 'affiliates' and the anchor entity being a TV network. Therefore, entities are reasonably resolvable.","temporal_specification_analysis":"No time frame is given and affiliate counts change over time. The present-tense phrasing ('have') implies interest in the latest available counts, which fits the common default for current-state statistics."},"meta":{},"created_at":"2025-10-13T10:55:30.646018Z","updated_at":"2025-10-22T05:46:58.593752Z","inner_id":145,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1856,"annotations":[{"id":556,"completed_by":1,"result":[{"id":"vbAj0HZ3om","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"-Y-Pn1pR4b","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"nLVn53FBQA","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"HAHquGzRRv","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"yGw6Ykwec_","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-13T11:27:08.640642Z","updated_at":"2025-10-13T11:27:08.640653Z","draft_created_at":"2025-10-13T11:12:21.944315Z","lead_time":896.444,"prediction":{"id":296,"task":1856,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766656Z","updated_at":"2025-10-13T10:55:30.766659Z","created_ago":"31 minutes","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"df8bff00-92c2-45c4-b263-0ab9cbb33b2a","import_id":null,"last_action":null,"bulk_created":false,"task":1856,"project":9,"updated_by":1,"parent_prediction":296,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[296],"data":{"id":6824,"query":"how long is the longest track listed?","dataset":"WikiTableQuestions","task_specification_analysis":"The user asks for the maximum duration among the listed tracks, i.e., return the value of the longest track’s length. This is a clear, single-step aggregation (max) over a known attribute.","scope_specification_analysis":"Compute the maximum of the track length\/duration field across all listed tracks and report that duration (in the dataset’s native units). No grouping, ranking metric ambiguity, or model choice is involved.","domain_specification_analysis":"The contextual boundary depends entirely on the unspecified list\/table referred to by \"listed\". Without knowing the dataset or domain (music, sports, etc.), the scope is ambiguous.","entity_specification_analysis":"The core entity \"track\" is ambiguous (could be a song, race track, running track, etc.) and \"listed\" references an unspecified list\/table. Without knowing which list or type of track, the entity cannot be uniquely identified.","temporal_specification_analysis":"No time frame is implied or needed; the question concerns a property (maximum length) within an unspecified list, not a temporal state."},"meta":{},"created_at":"2025-10-13T10:55:30.646045Z","updated_at":"2025-10-13T11:27:08.682417Z","inner_id":146,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1857,"annotations":[{"id":761,"completed_by":1,"result":[{"id":"IAr-piHhUU","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"3z7V4A6-a8","type":"choices","value":{"choices":["Not Applicable"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"3kS3VU9MIT","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"0MFuB-Edb8","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"aciwx8qSqi","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T06:02:54.819581Z","updated_at":"2025-10-22T06:02:54.819591Z","draft_created_at":"2025-10-22T06:02:40.879787Z","lead_time":50.269,"prediction":{"id":297,"task":1857,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Not Applicable"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766671Z","updated_at":"2025-10-13T10:55:30.766673Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"3c68e387-73b6-4d25-b3b5-4c201f1443f4","import_id":null,"last_action":null,"bulk_created":false,"task":1857,"project":9,"updated_by":1,"parent_prediction":297,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[297],"data":{"id":6540,"query":"what is the name of the laetare medalist listed before edward preuss?","dataset":"WikiTableQuestions","task_specification_analysis":"Direct lookup: identify the Laetare Medalist that appears immediately before “Edward Preuss” in the list. This is a concrete retrieval task.","scope_specification_analysis":"Procedure is clear: locate “Edward Preuss” in the ordered list and return the preceding entry (list order as presented). No additional metrics or methods required.","domain_specification_analysis":"\"Listed before\" depends on a specific list source and its ordering (e.g., Notre Dame’s official list vs. Wikipedia; ascending vs. descending). The list context is not specified, creating ambiguity.","entity_specification_analysis":"Entities are the Laetare Medal (a unique award) and the person \"Edward Preuss\" as a medal recipient; these can likely be uniquely identified with world knowledge (minor spelling variance risk).","temporal_specification_analysis":"The query concerns relative position in a list, not a time frame; no temporal filter is required."},"meta":{},"created_at":"2025-10-13T10:55:30.646071Z","updated_at":"2025-10-22T06:02:54.857178Z","inner_id":147,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1858,"annotations":[{"id":725,"completed_by":1,"result":[{"id":"4oef1sqI9i","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"Ijn0gbCY44","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"ZcDv42DO4l","type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"LFDpwoFAG0","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"ABoDviYNWp","type":"choices","value":{"choices":["Underspecified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-21T23:00:11.350549Z","updated_at":"2025-10-21T23:00:11.350561Z","draft_created_at":"2025-10-21T22:59:45.801195Z","lead_time":34.748,"prediction":{"id":298,"task":1858,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766685Z","updated_at":"2025-10-13T10:55:30.766688Z","created_ago":"1 week, 1 day","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"bd111242-c63b-4f37-97f0-3e938a17dc64","import_id":null,"last_action":null,"bulk_created":false,"task":1858,"project":9,"updated_by":1,"parent_prediction":298,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[298],"data":{"id":6517,"query":"what year has no place indicated?","dataset":"WikiTableQuestions","task_specification_analysis":"The query seeks to identify the year(s) for which a \"place\" value is not provided—essentially a filter\/list retrieval. The overall action (find and return the year(s)) is clear.","scope_specification_analysis":"Key procedural details are missing: (1) the definition of \"no place indicated\" (null, empty string, special codes like \"N\/A\"), (2) handling of multiple records per year (does any missing value qualify, or must all be missing for that year?), and (3) whether to return a single year or all applicable years. These choices can change the result materially.","domain_specification_analysis":"No contextual boundary is given for where a \"place\" would be indicated (e.g., country, organization, event series, database\/table). The domain is missing and cannot be inferred.","entity_specification_analysis":"The core entity is just \"year\" with reference to \"place indicated,\" but there is no subject dataset or event specified (e.g., which record, competition, list, or table). Multiple plausible interpretations exist, so the target entity cannot be resolved.","temporal_specification_analysis":"Although the query asks \"what year,\" there is no timeframe or context. The relevant time period depends entirely on an unspecified dataset or event, making the temporal intent ambiguous rather than defaulting to recency or all-time."},"meta":{},"created_at":"2025-10-13T10:55:30.646101Z","updated_at":"2025-10-21T23:00:11.393788Z","inner_id":148,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1859,"annotations":[{"id":852,"completed_by":1,"result":[{"id":"w7y2YszvNB","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"F7F7zBsEYd","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"DzUX2tT_6y","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"5qjualhkUW","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"gM-amTE6YN","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T15:09:47.268324Z","updated_at":"2025-10-22T15:09:47.268339Z","draft_created_at":"2025-10-22T15:09:35.087114Z","lead_time":30.005,"prediction":{"id":299,"task":1859,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766700Z","updated_at":"2025-10-13T10:55:30.766703Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"5528f5f8-3d0b-4b76-8876-3b28bad807bc","import_id":null,"last_action":null,"bulk_created":false,"task":1859,"project":9,"updated_by":1,"parent_prediction":299,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[299],"data":{"id":6696,"query":"how many total pillow pals were both reintroduced and retired in 1999?","dataset":"WikiTableQuestions","task_specification_analysis":"The query requests a count of items meeting specified conditions—i.e., a numeric total—so the analytical action is a straightforward COUNT operation.","scope_specification_analysis":"Filter the dataset to pillow pals where reintroduced_year = 1999 AND retired_year = 1999, then count unique items (by ID\/name). This is a clear boolean filter plus aggregation; reasonable default is distinct item count.","domain_specification_analysis":"The domain boundary is the Ty Inc. Pillow Pals product line, which is a globally recognized, singular context for \"Pillow Pals\"—especially consistent with the 1999 retirement timeline.","entity_specification_analysis":"\"Pillow Pals\" most plausibly refers to Ty Inc.'s Pillow Pals plush toy line (mid-1990s–1999). The query seeks a count of distinct Pillow Pal items that meet two conditions (reintroduced and retired) in 1999. In collector\/brand contexts, counting distinct characters\/SKUs is the conventional unit, making the entity reasonably resolvable.","temporal_specification_analysis":"The time frame is explicit: 1999, and the phrasing indicates both events (reintroduction and retirement) must occur within that year."},"meta":{},"created_at":"2025-10-13T10:55:30.646127Z","updated_at":"2025-10-22T15:09:47.322233Z","inner_id":149,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]},{"id":1860,"annotations":[{"id":840,"completed_by":1,"result":[{"id":"Wh0lgqZ1XK","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"entity_specification"},{"id":"kf3W7K5Qwb","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"temporal_specification"},{"id":"0SdwQez5ky","type":"choices","value":{"choices":["Underspecified (Assuming Universal Domain)"]},"origin":"manual","to_name":"query_text","from_name":"domain_specification"},{"id":"9gxfD7ZeD5","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"task_specification"},{"id":"dp4Z_Bzo0K","type":"choices","value":{"choices":["Specified"]},"origin":"manual","to_name":"query_text","from_name":"scope_specification"}],"was_cancelled":false,"ground_truth":false,"created_at":"2025-10-22T14:58:59.890154Z","updated_at":"2025-10-22T14:58:59.890166Z","draft_created_at":"2025-10-22T14:58:33.409280Z","lead_time":35.97,"prediction":{"id":300,"task":1860,"model":null,"score":1.0,"result":[{"type":"choices","value":{"choices":["Underspecified"]},"to_name":"query","from_name":"entity_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"temporal_specification"},{"type":"choices","value":{"choices":["Underspecified (Ambiguous)"]},"to_name":"query","from_name":"domain_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"task_specification"},{"type":"choices","value":{"choices":["Specified"]},"to_name":"query","from_name":"scope_specification"}],"cluster":null,"project":9,"model_run":null,"neighbors":null,"created_at":"2025-10-13T10:55:30.766715Z","updated_at":"2025-10-13T10:55:30.766718Z","created_ago":"1 week, 2 days","mislabeling":0.0,"model_version":"gpt-5"},"result_count":5,"unique_id":"9dd6db31-5573-4a7f-a09b-fdda1399d02c","import_id":null,"last_action":null,"bulk_created":false,"task":1860,"project":9,"updated_by":1,"parent_prediction":300,"parent_annotation":null,"last_created_by":null}],"file_upload":"a1658944-sufficient_specification_evaluation_sample_for_labelstudio.json","drafts":[],"predictions":[300],"data":{"id":6702,"query":"at which event did taylor jensen first win?","dataset":"WikiTableQuestions","task_specification_analysis":"The user requests the specific event name where Taylor Jensen achieved their first career win. This is a direct lookup of a single attribute (event) tied to the earliest win record.","scope_specification_analysis":"Procedure: filter records to Taylor Jensen’s wins; order wins chronologically by event date; select the earliest win; return the event name. If multiple wins occur on the same earliest date, return all corresponding events.","domain_specification_analysis":"The query lacks the contextual boundary (sport\/league\/tour\/organization). Without specifying the sport or competition (e.g., WSL Longboard Tour, national events, etc.), the domain is ambiguous.","entity_specification_analysis":"The core subject 'Taylor Jensen' is ambiguous; multiple individuals share this name. The query doesn’t specify which person (e.g., the professional longboard surfer vs. any other athlete or public figure). The target 'event' is not a unique entity and depends on the sport\/circuit context.","temporal_specification_analysis":"'First win' implies the earliest victory chronologically, providing an implicit all-time scope focused on the earliest occurrence. No additional time frame is needed."},"meta":{},"created_at":"2025-10-13T10:55:30.646157Z","updated_at":"2025-10-22T14:58:59.938352Z","inner_id":150,"total_annotations":1,"cancelled_annotations":0,"total_predictions":1,"comment_count":0,"unresolved_comment_count":0,"last_comment_updated_at":null,"project":9,"updated_by":1,"comment_authors":[]}]